\documentclass[12pt]{strath_thesis}

\usepackage{apacite}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[mathscr]{eucal}
\usepackage{ifpdf}
\usepackage{multirow}
\usepackage{eurosym}
\usepackage{rotating}
\usepackage{pdflscape}

%\usepackage{pslatex}
%\usepackage{palatino}

% \ifpdf
%   \usepackage[pdftex]{graphicx}
%   \graphicspath{{./pdf/}{./jpg/}}
%   \DeclareGraphicsExtensions{.pdf,.jpg,.png}
% \else
%   \usepackage[dvips]{graphicx}
%   \graphicspath{{./eps/}}
%   \DeclareGraphicsExtensions{.eps}
% \fi

\usepackage[ruled]{algorithm}
\usepackage{algorithmic}

\usepackage{tikz}
\usetikzlibrary{arrows,calc}
\usetikzlibrary{shadows}
\usetikzlibrary{decorations,decorations.pathmorphing}
\usetikzlibrary{positioning,shapes}
\tikzset{
>=stealth', % standard arrow tip
help lines/.style={dashed, thick}, % different line styles
axis/.style={<->},
important line/.style={thick},
connection/.style={thick, dotted},
}

% PGF/TikZ sequence diagrams.
\usepackage[underline=true,rounded corners=false]{pgf-umlsd}

\input{tikz/tikz_cmd}

\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\diag}{\mathop{\mathbf{diag}}}
\newcommand{\matpower}{\textsc{Matpower~}}
\newcommand{\matlab}{Matlab~}
\newcommand{\psat}{\textsc{Psat~}}
\newcommand{\pylon}{\textsc{Pylon~}}
\newcommand{\stable}{$\bullet$}
\newcommand{\unstable}{$\bullet$}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

% Optional argument for the parenthesis in bordermatrix.
\makeatletter
\newif\if@borderstar
\def\bordermatrix{\@ifnextchar*{%
\@borderstartrue\@bordermatrix@i}{\@borderstarfalse\@bordermatrix@i*}%
}
\def\@bordermatrix@i*{\@ifnextchar[{\@bordermatrix@ii}{\@bordermatrix@ii[()]}}
\def\@bordermatrix@ii[#1]#2{%
\begingroup
\m@th\@tempdima8.75\p@\setbox\z@\vbox{%
\def\cr{\crcr\noalign{\kern 2\p@\global\let\cr\endline }}%
\ialign {$##$\hfil\kern 2\p@\kern\@tempdima & \thinspace %
\hfil $##$\hfil && \quad\hfil $##$\hfil\crcr\omit\strut %
\hfil\crcr\noalign{\kern -\baselineskip}#2\crcr\omit %
\strut\cr}}%
\setbox\tw@\vbox{\unvcopy\z@\global\setbox\@ne\lastbox}%
\setbox\tw@\hbox{\unhbox\@ne\unskip\global\setbox\@ne\lastbox}%
\setbox\tw@\hbox{%
$\kern\wd\@ne\kern -\@tempdima\left\@firstoftwo#1%
\if@borderstar\kern2pt\else\kern -\wd\@ne\fi%
\global\setbox\@ne\vbox{\box\@ne\if@borderstar\else\kern 2\p@\fi}%
\vcenter{\if@borderstar\else\kern -\ht\@ne\fi%
\unvbox\z@\kern-\if@borderstar2\fi\baselineskip}%
\if@borderstar\kern-2\@tempdima\kern2\p@\else\,\fi\right\@secondoftwo#1 $%
}\null \;\vbox{\kern\ht\@ne\box\tw@}%
\endgroup
}
\makeatother


\ifpdf
\pdfinfo{/Author (Richard W. Lincoln)
  		 /Title  (Reinforcement Learning for Power Trade)
         /Subject (Electrical Power Engineering)}
\fi

%\title{Learning to Trade Power}
\title{Learning to Trade Power}
\author{Richard W. Lincoln}

\pagestyle{plain}

\begin{document}

\maketitle

\setcounter{page}{1}
\pagenumbering{roman}

\declaration

\begin{acknowledgements}
This research was funded by the UK Engineering and Physical Sciences Research
Council through the Supergen Highly Distributed Power Systems project under
grant GR/T28836/01.

I take this opportunity to thank my supervisors, Prof.~Graeme~Burt and
Dr~Stuart~Galloway, for their guidance and scholarship.  Many thanks also to
my parents for their support and help in editing this thesis.

This research made extensive use of software projects by researchers from other
institutions, made available as open source.  Optimal power flow solvers
were translated from \textsc{Matpower}, which is developed and maintained under the
direction of Ray~Zimmerman at Cornell University.  Reinforcement learning
algorithms and artificial neural networks were imported from PyBrain, which is
developed by researchers from the Dalle Molle Institute for Artificial
Intelligence (IDSIA) and the Technical University of Munich.  The Roth-Erev
learning method was translated from the Java Reinforcement Learning Module
(JReLM), developed by Charles Gieseler from Iowa State University.
\end{acknowledgements}

\begin{abstract}
In electric power Engineering, learning algorithms can be used to model
the strategies of electricity market participants.  The objective of this work
is to establish if policy gradient reinforcement learning methods can be used
to produce participant models superior to those using previously applied value
function based methods.

% Electricity is an unique commodity.  It must be supplied at the presise moment
% that it is demanded, as there are no practical methods for significant
% levels of storage in most systems.  There are also few options for controlling
% the direction in which power flows around networks, so generation or
% consumption at one point in the system has an effect at all other points.
Supply of electricity involves technology, money, people, natural resources and the environment.  All of these aspects are changing and the
discipline must be constantly researched to ensure that systems, such as
electricity markets, are fit for purpose.

In this thesis electricity markets are modelled using security constrained
optimal power flow that is formulated as a non-linear optimisation problem and
solved using a primal-dual interior point method.  Policy gradient reinforcement
learning methods are used to adjust the parameters of an artificial neural
networks that are used to approximate the policies market participants follow
when selecting quantities and prices to offer or bid in the marketplace.

Traditional reinforcement learning methods, that learn a function which returns
the \textit{value} associated with a particular action, have been previously
applied in simulated electricity trade, but are largely restricted to discrete
representations of the market environment.  Policy gradient methods have been
shown to offer good convergence properties in continuous environments, such as
in robotic control applications, and avoid many of the problems that mar value
function based methods.

The benefits of using policy gradient methods in electricity market simulation
are explored and the results demonstrate their superior trading ability when
operating with constrained networks.  This work provides some opportunities
revisit previous research in this field and opens possibilites for application
of policy gradient methods in decision support and automated energy trade.

% Reinforcement learning methods that use connectionist systems for value
% function approximation offer few convergence guarantees, even in simple
% systems.  Table-based value function reinforcement learning methods have been
% used previously for the simulation of electricity markets, but they operate
% only in discrete action and sensor domains.  If learning algorithms are to
% deliver on their potential for application in operational settings then it will
% be necessary for them to operate in continuous domains.  The principle
% contribution of this thesis is the demonstration of policy-gradient
% reinforcement learning algorithms being applied to continuous representations
% of electricity trading problems, showing that superior use of sensor data
% results in improved overall performance when compared with previously applied
% value-function methods.  From this it follows that learning methods which
% search directly in the policy space will be better suited to decision support
% applications and automated electric power trade.
\end{abstract}

\tableofcontents
\newpage

\listoffigures
\addcontentsline{toc}{chapter}{List of Figures}
\newpage

\listoftables
\addcontentsline{toc}{chapter}{List of Tables}
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}

\onehalfspacing

% Set the scene and problem statement. Introduce structure of thesis, state
% contributions (3-5).
\input{introduction}

% Demonstrate wider appreciation (context). Provide motivation.
\input{background}

% Survey and critical assessment. Relation to own work.
\input{related_work}

% Analysis, design, implementation and interpretation of results.
\input{method}

% Critical assessment of own work.
\input{results}

% State hypothesis. Further Work. Restate contribution.
\input{conclusion}

\bibliographystyle{apacite}
\renewcommand{\bibname}{Bibliography}
\bibliography{literature}
%\addcontentsline{toc}{chapter}{Bibliography}

\appendix
\input{appendix}

\end{document}
