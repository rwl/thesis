\documentclass[12pt]{strath_thesis}

\usepackage{apacite}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[mathscr]{eucal}
\usepackage{ifpdf}
\usepackage{multirow}
\usepackage{eurosym}
\usepackage{rotating}
\usepackage{pdflscape}

% \ifpdf
%   \usepackage[pdftex]{graphicx}
%   \graphicspath{{./pdf/}{./jpg/}}
%   \DeclareGraphicsExtensions{.pdf,.jpg,.png}
% \else
%   \usepackage[dvips]{graphicx}
%   \graphicspath{{./eps/}}
%   \DeclareGraphicsExtensions{.eps}
% \fi

\usepackage[ruled]{algorithm}
\usepackage{algorithmic}

\usepackage{tikz}
\usetikzlibrary{arrows,calc}
\tikzset{
>=stealth', % standard arrow tip
help lines/.style={dashed, thick}, % different line styles
axis/.style={<->},
important line/.style={thick},
connection/.style={thick, dotted},
}

\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\diag}{\mathop{\mathbf{diag}}}
\newcommand{\matpower}{\textsc{Matpower~}}
\newcommand{\matlab}{Matlab~}
\newcommand{\psat}{\textsc{Psat~}}
\newcommand{\pylon}{\textsc{Pylon~}}
\newcommand{\stable}{$\bullet$}
\newcommand{\unstable}{$\bullet$}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\ifpdf
\pdfinfo{/Author (Richard W. Lincoln)
  		 /Title  (Reinforcement Learning for Power Trade)
         /Subject (Electrical Power Engineering)}
\fi

\title{Learning to Trade Power}
\author{Richard W. Lincoln}

\pagestyle{plain}

\begin{document}

\maketitle

\setcounter{page}{1}
\pagenumbering{roman}

\declaration

\begin{acknowledgements}
This research was funded by the UK Engineering and Physical Sciences Research
Council through the Supergen Highly Distributed Power Systems project under
grant GR/T28836/01.

I take this opportunity to thank my supervisors, Prof.~Graeme~Burt and
Dr~Stuart~Galloway, for their guidance and scholarship.  Many thanks also to
my parents for their support and help in editing this thesis.

This research made extensive use of software projects by researchers from other
institutions, made available as open source.  Optimal power flow solvers
were translated from \textsc{Matpower}, which is developed and maintained under the
direction of Ray~Zimmerman at Cornell University.  Reinforcement learning
algorithms and artificial neural networks were imported from PyBrain, which is
developed by researchers from the Dalle Molle Institute for Artificial
Intelligence (IDSIA) and the Technical University of Munich.  The Roth-Erev
learning method was translated from the Java Reinforcement Learning Module
(JReLM), developed by Charles Gieseler from Iowa State University.
\end{acknowledgements}

\begin{abstract}
Connectionist reinforcement learning methods approximating value functions
offer few convergence guarantees, even in simple systems.  Reinforcement
learning has been applied previously to agent-based simulation of electricity
markets using discrete action and sensor domains. If learning algorithms
are to deliver on their potential for application in operational settings,
modelling continuous domains will be necessary.  The contribution of this
thesis is the proof that policy-gradient reinforcement learning algorithms can
be applied to continuous representations of electricity trading problems and
that their superior use of sensor data results in improved overall
performance, compared to previously applied value-function methods.  From this
it follows that learning methods which search directly in the policy space will
be better suited to decision support applications and automated electric power
trade.
\end{abstract}

\tableofcontents
\newpage

\listoffigures
\addcontentsline{toc}{chapter}{List of Figures}
\newpage

\listoftables
\addcontentsline{toc}{chapter}{List of Tables}
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}

\onehalfspacing

% Set the scene and problem statement. Introduce structure of thesis, state
% contributions (3-5).
\input{introduction}

% Demonstrate wider appreciation (context). Provide motivation.
\input{background}

% Survey and critical assessment. Relation to own work.
\input{related_work}

% Analysis, design, implementation and interpretation of results.
\input{method}

% Critical assessment of own work.
\input{results}

% State hypothesis. Further Work. Restate contribution.
\input{conclusion}

\bibliographystyle{apacite}
\renewcommand{\bibname}{Bibliography}
\bibliography{literature}
%\addcontentsline{toc}{chapter}{Bibliography}

\appendix
\input{appendix}

\end{document}
