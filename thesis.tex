\documentclass{strath_thesis}

\usepackage{apacite}
\usepackage{amsmath}
\usepackage{ifpdf}

\ifpdf
  \usepackage[pdftex]{graphicx}
  \graphicspath{{./pdf/}{./jpg/}}
  \DeclareGraphicsExtensions{.pdf,.jpg,.png}
\else
  \usepackage[dvips]{graphicx}
  \graphicspath{{./eps/}}
  \DeclareGraphicsExtensions{.eps}
\fi

\usepackage[ruled]{algorithm}
\usepackage{algorithmic}

\usepackage[T1]{fontenc} % small caps

\usepackage{tikz}
\usetikzlibrary{arrows,calc}
\tikzset{
>=stealth', % standard arrow tip
help lines/.style={dashed, thick}, % different line styles
axis/.style={<->},
important line/.style={thick},
connection/.style={thick, dotted},
}

\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\diag}{\mathop{\mathbf{diag}}}
\newcommand{\matpower}{M\textsc{ATPOWER~}}

\ifpdf
\pdfinfo{/Author (Richard W. Lincoln)
  		 /Title  (Reinforcement Learning for Power Trade)
         /Subject (Electrical Power Engineering)}
\fi

\title{Learning to Trade Power}
\author{Richard W. Lincoln}

\collegeordept{Department of Electronic and Electrical Engineering}
\crest{\includegraphics[width=80mm]{figures/strath_logo-mono}}
\university{University of Strathclyde}
\degree{Doctor of Philosophy}
\degreedate{2010}

\pagestyle{plain}

\begin{document}

\maketitle
\newpage

%\frontmatter   % use with book class
\setcounter{page}{1}
\pagenumbering{roman}

\thispagestyle{plain}
\begin{quote}
This thesis is the result of the author's original research.  It has been
composed by the author and has not been previously submitted for examination
which has led to the award of a degree.

The copyright of this thesis belongs to the author under the terms of the
United Kingdom Copyright Acts as qualified by University of Strathclyde
Regulation 3.51. Due acknowledgement must always be made of the use of any
material contained in, or derived from, this thesis.

% \vspace{1cm}
% Signed:\hspace{7.6cm} Date: \today
\end{quote}
\newpage

\thispagestyle{plain}
\begin{center}
\vspace*{1.5cm}
{\Large \bfseries Acknowledgements}
\end{center}
\vspace{0.5cm}
\begin{quote}
I would like to acknowledge ...
\end{quote}
\newpage

\thispagestyle{plain}
\begin{center}
  \vspace*{1.5cm}
  {\Large \bfseries  Abstract}
\end{center}
\vspace{0.5cm}
\begin{quote}
Connectionist reinforcement learning methods approximating value-functions
offer few convergence guarantees, even in simple systems.  Reinforcement
learning has been applied previously to agent-based simulation of energy
markets using only discrete action and sensor domains. If learning algorithms
are to deliver on their potential for application in operational settings,
modelling continuous domains is necessary.  The contribution of this paper is
to show that policy-gradient reinforcement learning algorithms can be applied
to continuous representations of energy trading problems and that their
superior use of sensor data results in improved performance over previously
applied value-function methods.  From this it follows that algorithms which
search directly in the policy space will be better suited to decision support
applications and automated energy trade.
\end{quote}

\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

%\mainmatter   % use with book class

\onehalfspacing
%\pagestyle{fancy}

% Set the scene and problem statement. Introduce structure of thesis, state
% contributions (3-5).
\input{introduction}

% Demonstrate wider appreciation (context). Provide motivation.
\input{background}

% Survey and critical assessment. Relation to own work.
\input{related_work}

%\input{problem_statement}

% Analysis, design, implementation and interpretation of results.
\input{core}

% Critical assessment of own work. State hypothesis. Further Work. Restate
% contribution.
\input{conclusion}

\bibliographystyle{apacite}
%\bibliographystyle{apacite}
%\renewcommand{\bibname}{References}
\bibliography{literature}
\addcontentsline{toc}{chapter}{Bibliography}

%\backmatter
\end{document}
