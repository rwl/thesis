\chapter{Background Theory}
\label{ch:background}
This chapter provides introductions to optimal power flow and reinforcement
learning.  The methods described are used to model competitive electricity
trade as decribed in Chapter \ref{ch:method}, below.  Interior-point methods
are commonly used to find solutions to optimal power flow problems and an
introduction is provided.  Optimal power flow is one of the most widely studied
subjects in Power Engineering and comprehensive literature reviews are
available \cite{momoh:part1,momoh:part2}.  Learning through reinforcement is a
broad concept and the theory has been explored and published in detail
\cite{suttonbarto:1998,bertsekas:96}.

\section{Optimal Power Flow}
\label{sec:opf}
Optimal power flow is a term used to describe a broad class of problems in
which control variables are automatically changed to arrive at the ``best''
solution with respect to a stated quantitative performance measure while
satisfying constraints dictated by operational and pyhical characteristics of
the electric power system.  It is the formulation of a numerical optimisation
problem or mathmatical program using power system equations.   Optimisation
problems have the general form:
\begin{equation}
\min_x f(x)
\end{equation}
subject to
\begin{eqnarray}
g(x)& =& 0\\
h(x)& \leq& 0\\
x_{min}\leq& x& \leq x_{max}
\end{eqnarray}

\subsection{Interior-Point Methods}

\section{Reinforcement Learning}
\label{sec:rl}
The problem of learning how best to interact with an environment so as to
maximise some long-term reward is a general one.  Reinforcement learning is a
term from the field of machine learning that is typically applied to
understanding, automating and solving this problem through adaptive
computational approaches.  Unlike many machine learning techinques, the
algorithms are not instructed as to which actions to take, but must learn to
maximise the long-term reward through trial-and-error.

Reinforcement learning starts with an interactive, goal-seeking individual
agent existing in an environment.  The agent requires the ability to;
\begin{itemize}
  \item Sense aspects of its environment,
  \item Perform actions that influence the state of its environment and
  \item be assigned rewards as a response to their chosen action.
\end{itemize}
An agent follows a particular \textit{policy} when mapping the perceived state
of its environment to an action choice.  Reinforcement learning methods adjust
the agent's policy.

% \subsection{Markov Decision Processes}
% \subsection{Dynamic Programming}
% \subsection{Monte-Carlo Methods}
% \subsection{Temporal-Difference Learning}
% \subsection{Artificial Neural-Networks}

\subsection{Value Function Methods}
\label{sec:valuebased}
Value-based methods attempt to find the optimal policy by
approximating a \textit{value-function} which returns the total reward an
agent can expect to accumulate, given an initial state and following the
current policy thereafter.  The policy is adjusted using the reinforcing
signal and information from previous action choices.

\subsubsection{Basic Roth-Erev}
\label{sec:rotherev}
The Roth-Erev reinforcement learning algorithm uses a stateless policy to
select actions from a discrete domain \cite{roth:games,roth:aer}.  The dataset
stored by each agent, $j$, contains an array of length $K$, where $K$ is the
number of feasible actions $k$.  Each value in the array represents the
propensity for selection of the associated action in all states of the
environment.  Following interaction $t$ in which agent $j$ performed action
$k^\prime$ on its environment, for arbitrary positive $t$, a reward
$r_{jk^\prime}(t)$ is calculated.  The propensity for agent $j$ to select
action $k$ for interaction $t+1$ is
\begin{equation}
q_{jk}(t+1) =
\begin{cases}
(1-\phi)q_{ik}(t) + r_{jk^\prime}(t)(1-\epsilon), & \text{$k = k^\prime$} \\
(1-\phi)q_{ik}(t) + r_{jk^\prime}(t)(\frac{\epsilon}{K-1}), & \text{$k \ne
k^\prime$}
\end{cases}
\end{equation}
where $\phi$ and $\epsilon$ denote \textit{recency} and
\textit{experimentation} parameters, respectively.  The recency (forgetting)
parameter degrades the propensities for all actions and prevents the values
from going unbounded.  It is intended to represent the tendency for players to
forget older action choices and to prioritise more recent experience.  The
experimentation parameter prevents the probability of choosing an action from
going to zero and encourages exploration of the action space.

Erev and Roth proposed that actions be selected according to a discrete
probability distribution function where action $k$ is selected for interaction
$t+1$ with probability:

\begin{equation}
p_{jk}(t+1) = \frac{q_{jk}(t+1)}{\sum_{l=0}^K q_{jl}(t+1)}
\end{equation}

Since $\sum_{l=0}^K q_{jl}(t+1)$ increases with $t$, a reward $r_{jk}(t)$ for
performing action $k$ will have a greater effect on the probability
$p_{jk}(t+1)$ during early interactions while $t$ is small.  This is intended
to represent Psychology's \textit{Power Law of Practice} in which it is
qualitatively stated that, with practice, learning occurs at a decaying
expoential rate and that a learning curve will eventually flatten out.

This algorithm may alternatively use a form of the \textit{softmax} method
\cite{suttonbarto:1998} using the Gibbs, or Boltzmann, distribution to select
action $k$ for the $t+1$th interaction with probability

\begin{equation}
p_{jk}(t+1) = \frac{e^{q_{jk}(t+1)/\tau}}{\sum_{l=0}^K e^{q_{jl}(t+1)/\tau}}
\end{equation}

where $\tau$ is the \textit{temperature} parameter.  This parameter may be
lowered in value over the course of an experiment since high values give all
actions similar probability and encourage exploration of the action space,
while low values promote exploitation of past experience.

\subsubsection{Variant Roth-Erev}
\label{sec:variant}
Two shortcomings of the basic Roth-Erev algorithm have been identified and a
variant formulation proposed \cite{nicolaisen:2001}.  The two issues are
\begin{itemize}
  \item The values by which propensities are updated can be zero or very small
  for certain combinations of the experimentation parameter $\epsilon$ and
  the total number of feasible actions $K$ and
  \item all propensity values are decreased by the same amount when the reward,
  $r_{jk^\prime}(t)$ is zero.
\end{itemize}
With the variant algorithm, the propensity of agent $j$ to select action $k$
for interaction $t+1$ is:
\begin{equation}
q_{jk}(t+1) =
\begin{cases}
(1-\phi)q_{ik}(t) + r_{jk^\prime}(t)(1-\epsilon), & \text{$k = k^\prime$} \\
(1-\phi)q_{ik}(t) + q_{jk}(t)(\frac{\epsilon}{K-1}), & \text{$k \ne
k^\prime$}
\end{cases}
\end{equation}
As with the basic Roth-Erev algorithm, the propensity for selection of the
action that the reward is associated with is adjusted by the experimentation
parameter.  All other action propensities are adjusted by a small proportion of
their current value.

\subsubsection{SARSA}
\label{sec:sarsa}
% Sarsa is an on-policy Temporal Difference control method.  The
% policy is represented by a $M \times N$ table, where $M$ and $N$ are
% arbitrary positive numbers equal to the total number of feasible states and
% actions. The action-value update for agent $j$ is defined by
%
% \begin{equation}
% Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma Q_j(s_{jt+1},a_{jt+1}) -
% Q_j(s_{jt},a_{jt})].
% \end{equation}
%
% While the Q-learning algorithm updates action-values using a greedy policy,
% which is different to that being followed, Sarsa uses the discounted future
% reward of the next state-action observation following the original policy.
The SARSA algorithm is an on-policy Temporal Difference control method where
the policy is typically represented by a $M \times N$ table, where $M$ and $N$
are arbitrary positive numbers equal to the total number of feasible states and
actions respectively.  The action-value update for agent $j$ is defined by
\begin{equation}
Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma Q_j(s_{jt+1},a_{jt+1}) -
Q_j(s_{jt},a_{jt})]
\end{equation}
While the Q-learning method (See Section \ref{sec:qlearning}, below) updates
action-values using a greedy policy, which is a different policy to that being
followed, SARSA uses the discounted future reward of the next state-action
observation following the original policy.

\subsubsection{Q-Learning}
\label{sec:qlearning}
This section describes the original off-policy Temporal Difference Q-learning
method developed by Watkins \cite{watkins:1989}.  The action-value function,
$Q(s,a)$, returns values from a $M \times N$ matrix where $M$ and $N$ are
arbitrary positive numbers equal to the total number of feasible states and
actions, respectively.  Each value represents the \textit{quality} of taking a
particular action, $a$, in state $s$.  Actions are selected using either the
$\epsilon$-greedy or softmax (See Section \ref{sec:rotherev}, above) methods.
The $\epsilon$-greedy method either selects the action (or one of the actions)
with the highest estimated value or it selects an action at random, uniformly,
independently of the estimated values with (typically small) probability
$\epsilon$.

Agent $j$ will observe a reward, $r_{jt}$, and a new state, $s_{jt+1}$,
after taking action $a_{jt}$ at step $t$ when in state $s_{jt}$.  The
state-action value, $Q_j(s_{jt},a_{jt})$, is updated according to the
maximum value of available actions in state $s_{t+1}$ and becomes
\begin{equation}
\label{eq:qlearning}
Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma\max_{a} Q_j(s_{jt+1},a_{jt}) -
Q_j(s_{jt},a_{jt})]
\end{equation}
where $\alpha$ and $\gamma$ are the learning rate, $0\leq\alpha\leq1$, and
discount factor, $0\leq\gamma\leq1$, respectively.  The learning rate
determines the extent to which new rewards will override the effect of older
rewards.  The discount factor allows the balance between maximising immediate
rewards and future rewards to be set.

\subsubsection{Q($\lambda$)}
\label{sec:qlambda}
In the Q-learning formulation, described in equation \ref{eq:qlearning}, only
the quality associated with the previous state, $s_{jt}$, is updated.  However,
the preceding states can also, in general, be said to be associated with the
reward $r_{jt+1}$.  Eligibility traces are a mechanism for representing this
effect and in algorithms such as Q($\lambda$), they are what the $\lambda$
refers to.  The eligibility trace for a state $e(s)$ represents how eligible
the state $s$ is to receive credit or blame for the error.  The term ``trace''
refers to fact that only recently visited states become eligible.  The
eligibility value for the current state is increased while for all other
states it is attenuated by a factor $\lambda$.

The off-policy nature of Q-learning requires special care to be taken when
implementing eligibility traces.  While the algorithm may learn a greedy
policy, in which the action with the maximum value would always be taken,
typically a policy with some degree of exploration will be followed when
choosing actions.  If an exploratory (pseudo-random) step is taken the
preceding states can no longer be considered eligible for credit or blame.
Setting $\lambda$ to $0$ for non-greedy actions removes much of the benefit of
using eligibility traces if exploratory actions are frequent.  A solution to
this has been developed, but requires a very complex implementation
\cite{peng:1996}.  A na\"ive approach can be taken, where the effect of
exploratory actions is ignored, but the results of this are unexplored.

\subsection{Policy Gradient Methods}
\label{sec:policygradient}
The value-function methods defined in Section \ref{sec:valuebased} typically
rely upon discretisation of the sensor and action spaces so the associated
values may be stored in tables.  The memory requirements for this restrict the
application of these methods to small environments.  Many environments,
particularly from real applications, exhibit continuous sensor and/or action
spaces and require generalisation techniques to be employed to provide a more
compact policy representation.

Policy-gradient methods use only the reward signal and search directly in the
space of the policy parameters.  The agent's policy function approximator is
updated according to the gradient of expected reward with respect to its
parameters.

\subsubsection{REINFORCE}
\label{sec:reinforce}
REINFORCE is an associative reinforcement learning method that determines
a policy by modifying the parameters of a policy function approximator, rather
than approximating a value function \cite{williams:reinforce}.  Commonly,
feedforward artificial neural networks are used to represent the policy, where
the input is a representation of the state and the output is action selection
probabilities.  In learning, a policy gradient approach is taken where the
weights of the network are adjusted in the direction of the gradient of
expected reinforcement.

Defining the network, let $\mathbf{x}^i$ denote the vector of inputs to the
$i$th unit and $y_i$ denote output of the unit.  In the input layer of the
network the elements $x_j$ of $\mathbf{x}^i$ are normalised sensor values from
the environment and in the output layer, or in any hidden layers, they are
outputs from the $j$ unit in the preceding layer.  Let $\mathbf{w}^i$ denote
the vector of the weights, $w_{ij}$, on the connections to the $i$th unit.  The
output of the $i$th unit is dependant on the vector of inputs, $\mathbf{x}^i$,
and the associated weights, $\mathbf{w}^i$.

For each interaction of the agent with the environment, each parameter $w_{ij}$
is incremented by
\begin{equation}
\label{eq:reinforce}
\Delta w_{ij} = \alpha_{ij}(r - b_{ij})\frac{\partial\ln\rho_i}{\partial
w_{ij}}
\end{equation}
where $\alpha_{ij}$ is the \textit{learning factor}, $b_{ij}$ is the
\textit{reinforcement baseline} and $\rho_i$ is the performance of the policy
(e.g., the average reward per interaction).

\subsubsection{ENAC}
\label{sec:enac}
ToDo: Episodic Natural Actor Critic\cite{peters:enac}.
