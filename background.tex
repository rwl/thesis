\chapter{Background Theory}
\label{ch:background}
This chapter provides introductions to optimal power flow and reinforcement
learning.  The methods described are used to model competitive electricity
trade as decribed in Chapter \ref{ch:method}, below.  Interior-point methods
are commonly used to find solutions to optimal power flow problems and an
introduction is provided.  Optimal power flow is one of the most widely studied
subjects in Power Engineering and comprehensive literature reviews are
available \cite{momoh:part1,momoh:part2}.  Learning through reinforcement is a
broad concept and the theory has been explored and published in detail
\cite{suttonbarto:1998,bertsekas:96}.

\section{Optimal Power Flow}
\label{sec:opf}
Nationalised electricity supply industries are typically planned operated and
controlled centrally.  A system operator determines which generators must
operate and the required output of the operating units usch that demand is met
and the overall cost of production is minimised.  In electric power engineering,
this is termed the \textit{unit commitment} and \textit{economic dispatch}
problem.

In 1962 a unit commitment formulation was published that incorporated
network constraints \cite{carpentier:opf}.  \textit{Optimal power flow} is the
integration of both economic and power flow aspects of power systems into a
mathematical optimisation problem.  The ability to solve centralised
power system operation problems and determine prices in power pool markets has
led to optimal power flow being one of the most widely studied subjects in the
power systems community.

\subsection{Power Flow Formulation}
\label{sec:pf_form}
Optimal power flow derives its name from the \textit{power flow}, or load flow,
steady-state power system analysis technique.  Given sets of generator data,
load data and a nodal admittance matrix $Y_{bus}$, a power flow study
determines the voltage
\begin{equation}
V_i = \vert V_i \vert \angle\delta_i = \vert
V_i\vert(\cos\delta_i + j\sin\delta_i)
\end{equation}
at each node $i$ in the power system.
The structure of $Y_{bus}$ is dependant upon the transmission line, transformer
and shunt capacitor models employed and any transformer tap settings.  A
typical formulation of $Y_{bus}$ is given in Section \ref{sec:acpf}.  Crucially,
the relationship between nodal voltages and power entering the network is
non-linear.  For a network of $n_b$ nodes, the current injected at node $i$ can
be extracted as
\begin{equation}
I_i = \sum_{j=1}^{n_b} Y_{ij} V_j
\end{equation}
where $Y_{ij} = \vert Y_{ij}\vert \angle\theta_{ij}$ is the $(i,j)^{th}$ element
if the, $n_b \times n_b$, $Y_{bus}$ matrix.  Hence, the apparent power entering
the network at bus $i$ is
\begin{equation}
S_i = P_i+Q_i = V_iI_i^* = \sum_{n=1}^{n_b} \vert Y_{ij}V_iV_j \vert \angle
(\delta_i - \delta_j - \theta_{ij})
\end{equation}
Converting to polar coordinates and separating the real and imaginary part, the
active power
\begin{equation}
P_i = \sum_{n=1}^{n_b} \vert Y_{ij}V_iV_j \vert \cos(\delta_i - \delta_j -
\theta_{ij})
\end{equation}
and reactive power entering the network
\begin{equation}
Q_i = \sum_{n=1}^{n_b} \vert Y_{ij}V_iV_j \vert \sin(\delta_i - \delta_j -
\theta_{ij})
\end{equation}
at bus $i$ are non-linear functions of $V_i$, as indicated by the presence of
the sine and cosine terms.  Kirchoff's Current Law requires that the net
complex power injection (generation - load) at each bus equals the sum of
complex power flows on each connected branch, given by the power balance
equations
\begin{equation}
\label{eq:p_balance}
P_{g,i} - P_{d,i} = P_i
\end{equation}
and
\begin{equation}
\label{eq:q_balance}
Q_{g,i} - Q_{d,i} = Q_i .
\end{equation}

\subsection{Optimal Power Flow Formulation}
Optimal power flow is formulated as a mathematical optimisation problem in
which the complex power balance equations \ref{eq:p_balance} and
\ref{eq:q_balance} must be satisfied.  Optimisation problems have the general
form
\begin{equation}
\min_x f(x)
\end{equation}
subject to
\begin{eqnarray}
\label{eq:equality}
g(x)& =& 0\\
\label{eq:inequality}
h(x)& \leq& 0
\end{eqnarray}
where $x$ is the optimisation variable, $f$ is the objective function and
equations (\ref{eq:equality}) and (\ref{eq:inequality}) are sets of equality
and inequality constraints on $x$, respectively.  Typical inequality
constraints are bus voltage magnitude contingency state limits, generators
output limits and branch power (or current) flow limits.  The optimisation
variable $x$ may contain generator set-points, bus voltages, transformer tap
settings etc.  If the optimisation variable $x$ is empty the formulation reduces
to a power flow problem as described in Section \ref{sec:pf_form}, above.

A common objective of optimal power flow is total system cost minimisation.
For and network of $n_g$ generators the objective function is
\begin{equation}
\min \sum_{k=1}^{n_g} C_k(P_{g,k})
\end{equation}
where $C_k$ is a cost function (typically quadratic) of the set-point $P_{g,k}$
of generator $k$.  Alternative objectives may be to minimise losses, maxmimise
the voltage stability margin or minimise deviation of an optimisation variable
from a particular schedule.

% \subsection{Interior-Point Methods}

\section{Reinforcement Learning}
\label{sec:rl}
% The problem of learning how best to interact with an environment so as to
% maximise some long-term reward is a general one.  Reinforcement learning is a
% term from the field of machine learning that is typically applied to
% understanding, automating and solving this problem through adaptive
% computational approaches.  Unlike many machine learning techinques, the
% algorithms are not instructed as to which actions to take, but must learn to
% maximise the long-term reward through trial-and-error.

% Reinforcement learning starts with an interactive, goal-seeking individual
% agent existing in an environment.  The agent requires the ability to;
% \begin{itemize}
%   \item Sense aspects of its environment,
%   \item Perform actions that influence the state of its environment and
%   \item be assigned rewards as a response to their chosen action.
% \end{itemize}
% An agent follows a particular \textit{policy} when mapping the perceived state
% of its environment to an action choice.  Reinforcement learning methods adjust
% the agent's policy.
Reinforcement learning is learning from reward by mapping situations to actions
when interating with an uncertain environment.  An agent learns \textit{what}
to do to achieve a task through trial-and-error using a numerical reward or
penalty signal without being instructed \textit{how} to achieve it.  In
challenging cases, actions may not yield immediate reward or may affect the
next situation and all subsequent rewards.  A compromise must be made between
exploitation of past experiences and exploration of the environment through new
action choices.  A reinforcement learning agent must be able to:
\begin{itemize}
  \item Sense aspects of its environment,
  \item Take actions that influence its environment and,
  \item Have an explicit goal or set of goals relating to the state of its
  environment.
\end{itemize}
In the classic model of agent--environment interaction, at each time step $t$
in a sequence of discrete time steps $t = 1,2,3\dotsc$ an agent receives
some form of the environment's state $s_t \in \mathscr{S}$, where $\mathscr{S}$
is the set of possible states.  A set of actions $\mathscr{A}(s_t)$ are
available to the agent in state $s_t$, from which the agent selects an action
$a_t$ and performs it upon the environment.  The environment enters a new state
$s_{t+1}$ in the next time step and the agent receives a scalar numerical
reward $r_{t+1} \in \mathbb{R}$ in part as a result of its action.  The agent
may then learn using the state representations $s_t$ and $s_{t+1}$, the chosen
action $a_t$ and the reinforcement signal $r_{t+1}$ before beginning the next
interaction.  Figure X diagrams the agent--environment interaction event
sequence.

\subsection{Markov Decision Processes}
For a finite number of states $\mathscr{S}$, if all states are Markov, the
agent is interacting with a finite Markov decision process (MDP).  Informally,
for a state to be Markov it must retain all relevant information about the
complete sequence of positions leading up to the state, such that all future
states and expected rewards can be predicted as well as would be possible given
a complete history.  A particular MDP is defined for a discrete set of time
steps by a state set $\mathscr{S}$, an action set $\mathscr{A}$, a set of state
transition probabilities $\mathscr{P}$ and a set of expected reward values
$\mathscr{R}$.  Given a state $s$ and an action $a$, the probability of
transitioning to each possible next state $s^\prime$ is
\begin{equation}
\mathscr{P}^a_{ss^\prime} = \Pr \bigl\lbrace s_{t+1} = s^\prime \vert s_t=s,
a_t=a \bigr\rbrace .
\end{equation}
Given the next state $s^\prime$, the expected value of the next reward is
\begin{equation}
\mathscr{R}^a_{ss^\prime} = E \bigl\lbrace r_{t+1} \vert s_t=s, a_t=a,
s_{t+1}=s^\prime \bigr\rbrace .
\end{equation}
In practice not all state signals are Markov, but should provide a good basis
for predicting subsequent states, future rewards and selecting actions.  If the
state transition probabilities and expected reward values are not known, only
the states and actions, then samples from the MDP must be taken and a value
function approximated iteratively based on new experiences generated by
performing actions.

\subsection{Value Function Methods}
\label{sec:valuebased}
% Value-based methods attempt to find the optimal policy by
% approximating a \textit{value-function} which returns the total reward an
% agent can expect to accumulate, given an initial state and following the
% current policy thereafter.  The policy is adjusted using the reinforcing
% signal and information from previous action choices.
Any method that can optimise control of a MDP may be considered a reinforcement
learning method.  All search for an optimal policy $\pi^*$ that maps state
$s \in \mathscr{S}$ and action $a \in \mathscr{A}$ to the probability
$\pi^*(s,a)$ of taking $a$ in $s$ and maxmimises the sum of rewards over the
agents lifetime.

Each state $s$ under policy $\pi$ may be associated with a \textit{value}
$V^\pi(s)$ equal to the expected return from following policy $\pi$ from state
$s$.  Most reinforcement learning methods are based on estimating the
state-value function
\begin{equation}
\label{eq:statevalue}
V^\pi(s) = E \Bigg\lbrace \sum^\infty_{t=0} \gamma^t r_t \Bigg\vert s_0 = s
\Bigg\rbrace
\end{equation}
where $\gamma$ is a discount factor, with $0\leq \gamma \leq 1$.
Performing certain action may result in no state change, creating a loop and
causing the value of that action to be infinite for some policies.
The discount factor $\gamma$ prevents values from going unbounded and
represents reduced trust in the reward $r_t$ as discrete time $t$
increases.  Many reinforcement learning methods estimate the action-value
function
\begin{equation}
\label{eq:actionvalue}
Q^\pi(s,a) = E \Bigg\lbrace \sum^\infty_{t=0} \gamma^t r_t \Bigg\vert s_0 = s,
a_0 = a \Bigg\rbrace
\end{equation}
which defines the value of taking action $a$ in state $s$ under fixed policy
$\pi$.

\subsubsection{Temporal-Difference Learning}
Temporal Difference (TD) learning is a central idea in reinforcement learning.
TD methods do not attempt to estimate the state transition probabilities and
expected rewards of the finite MDP, but estimate the value function directly.
They learn to \textit{predict} the expected value of total reward returned by
the state-value function (\ref{eq:statevalue}).  For an exploratory policy $\pi$
and a non-terminal state $s$, an estimate of $V^\pi(s_t)$ at any given time step
$t$ is updated using the estimate at the next time step $V^\pi(s_{t+1})$ and the
observed reward $r_{t+1}$
\begin{equation}
V^\pi(s_t) \leftarrow V^\pi(s_t) + \alpha \bigl[r_{t+1} + \gamma
V^\pi(s_{t+1}) - V^\pi(s_t) \bigr]
\end{equation}
where $\alpha$ is the learning rate, with $0 \leq \alpha \leq 1$, which controls
how much attention is paid to new data when updating $V^\pi$.  TD learning
evaluates a particular policy and offers strong convergence guarantees, but does
not learn a better policy.

\subsubsection{Sarsa}
\label{sec:sarsa}
% The SARSA algorithm is an on-policy Temporal Difference control method where
% the policy is typically represented by a $M \times N$ table, where $M$ and $N$
% are arbitrary positive numbers equal to the total number of feasible states and
% actions respectively.  The action-value update for agent $j$ is defined by
% \begin{equation}
% Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma Q_j(s_{jt+1},a_{jt+1}) -
% Q_j(s_{jt},a_{jt})]
% \end{equation}
% While the Q-learning method (See Section \ref{sec:qlearning}, below) updates
% action-values using a greedy policy, which is a different policy to that being
% followed, SARSA uses the discounted future reward of the next state-action
% observation following the original policy.
Sarsa (or modified Q-learning) is an off-policy TD control method that
approximates the state-action value function in (\ref{eq:actionvalue}) which
returns the total expected reward for an agent following a policy for selecting
actions as a function of future states.
\begin{equation}
\label{eq:sarsa}
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \bigl[r_{t+1} + \gamma
Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\bigr]
\end{equation}
The update also uses the action from the next time step $a_{t+1}$ also and the
requirement to transition through state-action-reward-state-action for each time step is from where the
algorithm's name, Sarsa, is derived.

\subsubsection{Q-Learning}
\label{sec:qlearning}
% This section describes the original off-policy Temporal Difference Q-learning
% method developed by Watkins \cite{watkins:1989}.  The action-value function,
% $Q(s,a)$, returns values from a $M \times N$ matrix where $M$ and $N$ are
% arbitrary positive numbers equal to the total number of feasible states and
% actions, respectively.  Each value represents the \textit{quality} of taking a
% particular action, $a$, in state $s$.  Actions are selected using either the
% $\epsilon$-greedy or softmax (See Section \ref{sec:rotherev}, above) methods.
% The $\epsilon$-greedy method either selects the action (or one of the actions)
% with the highest estimated value or it selects an action at random, uniformly,
% independently of the estimated values with (typically small) probability
% $\epsilon$.
%
% Agent $j$ will observe a reward, $r_{jt}$, and a new state, $s_{jt+1}$,
% after taking action $a_{jt}$ at step $t$ when in state $s_{jt}$.  The
% state-action value, $Q_j(s_{jt},a_{jt})$, is updated according to the
% maximum value of available actions in state $s_{t+1}$ and becomes
% \begin{equation}
% \label{eq:qlearning}
% Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma\max_{a} Q_j(s_{jt+1},a_{jt}) -
% Q_j(s_{jt},a_{jt})]
% \end{equation}
% where $\alpha$ and $\gamma$ are the learning rate, $0\leq\alpha\leq1$, and
% discount factor, $0\leq\gamma\leq1$, respectively.  The learning rate
% determines the extent to which new rewards will override the effect of older
% rewards.  The discount factor allows the balance between maximising immediate
% rewards and future rewards to be set.
Q-learning is an off-policy TD method that again does not estimate the finite
MDP directly, but alternatively iteratively approximates the state-action value
function which returns the value of taking action $a$ in state $s$ and taking
the maximum across all actions, following an \textit{optimal} policy
thereafter. The same theorems used in defining the TD error also apply for
state-action values.
\begin{equation}
\label{eq:qlearning}
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \bigl[r_{t+1} + \gamma\max_a
Q(s_{t+1},a)-Q(s_t,a_t) \bigr]
\end{equation}
The function is independant of the policy being followed and only requires that
all state-action pairs are continually updated.

\subsubsection{Eligibility Traces}
With the TD methods described above, only the value for the immediately
preceeding state or state-action pair is updated at each time step.  The
prediction $V(s_{t+1}$ also provides information converning earlier predictions
and TD methods can be extended to update a set of values at each step.  An
eligibility trace $e(s)$ represents how eligible the state $s$ is to receive
credit or blame for the TD error $\delta$.  Recall, that for state-value
prediction
\begin{equation}
\delta = r + \gamma V(s^\prime) - V(s)
\end{equation}
When extended with eligibility traces TD methods update values for all states
\begin{equation}
%V(s) \leftarrow V(s) + \gamma \delta e(s)
\Delta V_t(s) = \alpha \delta_t e_t(s)
\end{equation}
For the current state $e(s) \leftarrow e(s) + 1$ and for all states the
elegibility traces is attenuated by a factor $\lambda$, $e(s) \leftarrow \gamma
\lambda e(s)$, from which the extended TD methods TD($\lambda$), Q($\lambda$)
and Sarsa($\lambda$) derive their names.  For $\lambda = 0$ only the preceeding
value is updated is updated, as in the unextended definitions, and for $\lambda
= 1$ all preceeding state-values or state-action values are updated equally.

\subsubsection{Action Selection}

% \subsubsection{Q($\lambda$)}
% \label{sec:qlambda}
% In the Q-learning formulation, described in equation \ref{eq:qlearning}, only
% the quality associated with the previous state, $s_{jt}$, is updated.  However,
% the preceding states can also, in general, be said to be associated with the
% reward $r_{jt+1}$.  Eligibility traces are a mechanism for representing this
% effect and in algorithms such as Q($\lambda$), they are what the $\lambda$
% refers to.  The eligibility trace for a state $e(s)$ represents how eligible
% the state $s$ is to receive credit or blame for the error.  The term ``trace''
% refers to fact that only recently visited states become eligible.  The
% eligibility value for the current state is increased while for all other
% states it is attenuated by a factor $\lambda$.
%
% The off-policy nature of Q-learning requires special care to be taken when
% implementing eligibility traces.  While the algorithm may learn a greedy
% policy, in which the action with the maximum value would always be taken,
% typically a policy with some degree of exploration will be followed when
% choosing actions.  If an exploratory (pseudo-random) step is taken the
% preceding states can no longer be considered eligible for credit or blame.
% Setting $\lambda$ to $0$ for non-greedy actions removes much of the benefit of
% using eligibility traces if exploratory actions are frequent.  A solution to
% this has been developed, but requires a very complex implementation
% \cite{peng:1996}.  A na\"ive approach can be taken, where the effect of
% exploratory actions is ignored, but the results of this are unexplored.

\subsection{Policy Gradient Methods}
\label{sec:policygradient}
% The value-function methods defined in Section \ref{sec:valuebased} typically
% rely upon discretisation of the sensor and action spaces so the associated
% values may be stored in tables.  The memory requirements for this restrict the
% application of these methods to small environments.  Many environments,
% particularly from real applications, exhibit continuous sensor and/or action
% spaces and require generalisation techniques to be employed to provide a more
% compact policy representation.
%
% Policy-gradient methods use only the reward signal and search directly in the
% space of the policy parameters.  The agent's policy function approximator is
% updated according to the gradient of expected reward with respect to its
% parameters.
Value function based methods have been successfully applied with discrete lookup
table parameterisation to many problems [].  However, the number of discrete
states required increases exponentially as the dimensions of the state space
increase if all possibly relevant situations are to be covered and these
methods become subject to Bellman's Curse of Dimensionality
\cite{bellman:1961}.  Value function based methods can be used in conjunction
with function approximators, artificial neural networks are popular, to work
with continuous state and action space.  However, when used with value
function approximation they have been shown to offer poor convergence and even
divergence characteristics, even in simple systems.

These convergence problems have motivated research into policy gradient methods
which make small incremental changes to the parameters $\theta$ of a policy
function approximator.  With artificial neural networks, the parameters being
the weights of the network connections.  Policy gradient methods update
$\theta$ in the direction of the gradient of some policy performance measure
$Y$ with respect to the parameters
\begin{equation}
\theta_{i+1} = \theta_i + \alpha \frac{\partial Y}{\partial \theta_i}
\end{equation}
where $\alpha$ is a positive definite step size learning rate.

Aswell as working with continuous state and actions space, policy gradient
methods offer strong convergence guarantees, do not require all states to be
continually updated and althought uncertainty in state data can degrade policy
performance, the techniques need not be altered.

Policy gradient methods are differentiated largely by the techniques used to
obtain an estimate of the policy gradient $\frac{\partial Y}{\partial \theta}$.
The most successful real-world robotics results have been yielded using
REINFORCE likelihood ratio methods \cite{williams:reinforce} and natural policy
gradient methods such as Natual Actor-Critic \cite{peters:enac}.

% \subsubsection{REINFORCE}
% \label{sec:reinforce}
% REINFORCE is an associative reinforcement learning method that determines
% a policy by modifying the parameters of a policy function approximator, rather
% than approximating a value function \cite{williams:reinforce}.  Commonly,
% feedforward artificial neural networks are used to represent the policy, where
% the input is a representation of the state and the output is action selection
% probabilities.  In learning, a policy gradient approach is taken where the
% weights of the network are adjusted in the direction of the gradient of
% expected reinforcement.
%
% Defining the network, let $\mathbf{x}^i$ denote the vector of inputs to the
% $i$th unit and $y_i$ denote output of the unit.  In the input layer of the
% network the elements $x_j$ of $\mathbf{x}^i$ are normalised sensor values from
% the environment and in the output layer, or in any hidden layers, they are
% outputs from the $j$ unit in the preceding layer.  Let $\mathbf{w}^i$ denote
% the vector of the weights, $w_{ij}$, on the connections to the $i$th unit.  The
% output of the $i$th unit is dependant on the vector of inputs, $\mathbf{x}^i$,
% and the associated weights, $\mathbf{w}^i$.
%
% For each interaction of the agent with the environment, each parameter $w_{ij}$
% is incremented by
% \begin{equation}
% \label{eq:reinforce}
% \Delta w_{ij} = \alpha_{ij}(r - b_{ij})\frac{\partial\ln\rho_i}{\partial
% w_{ij}}
% \end{equation}
% where $\alpha_{ij}$ is the \textit{learning factor}, $b_{ij}$ is the
% \textit{reinforcement baseline} and $\rho_i$ is the performance of the policy
% (e.g., the average reward per interaction).
%
% \subsubsection{ENAC}
% \label{sec:enac}
% ToDo: Episodic Natural Actor Critic\cite{peters:enac}.


\subsection{Roth-Erev Method}
\label{sec:rotherev}
The Roth-Erev reinforcement learning algorithm uses a stateless policy to
select actions from a discrete domain \cite{roth:games,roth:aer}.  The dataset
stored by each agent, $j$, contains an array of length $K$, where $K$ is the
number of feasible actions $k$.  Each value in the array represents the
propensity for selection of the associated action in all states of the
environment.  Following interaction $t$ in which agent $j$ performed action
$k^\prime$ on its environment, for arbitrary positive $t$, a reward
$r_{jk^\prime}(t)$ is calculated.  The propensity for agent $j$ to select
action $k$ for interaction $t+1$ is
\begin{equation}
q_{jk}(t+1) =
\begin{cases}
(1-\phi)q_{ik}(t) + r_{jk^\prime}(t)(1-\epsilon), & \text{$k = k^\prime$} \\
(1-\phi)q_{ik}(t) + r_{jk^\prime}(t)(\frac{\epsilon}{K-1}), & \text{$k \ne
k^\prime$}
\end{cases}
\end{equation}
where $\phi$ and $\epsilon$ denote \textit{recency} and
\textit{experimentation} parameters, respectively.  The recency (forgetting)
parameter degrades the propensities for all actions and prevents the values
from going unbounded.  It is intended to represent the tendency for players to
forget older action choices and to prioritise more recent experience.  The
experimentation parameter prevents the probability of choosing an action from
going to zero and encourages exploration of the action space.

Erev and Roth proposed that actions be selected according to a discrete
probability distribution function where action $k$ is selected for interaction
$t+1$ with probability:

\begin{equation}
p_{jk}(t+1) = \frac{q_{jk}(t+1)}{\sum_{l=0}^K q_{jl}(t+1)}
\end{equation}

Since $\sum_{l=0}^K q_{jl}(t+1)$ increases with $t$, a reward $r_{jk}(t)$ for
performing action $k$ will have a greater effect on the probability
$p_{jk}(t+1)$ during early interactions while $t$ is small.  This is intended
to represent Psychology's \textit{Power Law of Practice} in which it is
qualitatively stated that, with practice, learning occurs at a decaying
expoential rate and that a learning curve will eventually flatten out.

This algorithm may alternatively use a form of the \textit{softmax} method
\cite{suttonbarto:1998} using the Gibbs, or Boltzmann, distribution to select
action $k$ for the $t+1$th interaction with probability

\begin{equation}
p_{jk}(t+1) = \frac{e^{q_{jk}(t+1)/\tau}}{\sum_{l=0}^K e^{q_{jl}(t+1)/\tau}}
\end{equation}

where $\tau$ is the \textit{temperature} parameter.  This parameter may be
lowered in value over the course of an experiment since high values give all
actions similar probability and encourage exploration of the action space,
while low values promote exploitation of past experience.

\subsubsection{Variant Roth-Erev Method}
\label{sec:variant}
Two shortcomings of the basic Roth-Erev algorithm have been identified and a
variant formulation proposed \cite{nicolaisen:2001}.  The two issues are
\begin{itemize}
  \item The values by which propensities are updated can be zero or very small
  for certain combinations of the experimentation parameter $\epsilon$ and
  the total number of feasible actions $K$ and
  \item all propensity values are decreased by the same amount when the reward,
  $r_{jk^\prime}(t)$ is zero.
\end{itemize}
With the variant algorithm, the propensity of agent $j$ to select action $k$
for interaction $t+1$ is:
\begin{equation}
q_{jk}(t+1) =
\begin{cases}
(1-\phi)q_{ik}(t) + r_{jk^\prime}(t)(1-\epsilon), & \text{$k = k^\prime$} \\
(1-\phi)q_{ik}(t) + q_{jk}(t)(\frac{\epsilon}{K-1}), & \text{$k \ne
k^\prime$}
\end{cases}
\end{equation}
As with the basic Roth-Erev algorithm, the propensity for selection of the
action that the reward is associated with is adjusted by the experimentation
parameter.  All other action propensities are adjusted by a small proportion of
their current value.
