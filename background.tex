\chapter{Background}
\label{ch:background}
This chapter provides background information on electricity market and
electrical power system simulation. Brief introductions to national electricity
supply and the history of UK wholesale electricity markets are given in order to
broadly define the systems that require modelling. Market simulation techniques
that account for the constraints of transmission systems are described and
definitions of the learning algorithms that are later used to model market
participant behaviour are provided.

\section{Electric Power Supply}

\ifthenelse{\boolean{includefigures}}{\input{tikz/structure}}{}

Generation and bulk movement of electricity in the UK takes place in a
three-phase alternating current (AC) power system.  The \textit{phases} are high
voltage, sinusoidal electrical waveforms, offset in time from each other by 120
degrees and oscillating at approximately 50Hz. Synchronous generators (sometimes
known as alternators), typically rotating at 3000 or 1500 revolutions per
minute, generate apparent power $S$ at a line voltage $V_l$ typically between
11kV and 25kV.  One of the principal reasons that AC, and not direct current
(DC), systems are common in electricity supply is that they allow power to be
transformed between voltages with very high efficiency. The output from a power
station is typically stepped-up to 275kV or 400kV for transmission over long
distances. The apparent power $s$ conducted by a three-phase transmission line
$l$ is the product of the line current $i_l$ and the line voltage $v_l$:
\begin{equation}
s = \sqrt{3} v_l i_l
\end{equation}
Therefore the line current is inversely proportional to the voltage at which
the power is transmitted. Ohmic heating losses $p_{r}$ are directly
proportional to the \textit{square} of the line current
\begin{equation}
p_{r} = 3 i_l^2 r_l
\end{equation}
where $r_l$ is the resistance of the transmission line.  Hence, any reduction in
line current dramatically reduces the amount of energy wasted through heating
losses.  One consequence of high voltages is the larger extent and integrity
of the insulation required between conductors, neutral and earth.  This is the
reason that transmission towers are typically large and undergrounding systems
is expensive.

The UK transmission system operates at 400kV and 275kV (and 132kV in Scotland),
but systems with voltages up to and beyond 1000kV are used in larger countries
such as Canada and China \cite{eletra:1000kV}.  For transmission over very long
distances or undersea, high voltage direct current (HVDC) systems have become
economically viable in recent years.  The reactance of a transmission line is
proportional to frequency, so one advantage of an HVDC system is that the
reactive power component is nil and more active power flow can be transmitted in
a line/cable of a certain diameter.

The ability to transform power between voltages and transmit large volumes of
power over long distances allows electricity generation to take place at high
capacity power stations, which offer economies of scale and lower operating
costs. It allows electricity to be transmitted across country borders and from
renewable energy systems, such as hydro-electric power stations, located in
remote areas.
% A HVDC interconnector between Folkstone in the UK and Sangatte in France
% allows upto 2GW of electricity to be imported/exported.  The Moyle HVDC
% interconnector can export upto 500MW from Auchencrosh in Scotland to
% Ballycronan More in Northern Ireland or import upto 80MW.  Further HVDC
% interconnectors are planned between England and the Netherlands and between
% Wales and The Republic of Ireland.
Figure \ref{fig:ngt_gen} shows how larger power stations in the UK are located
away from load centres and close to sources of fuel, such as the coal fields in
northern England and gas supply terminals near Cardiff and London.

%\ifthenelse{\boolean{includefigures}}{\input{tikz/ngt_gen}}{}
\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/ngt_gen}
	  \caption{UK power station locations.}
	  \label{fig:ngt_gen}
	\end{figure}
}{}

For delivery to most consumers, electric energy is transferred at a substation
from the transmission system to a distribution system. Distribution networks in
the UK are also three-phase AC power systems, but operate at lower voltages and
differ in their general structure (or topology) from transmission networks.
Transmission networks are typically highly interconnected, providing multiple
paths for power flow. Distribution networks in rural areas typically consist of
long radial feeders (usually overhead lines) and in urban areas, of many ring
circuits (usually cables). Three-phase transformers, that step the voltage down
to levels more convenient for general use (typically from 11kV or 33kV to 400V),
are spaced out on the feeders/rings. All three-phases at 400V may be provided
for industrial and commercial loads or individual phases at 230V supply typical
domestic and other commercial loads. Splitting of phases is usually planned so
that each is loaded equally. If achieved, this produces a balanced, symmetrical
system, with zero current flow on the neutral conductor, that can be analysed as
a \textit{single} phase circuit (See Section \ref{sec:pf_form} below).
Figure \ref{fig:powersystem} illustrates the basic structure of a typical
national electric power system \cite{blackout04}.

\section{Electricity Markets}
The UK was the first large country to privatise its electricity supply industry
when restructuring took place in 1990 \cite{newbery:2005}.  The approach adopted
has since been used as a model by other countries and the market structures that
the UK has implemented have utilised many of the main concepts for national
electricity market design.

The England and Wales Electricity Pool was created in 1990 to break up the
vertically integrated Central Electricity Generating Board (CEGB) and to
gradually introduce competition in generation and retail supply.
% Early adoption of electricity markets by the UK has lead to the country
% hosting many of the main European power and gas exchanges and the UK boasts a
% high degree of consumer switching compared to other European countries.
The Pool has since been replaced by trading arrangements in which market
outcomes are not centrally determined, but arise largely from bilateral
agreements between producers and suppliers.

\subsection{The England and Wales Electricity Pool}
\label{sec:thepool}
The Electric Lighting Act 1882 initiated the development of the UK's electricity
supply industry by permitting persons, companies and local authorities to set up
supply systems, principally at the time for the purposes of street lighting and
trams.  The Central Electricity Board started operating the first grid of
interconnected regional networks (synchronised at 132kV, 50Hz) in 1933. This
began operation as a national system five years later and was nationalised in
1947.  Over 600 electricity companies were merged in the process and the British
Electricity Authority was created.  It was later dissolved and replaced with the
CEGB and the Electricity Council under The Electricity Act 1957.  The CEGB was
responsible for planning the network and generating sufficient electricity until
the beginning of privatisation.

The UK electricity supply industry was privatised, and The England and Wales
Electricity Pool created, in March 1990.  Control of the transmission system was
transferred from the CEGB to the National Grid Company, which was originally
owned by twelve regional electricity companies and has since become publicly
listed.  The Pool was a multilateral contractual arrangement between generators
and suppliers and did not itself buy or sell electricity.  Competition in
generation was introduced gradually, by first entitling customers with
consumption greater than or equal to 1MW (approximately 45\% of the non-domestic
market \cite{decc:dukes09}) to purchase electricity form any listed supplier.
This limit was lowered in April 1994 to include customers with peak loads of
100kW or more.  Finally, between September 1998 and March 1999 the market was
opened to all customers.

Scheduling of generation was on a merit order basis (cheapest first) at a day
ahead stage and set a wholesale electricity price for each half-hour period of
the schedule day.  Forecasts of total demand in MW, based on historic data and
adjusted for factors such as the weather, for each settlement period were used
by generating companies and organisations with interconnects to the England and
Wales grid to formulate bids that had to be submitted to the grid operator by
10AM on the day before the schedule day.

\ifthenelse{\boolean{includefigures}}{\input{tikz/pool_bid}}{}

Figure \ref{fig:poolbids} illustrates four of the five price parameters that
would make up a bid.  A start-up price would also be stated, representing the
cost of turning on the generator from cold.  The no-load price $c_{0}$
represents the cost in pounds of keeping the generator running regardless of
output. Three incremental prices $c_1$, $c_2$ and $c_3$ specify the cost in
\pounds/MWh of generation between set-points $p_1$, $p_2$ and $p_3$.

A settlement algorithm would determine an unconstrained schedule (with no
account being taken for the physical limitations of the transmission system),
meeting the forecast demand and requirements for reserve while minimising cost.
Cheapest bids up to the marginal point would be accepted first and the bid price
from the marginal generator would generally determine the system marginal price
for each settlement period.  The system marginal price would form the basis of
the prices paid by consumers and paid to generators, which would be adjusted
such that the costs of transmission are covered by the market and that the
availability of capacity is encouraged at certain times.

Variations in demand and changes in plant availability would be accounted for by
the grid operator between day close and physical delivery, producing a
constrained schedule. Generators having submitted bids would be instructed to
increase or reduce production as appropriate.  Alternatively, the grid operator
could instruct large customers with contracts to curtail their demand or
generators contracted to provide ancillary services to adjust production. This
market performed effectively for 11 years.

\subsection{British Electricity Transmission and Trading Arrangements}
% \subsection{New Electricity Trading Arragements}
\label{sec:betta}
Concerns over the exploitation of market power in The England and Wales
Electricity Pool and over the ability of the market to reduce consumer
electricity prices prompted the introduction of New Electricity Trading
Arrangements (NETA) in March 2001 \cite{martoccia:2005}.  The aim was to improve
efficiency, price transparency and provide greater choice to participants.
Control of the Scottish transmission system was included with the introduction
of the nationwide British Electricity Transmission and Trading Arrangements
(BETTA) in April 2005 under The Energy Act 2004.  While The Pool operated a
single daily day-ahead auction and dispatched plant centrally, under the new
arrangements participants became self-dispatching and market positions became
determined through continuous bilateral trading between generators, suppliers,
traders and consumers.

The majority of power is traded under the BETTA through long-term contracts that
are customised to the requirements of each party \cite{kirschen:book}. These
instruments suit participants responsible for large power stations or those
purchasing large volumes of power for many customers.  Relatively, large amounts
of time and effort are typically required for these long-term contracts to be
initially formed and this results in a high associated transaction cost.
However, they reduce risk for large players and often include a degree of
flexibility.

Electric power is also traded directly between participants through
over-the-counter contracts that usually have a standardised form.  Such
contracts typically concern smaller volumes of power and have lower associated
transaction costs.  Often they are used by participants to refine their market
position ahead of delivery time \cite{kirschen:book}.

Additional trading facilities, such as power exchanges, provide a means for
participants to fine-tune their positions further, through short-term
transactions for often relatively small quantities of energy.  Modern exchanges,
such as APX, are computerised and accept anonymous offers and bids submitted
electronically.
% A submitted offer/bid will be paired with any outstanding bids/offers in the
% system with compatible price and quantity values.  The details are then
% displayed for traders to observe and to use in subsequent trading.

All bilateral trading must be completed before ``gate-closure'': a point in time
before delivery that gives the system operator an opportunity to balance supply
and demand and mitigate potential breaches of system limits.  In keeping with
the UK's free market philosophy, a competitive spot market \cite{schweppe:spot}
forms part of the balancing mechanism.  A generator that is not fully loaded may
offer a price at which it is willing to increase its output by a specified
quantity, stating the rate at which it is capable of doing so.  Certain loads
may also offer demand reductions at a price which can typically be implemented
very quickly.  Longer-term contracts for balancing services are also struck
between the system operator and generators/suppliers in order to avoid the price
volatility often associated with spot markets \cite{kirschen:book}.

% TODO: Section on international markets.

\section{Electricity Market Simulation}

Previous sections have shown the importance of electricity to modern societies
and explained how supply in the UK is entrusted, almost entirely, to
unadministered bilateral trade. It is not practical to experiment with
alternative trading arrangements on actual systems, but game theory (a branch of
applied mathematics that captures behaviour in strategic situations) can be used
to create simulations of market dynamics.  This typically involves modelling
trading systems and players as a closed-form mathematical optimisation problem
and observing states of equilibrium that form when the problem is solved.

In this thesis an alternative approach is taken in which each market entity is
modelled as an individual entity.  This section will describe the technique and
define an optimisation problem, called optimal power flow, that will be used to
model a central market/system operator agent.

% All aspects of electricity supply are constantly changing and electricity
% markets must be suitably researched to ensure that their designs are fit for
% purpose.  The value of electricity to society means that it is not practical
% to experiment with radical changes to trading arrangements on real systems.
% Game theory is the branch of applied mathematics in which behaviour in
% strategic situations is captured mathematically.  A common approach to doing
% this is to model the system and players as a mathematical optimisation
% problem.  Optimal power flow is a classic optimisation problem in the field of
% Electrical Power Engineering and variants of it are widely used in electricity
% market research. In this thesis, optimal power flow forms part of an
% \textit{agent-based} simulation: an alternative approach to the mathematics of
% games.

\subsection{Agent-Based Simulation}
Social systems, such as electricity markets, are inherently complex and involve
interactions between different types of individual and between individuals and
collective entities, such as organisations or groups, the behaviour of which is
itself the product of individual interactions \cite{rossiter:2010}.  This
complexity drives traditional closed-form equilibrium models to their limits
\cite{ehrenmann:2009}. The models are often highly stylised and limited to small
numbers of players with strong constraining assumptions made on their behaviour.

Agent-based simulation involves modelling the simultaneous operations of and
interactions between adaptive agents and then assessing their effect on the
system as a whole.  System properties arise from agent interactions, even those
with simple behavioural rules, that could not be deduced by simply aggregating
the agent's properties. % Game of Life

Following \citeA{tesfatsi:handbook}, the objectives of agent-based modelling
research fall roughly into four strands: empirical, normative, heuristic and
methodological. The \textit{empirical} objectives are to understand how and why
macro-level regularities have evolved from micro-level interactions when little
or no top-down control is present.  Research with \textit{normative} goals aims
to relate agent-based models to an ideal standard or optimal design.  The
objective being to evaluate proposed designs for social policy, institutions or
processes in their ability to produce socially desirable system performance. The
\textit{heuristic} strand aims to generate theories on the fundamental causal
mechanisms in social systems that can be observed when there are alternative
initial conditions.  This thesis aims to provide \textit{methodological}
advancement with respect to agent modelling research.  Improvements in the tools
and methods available can aid research with the former objectives.

\subsection{Optimal Power Flow}
\label{sec:opf}
Nationalised electricity supply industries were for many years planned, operated
and controlled centrally. A system operator would determine which generators
must operate and the required output of the operating units such that demand and
reserve requirements were met and the overall cost of production was minimised.
In electric power engineering, this is termed the \textit{unit commitment} and
\textit{economic dispatch} problem \cite{wood:pgoc}.

A formulation of the economic dispatch problem was published in 1962 that
incorporated electric power system constraints \cite{carpentier:opf}. This has
come to be known as the \textit{optimal power flow} problem and involves the
combination of economic and power flow aspects of power systems into one
mathematical optimisation problem.  The ability of optimal power flow to solve
centralised power system operation problems and to determine prices in
centralised power pool markets has resulted in it becoming one of the most
widely studied subjects in the electric power systems community.  Many solution
methods for optimal power flow have been developed since the problem was
introduced and a review of the main techniques can be found in
\citeA{momoh:part1,momoh:part2}.

\subsubsection{Power Flow Formulation}
\label{sec:pf_form}
Optimal power flow derives its name from the power flow (or load flow)
steady-state power system analysis technique \cite[\S18]{kallrath:2009}.  Given
data on the system loads, generation and the network, a power flow study
determines the complex voltage
\begin{equation}
\label{eq:ybus}
v_i = \vert v_i \vert \angle\delta_i = \vert
v_i\vert(\cos\delta_i + j\sin\delta_i)
\end{equation}
at each node $i$ in the power system, where $\delta_i$ is the voltage phase
angle at the node, from which line flows may be calculated \cite{wood:pgoc}.

% \paragraph{Nodal Admittance Matrix}
A network of $n_b$ buses is defined by a $n_b \times n_b$ nodal admittance
matrix $Y$ \cite{wood:pgoc,gloversarma:psad}.  The current injected into the
network at bus $f$ is
\begin{equation}
i_f = \sum_{t=1}^{n_b} y_{ft} v_t
\end{equation}
where $y_{ft} = \vert y_{ft}\vert \angle\theta_{ft}$ is the $(f,t)^{th}$
element of $Y$ and $v_t$ is voltage at bus $t$.
Typically, each bus can be assumed to be connected by either a transmission line
or a transformer. A nominal-$\pi$ model can be used to represent medium length
transmission lines \cite{grainger:psa} and transformers can be made
\textit{regulating} by introducing a winding ratio $N$ that can be changed in
response to the voltage on the load side \cite{crow:2009}. These two models may
be put in series to form a single model used for all branches
\cite{zimmerman:matpower}. The transmission line has total series admittance
$y_s = 1/(r_s+jx_s)$, where $r_s$ is the series resistance and $x_s$ is the
series reactance, and total shunt capacitance $b_c$. The winding ratio
may be complex, thus $N = \tau e^{j\theta_{shift}}$, where $\tau$ is the tap
ratio and $\theta_{shift}$ is the phase shift angle.

\ifthenelse{\boolean{includefigures}}{\input{tikz/branch_model}}{}

To understand how the nodal admittance matrix is formed from a network of branch
models, consider the system in Figure \ref{fig:branch_model} with one branch
connected from bus $f$ to bus $t$. According to Kirchhoff's Current Law, the
current in the series impedance is
\begin{equation}
\label{eq:iseries}
i_s = \frac{b_c}{2}v_t - i_t
\end{equation}
and from Kirchhoff's Voltage Law the voltage across the secondary winding of
the transformer is
\begin{equation}
\frac{v_{f}}{N} = v_t + \frac{i_s}{y_s}
\end{equation}
Substituting $i_s$ from equation (\ref{eq:iseries}), gives
\begin{equation}
\label{eq:vfrom}
\frac{v_{f}}{N} = v_t - \frac{i_t}{y_s} + v_t\frac{b_c}{2y_s}
\end{equation}
and rearranging in terms if $i_t$, gives
\begin{equation}
\label{eq:ito}
i_t = v_f \left( \frac{-y_s}{\tau e^{\theta_{shift}}} \right) +
v_t \left( y_s + \frac{b_c}{2} \right)
\end{equation}
The current through the secondary winding of the transformer is
\begin{equation}
N^*i_f = i_s + \frac{b_c}{2}\frac{v_{f}}{N}
\end{equation}
Substituting $i_f$ from equation (\ref{eq:iseries}) again, gives
\begin{equation}
N^*i_f = \frac{b_c}{2}v_t - i_t + \frac{b_c}{2}\frac{v_{f}}{N}
\end{equation}
and substituting $\frac{v_{f}}{N}$ from equation (\ref{eq:vfrom}) and
rearranging in terms if $i_f$, gives
\begin{equation}
\label{eq:ifrom}
i_s = v_f \left( \frac{1}{\tau^2} \left(y_s + \frac{b_c}{2}\right) \right) +
v_t \left(\frac{y_s}{\tau e^{-j\theta}}\right)
\end{equation}
Observing equations (\ref{eq:ito}) and (\ref{eq:ifrom}), the
off-diagonal elements of $Y$ are
\begin{eqnarray}
y_{ft}& =& \frac{y_s}{\tau e^{-j\theta_{shift}}}\\
y_{tf}& =& \frac{-y_s}{\tau e^{j\theta_{shift}}}
\end{eqnarray}
and the diagonal elements of $Y$ are:
\begin{eqnarray}
y_{ff}& =& \frac{1}{\tau^2} \left(y_s + \frac{b_c}{2}\right)\\
y_{tt}& =& y_s + \frac{b_c}{2}
\end{eqnarray}
% the \textit{from} and
% \textit{to} end complex current injections for branch $l$ are
% \begin{equation}
% \label{eq:ybranch}
% \begin{bmatrix}
% i_f^l\\
% i_t^l
% \end{bmatrix}
% =
% \begin{bmatrix}
% y_{ff}^l& y_{ft}^l\\
% y_{tf}^l& y_{tt}^l
% \end{bmatrix}
% \begin{bmatrix}
% v_f^l\\
% v_t^l
% \end{bmatrix}
% \end{equation}
% where
% \begin{eqnarray}
% \label{eq:yff}
% y_{ff}^l& =& \frac{1}{\tau^2} \left(y_s + \frac{b_c}{2}\right)\\
% \label{eq:yft}
% y_{ft}^l& =& \frac{y_s}{\tau e^{-j\theta_{shift}}}\\
% \label{eq:ytf}
% y_{tf}^l& =& \frac{-y_s}{\tau e^{j\theta_{shift}}}\\
% \label{eq:ytt}
% y_{tt}^l& =& y_s + \frac{b_c}{2}
% \end{eqnarray}
% Let $Y_{ff}$, $Y_{ft}$, $Y_{tf}$ and $Y_{tt}$ be vectors of length $n_l$ where
% the $l^{th}$ element of each corresponds to $y_{ff}^l$, $y_{ft}^l$, $y_{tf}^l$
% and $y_{tt}^l$, respectively.  Furthermore, let $C_f$ and $C_t$ be the $n_l
% \times n_b$ branch-bus connection matrices, where $C_{fij} = 1$ and $C_{tik} =
% 1$ if branch $i$ connects from bus $j$ to bus $k$.
% Following \citeA[p.12]{pserc:mp_manual}, the $n_l \times n_b$ branch admittance
% matrices are
% \begin{eqnarray}
% Y_f& =& \diag(Y_{ff})C_f + \diag(Y_{ft})C_t\\
% Y_t& =& \diag(Y_{tf})C_f + \diag(Y_{tt})C_t
% \end{eqnarray}
% and the
% $n_b \times n_b$ nodal admittance matrix is
% \begin{eqnarray}
% Y_{bus}& =& C_f^\mathsf{T} Y_f + C_t^\mathsf{T} Y_t .
% \end{eqnarray}
% and it relates the complex bus voltages to the nodal current injections
% \begin{eqnarray}
% I_{bus}& =& Y_{bus}V
% \end{eqnarray}
% The complex bus power injections are expressed as a non-linear function of $V$
% \begin{eqnarray}
% S_{bus}(V)& =& \diag(V)I_{bus}^* \nonumber \\
% \label{eq:sbus}
% &= & \diag(V)Y_{bus}^*V^*
% \end{eqnarray}
% The net complex power injection (generation - load) at each bus must equal the
% sum of complex power flows on each branch connected to the bus.  Hence the AC
% power balance equations are
% \begin{equation}
% \label{eq:mismatch}
% S_{bus}(V) + S_d - S_g = 0
% \end{equation}
%\paragraph{Power Balance}
% Importantly, the relationship between nodal voltages and power entering the
% network is non-linear.
Recalling equation (\ref{eq:ybus}), the apparent power entering the network at
bus $f$ is
\begin{equation}
s_f = p_f+jq_f = v_f i_f^* = \sum_{t=1}^{n_b} \vert y_{ft}v_f v_t \vert \angle
(\delta_f - \delta_t - \theta_{ft})
\end{equation}
Converting to polar coordinates and separating the real and imaginary parts,
the active power
\begin{equation}
p_f = \sum_{t=1}^{n_b} \vert y_{ft}v_f v_t \vert \cos(\delta_f - \delta_t -
\theta_{ft})
\end{equation}
and the reactive power
\begin{equation}
q_f = \sum_{t=1}^{n_b} \vert y_{ft}v_f v_t \vert \sin(\delta_f - \delta_t -
\theta_{ft})
\end{equation}
entering the network at bus $f$ are non-linear functions of voltage, as
indicated by the presence of the sine and cosine terms.
Kirchhoff's Current Law requires that the net complex power injection
(generation - demand) at each bus must equal the sum of complex power flows on
each branch connected to the bus.  The power balance equations
\begin{equation}
\label{eq:p_balance}
p_g^f - p_d^f = p^f
\end{equation}
and
\begin{equation}
\label{eq:q_balance}
q_g^f - q_d^f = q^f,
\end{equation}
where the subscripts $g$ and $d$ indicate generation and demand
respectively, form the principal non-linear constraints in the optimal power
flow problem.

% \paragraph{DC Power Flow}
% The power balance equations may be linearised and problem of solving them
% greatly simplified if the following additional assumptions are made
% \cite[p.14]{pserc:mp_manual}:
% \begin{itemize}
%   \item The resistance $r_s$ and shunt capacitance $b_c$ of all branches can be
%   considered negligible.
%   \begin{equation}
%   \label{eq:lossless}
%   y_s \approx \frac{1}{jx_s}, \quad b_c \approx 0
%   \end{equation}
%   \item Bus voltage magnitudes $v_{m,i}$ are all approximately 1 per-unit.
%   \begin{equation}
%   \label{eq:oneperunit}
%   v_i \approx 1e^{j\theta_i}
%   \end{equation}
%   \item The voltage angle difference between bus $i$ and bus $j$ is small enough
%   that
%   \begin{equation}
%   \label{eq:busangdiff}
%   \sin\theta_{ij} \approx \theta_{ij}
%   \end{equation}
% \end{itemize}
% Applying the assumption that branches are lossless from equation
% (\ref{eq:lossless}), the quadrants of the branch admittance matrix in equations
% (\ref{eq:yff}), (\ref{eq:yft}), (\ref{eq:ytf}) and (\ref{eq:ytt}), approximate
% to
% \begin{eqnarray}
% y_{ff}^l& =& \frac{1}{jx_s \tau^2}\\
% y_{ft}^l& =& \frac{-1}{jx_s \tau e^{-j\theta_{shift}}}\\
% y_{tf}^l& =& \frac{-1}{jx_s \tau e^{j\theta_{shift}}}\\
% y_{tt}^l& =& \frac{1}{jx_s}
% \end{eqnarray}
% respectively.  Applying the uniform bus voltage magnitude assumption from
% equation (\ref{eq:oneperunit}) to equation (\ref{eq:ybranch}), the branch
% ``from'' end current approximates to
% \begin{eqnarray}
% i_f& \approx& \frac{e^{j\theta_f}}{jx_s\tau^2} -
% \frac{e^{j\theta_t}}{jx_s \tau e^{-j\theta_{shift}}}\\
% & =& \frac{1}{jx_s\tau} ( \frac{1}{\tau}e^{j\theta_f} -
% e^{j(\theta_t + \theta_{shift})} )
% \end{eqnarray}
% % ToDo: Branch to end current derivation.
% and the branch ``from'' end complex power flow $s_f = v_f \cdot i_f^*$
% approximates to
% \begin{eqnarray}
% s_f& \approx& e^{j\theta_f} \cdot \frac{j}{x_s\tau}
% (\frac{1}{\tau}e^{-j\theta_f} - e^{j(\theta_t + \theta_{shift})})\\
% & =& \frac{1}{x_s\tau} \left[ \sin(\theta_f-\theta_t-\theta_{shift}) +
% j\left( \frac{1}{\tau} - \cos(\theta_f-\theta_t-\theta_{shift}) \right) \right]
% \end{eqnarray}
% Applying the voltage angle difference assumption from equation
% (\ref{eq:busangdiff}) yields the approximation
% \begin{equation}
% p_f \approx \frac{1}{x_s\tau}(\theta_f-\theta_t-\theta_{shift})
% \end{equation}
% Let $B_{ff}$ and $P_{f,ph}$ be the $n_l \times 1$ vectors where
% $B_{ff_i} = 1 / (x_s^i\tau^i)$ and $P_{f,ph_i} =
% -\theta_{shift}^i / (x_s^i\tau^i)$.  Then if the system $B$ matrices are
% \begin{eqnarray}
% B_f& =& \diag(B_{ff})(C_f-C_t)\\
% B_{bus}& = &(C_f-C_t)^\mathsf{T}B_f
% \end{eqnarray}
% then the real power bus injections are
% \begin{equation}
% \label{eq:bbus}
% P_{bus}(\Theta) = B_{bus}\Theta + P_{bus,ph}
% \end{equation}
% where $\Theta$ is the $n_b \times 1$ vector of bus voltage angles and
% \begin{equation}
% P_{bus,ph} = (C_f-C_t)^\mathsf{T} + P_{f,ph}
% \end{equation}
% The active power flows at the branch ``from'' ends are
% \begin{equation}
% \label{eq:pf_loss}
% P_f(\Theta) = B_f\Theta + P_{f,ph}
% \end{equation}
% and $P_t = -P_f$ since all branches are assumed lossless.

\subsubsection{Optimal Power Flow Formulation}
Optimal power flow is a mathematical optimisation problem constrained by the
complex power balance equations (\ref{eq:p_balance}) and (\ref{eq:q_balance}).
Mathematical optimisation problems have the general form
\begin{equation}
\min_x f(x)
\end{equation}
subject to
\begin{eqnarray}
\label{eq:equality}
g(x)& =& 0\\
\label{eq:inequality}
h(x)& \leq& 0
\end{eqnarray}
where $x$ is the vector of optimisation variables, $f$ is the objective
function and equations (\ref{eq:equality}) and (\ref{eq:inequality}) are sets
of equality and inequality constraints on $x$, respectively.

In optimal power flow, typical inequality constraints are bus voltage magnitude
contingency state limits, generator output limits and branch power or current
flow limits.  The vector of optimisation variables $x$ may consist of generator
set-points, bus voltages, transformer tap settings etc.  If $x$ is empty then
the formulation reduces to the general power flow problem described above.

% Branch complex power flow limits $S_{max}$ are enforced by the inequality
% constraints
% \begin{eqnarray}
% \abs{S_f(V)} - S_{max}& \leq &0\\
% \abs{S_f(V)} - S_{max}& \leq &0
% \end{eqnarray}
% and the reference bus voltage angle $\theta_i$ is fixed with the equality
% constraint
% \begin{equation}
% \label{eq:refbusang}
% \theta_i^{ref} \leq \theta_i \leq \theta_i^{ref}, \quad i \in \mathcal{I}_{ref}
% \end{equation}
% Upper and lower limits on the optimisation variables $V_m$, $P_g$ and $Q_g$ are
% enforced by the inequality constraints
% \begin{eqnarray}
% v_m^{i,min} \leq v_m^i \leq v_m^{i,max},& \quad i= 1 \dotsc n_b&\\
% \label{eq:pglim}
% p_g^{i,min} \leq p_g^i \leq p_q^{i,max},& \quad i= 1 \dotsc n_g&\\
% q_g^{i,min} \leq q_g^i \leq q_q^{i,max},& \quad i= 1 \dotsc n_g&
% \end{eqnarray}

A common objective in the optimal power flow problem is total system cost
minimisation. For a network of $n_g$ generators the objective function is
\begin{equation}
\label{eq:objfunc}
\min_{\theta, V_m, P_g, Q_g} \sum_{k=1}^{n_g} c^k_P(p_g^k) + c_Q^k(q_g^k)
\end{equation}
where $c_P^k$ and $c_Q^k$ are cost functions (typically quadratic) of the
real power set-point $p_g^k$ and reactive power set-point $q_g^k$ for generator
$k$, respectively. Alternative objectives may be to minimise losses, maximise
the voltage stability margin or minimise deviation of an optimisation variable
from a particular schedule \cite[\S18]{kallrath:2009}.

\subsubsection{Nodal Marginal Prices}
One of the most robust solution strategies for optimal power flow is to solve
the Lagrangian function
\begin{equation}
\mathcal{L}(x) = f(x) + \lambda^\mathsf{T}g(x) + \mu^\mathsf{T}h(x),
\end{equation}
where $\lambda$ and $\mu$ are vectors of Lagrangian multipliers, using an
Interior Point Method \cite{cvxopt:2004,zimmerman:ccv}.  When solved, the
Lagrangian multiplier for a constraint gives the rate of change of the objective
function value with respect to the constraint variable.  If the objective
function is equation (\ref{eq:objfunc}), the Lagrangian multipliers
$\lambda^f_P$ and $\lambda^f_Q$ for the power balance constraint at each bus
$f$, given by equations (\ref{eq:p_balance}) and (\ref{eq:q_balance}), are the
nodal marginal prices and can be interpreted as the increase in the total
system cost for an additional injection at bus $f$ of 1MW or 1MVAr,
respectively.

For a case in which none of the inequality constraints $h(x)$ (such as branch
power flow or bus voltage limits) are binding, the nodal marginal prices are
uniform across all buses and equal the cost of the marginal generating unit.
When the constraints \textit{are} binding, the nodal marginal prices are
elevated for buses at which adjustments to power injection are required for the
constraints to be satisfied.  Nodal marginal prices are commonly used in
agent-based electricity market simulation to determine the revenue for
generating units as they reflect the increased value of production in
constrained areas of the power system.

\subsection{Summary}
The agent-based market simulation approach used in this thesis is an alternative
to traditional closed-form equilibrium analysis that has the potential to scale
to much larger problems. It is a ``bottom-up'' approach in which each market
participant is modelled as an individual that must develop a strategy for
selecting the price and quantity of power to be bought or sold.  Cost functions
and generator capacity limits can be derived from these choices and used in an
optimal power flow problem that can represent the process of a system operator
minimising total system cost while adhering to system constraints. Developing an
optimal bidding strategy in a competitive environment is a non-trivial task and
requires advanced adaptive algorithms from the field of artificial intelligence.

\newpage
\section{Reinforcement Learning}
\label{sec:rl}
% The problem of learning how best to interact with an environment so as to
% maximise some long-term reward is a general one.  Reinforcement learning is a
% term from the field of machine learning that is typically applied to
% understanding, automating and solving this problem through adaptive
% computational approaches.  Unlike many machine learning techinques, the
% algorithms are not instructed as to which actions to take, but must learn to
% maximise the long-term reward through trial-and-error.

% Reinforcement learning starts with an interactive, goal-seeking individual
% agent existing in an environment.  The agent requires the ability to;
% \begin{itemize} \item Sense aspects of its environment, \item Perform actions
% that influence the state of its environment and \item be assigned rewards as a
% response to their chosen action. \end{itemize} An agent follows a particular
% \textit{policy} when mapping the perceived state of its environment to an
% action choice.  Reinforcement learning methods adjust the agent's policy.
Reinforcement learning is learning from reward by mapping situations to actions
when interacting with an uncertain environment
\cite{suttonbarto:1998,bertsekas:96,kaelbling:1996}.  An individual learns
\textit{what} to do in order to achieve a task through trial-and-error using a
numerical reward or a penalty signal without being instructed \textit{how} to
achieve it.  Some actions may not yield immediate reward or may effect the next
situation and all subsequent rewards.  A compromise must be made between the
exploitation of past experiences and the exploration of the environment through
new action choices. In reinforcement learning an agent must be able to:
\begin{itemize}
  \item Sense aspects of its environment,
  \item Take actions that influence its environment and,
  \item Have an explicit goal or set of goals relating to the state of its
  environment.
\end{itemize}

In the classical model of agent-environment interaction \cite{suttonbarto:1998},
at each time step $t$ in a sequence of discrete time steps $t = 1,2,3\dotsc$ an
agent receives as input some form of the environment's state $s_t \in
\mathscr{S}$, where $\mathscr{S}$ is the set of possible states.  From a set of
actions $\mathscr{A}(s_t)$ available to the agent in state $s_t$ the agent
selects an action $a_t$ and performs it in its environment.  The environment
enters a new state $s_{t+1}$ in the next time step and the agent receives a
scalar numerical reward $r_{t+1} \in \mathbb{R}$ in part as a result of its
action. The agent then learns from the state representation, the chosen action
$a_t$ and the reinforcement signal $r_{t+1}$ before beginning its next
interaction.  Figure \ref{fig:seq_rl} defines the classical agent-environment
interaction process in reinforcement learning using a UML (Unified Modeling
Language) sequence diagram \cite{alhir:1998}.

\ifthenelse{\boolean{includefigures}}{\input{tikz/seq_rl}}{}

% \subsection{Markov Decision Processes}
For a finite number of states, if all states are Markov, the agent is
interacting with a finite Markov decision process (MDP)
\cite{howard:1964,aima:2003}. Informally, for a state to be Markov it must
retain all relevant information about the complete sequence of positions leading
up to the state, such that all future states and expected rewards can be
predicted as well as would be possible given a complete history
\cite{suttonbarto:1998}.  A particular MDP is defined for a discrete set of time
steps by a state set $\mathscr{S}$, an action set $\mathscr{A}$, a set of state
transition probabilities $\mathscr{P}$ and a set of expected reward values
$\mathscr{R}$.
% Given a state $s$ and an action $a$, the probability of transitioning to each
% possible next state $s^\prime$ is \begin{equation} \mathscr{P}^a_{ss^\prime} =
% \Pr \bigl\lbrace s_{t+1} = s^\prime \vert s_t=s, a_t=a \bigr\rbrace .
% \end{equation} Given the next state $s^\prime$, the expected value of the next
% reward is \begin{equation} \mathscr{R}^a_{ss^\prime} = E \bigl\lbrace r_{t+1}
% \vert s_t=s, a_t=a, s_{t+1}=s^\prime \bigr\rbrace . \end{equation}
In practice not all state signals are Markov, but they should provide a good
basis for predicting subsequent states, future rewards and selecting actions.

If the state transition probabilities and expected reward values are not known,
only the states and actions, then samples from the MDP must be taken and a value
function approximated iteratively based on new experiences generated by
performing actions.

\subsection{Value Function Methods}
\label{sec:valuebased}
% Value-based methods attempt to find the optimal policy by approximating a
% \textit{value-function} which returns the total reward an agent can expect to
% accumulate, given an initial state and following the current policy
% thereafter.  The policy is adjusted using the reinforcing signal and
% information from previous action choices.
Any method that can optimise control of a MDP may be considered a reinforcement
learning method.  All agents have a policy $\pi$ for selecting actions given the
state of the environment. Reinforcement learning methods search for an optimal
policy $\pi^*$ that maximises the sum of rewards over the agents lifetime.

Each state $s$ under policy $\pi$ may be associated with a \textit{value}
$V^\pi(s)$ equal to the expected return from following policy $\pi$ from state
$s$.  Most reinforcement learning methods are based on estimating the
state-value function
\begin{equation}
\label{eq:statevalue}
V^\pi(s) = E \Bigg\lbrace \sum^\infty_{t=0} \gamma^t r_t \Bigg\vert s_0 = s
\Bigg\rbrace
\end{equation}
where $\gamma$ is a discount factor, with $0\leq \gamma \leq 1$ and $E$
indicates that it is an estimate \cite{suttonbarto:1998}. Performing certain
actions may result in no state change, creating a loop and causing the value of
that action to be infinite for certain policies. The discount factor $\gamma$
prevents values from going unbounded and represents reduced trust in the reward
$r_t$ as discrete time $t$ increases.  Each action $a$ in state $s$ may also
be associated with a certain quality $Q^\pi(s,a)$ and many reinforcement
learning methods estimate this action-value function
\begin{equation}
\label{eq:actionvalue}
Q^\pi(s,a) = E \Bigg\lbrace \sum^\infty_{t=0} \gamma^t r_t \Bigg\vert s_0 = s,
a_0 = a \Bigg\rbrace
\end{equation}
which defines the quality of an action $a$ in state $s$ under fixed policy
$\pi$ \cite{suttonbarto:1998}.

\subsubsection{Temporal-Difference Learning}
Temporal Difference (TD) learning is a fundamental concept in reinforcement
learning that was introduced by \citeA{sutton:1998}. TD methods do not attempt
to estimate the state transition probabilities and expected rewards of the
finite MDP, but estimate the value function directly. They learn to
\textit{predict} the expected value of total reward returned by the state-value
function (\ref{eq:statevalue}).  For an exploratory policy $\pi$ and a
non-terminal state $s$, an estimate of $V^\pi(s_t)$ at any given time step $t$
is updated using the estimate at the next time step $V^\pi(s_{t+1})$ and the
observed reward $r_{t+1}$
\begin{equation}
V^\pi(s_t) = V^\pi(s_t) + \alpha \bigl[r_{t+1} + \gamma
V^\pi(s_{t+1}) - V^\pi(s_t) \bigr]
\end{equation}
where $\alpha$ is the learning rate, with $0 \leq \alpha \leq 1$, which controls
how much attention is paid to new data when updating $V^\pi$.  Plain TD
learning evaluates a particular policy and offers strong convergence
guarantees, but does not learn better policies.

\subsubsection{Q-Learning}
\label{sec:qlearning}
% This section describes the original off-policy Temporal Difference Q-learning
% method developed by Watkins \cite{watkins:1989}.  The action-value function,
% $Q(s,a)$, returns values from a $M \times N$ matrix where $M$ and $N$ are
% arbitrary positive numbers equal to the total number of feasible states and
% actions, respectively.  Each value represents the \textit{quality} of taking a
% particular action, $a$, in state $s$.  Actions are selected using either the
% $\epsilon$-greedy or softmax (See Section \ref{sec:rotherev}, above) methods.
% The $\epsilon$-greedy method either selects the action (or one of the actions)
% with the highest estimated value or it selects an action at random, uniformly,
% independently of the estimated values with (typically small) probability
% $\epsilon$.
%
% Agent $j$ will observe a reward, $r_{jt}$, and a new state, $s_{jt+1}$,
% after taking action $a_{jt}$ at step $t$ when in state $s_{jt}$.  The
% state-action value, $Q_j(s_{jt},a_{jt})$, is updated according to the
% maximum value of available actions in state $s_{t+1}$ and becomes
% \begin{equation}
% \label{eq:qlearning}
% Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma\max_{a} Q_j(s_{jt+1},a_{jt}) -
% Q_j(s_{jt},a_{jt})]
% \end{equation}
% where $\alpha$ and $\gamma$ are the learning rate, $0\leq\alpha\leq1$, and
% discount factor, $0\leq\gamma\leq1$, respectively.  The learning rate
% determines the extent to which new rewards will override the effect of older
% rewards.  The discount factor allows the balance between maximising immediate
% rewards and future rewards to be set.
Q-learning \cite{watkins:1989} is an off-policy TD method that does not estimate
the finite MDP directly, but iteratively approximates a state-action value
function which returns the value of taking action $a$ in state $s$ and
following an \textit{optimal} policy thereafter. The same theorems used in
defining the TD error also apply for state-action values that are updated
according to
\begin{equation}
\label{eq:qlearning}
Q(s_t,a_t) = Q(s_t,a_t) + \alpha \bigl[r_{t+1} + \gamma\max_a
Q(s_{t+1},a)-Q(s_t,a_t) \bigr].
\end{equation}
The method is off-policy since the update function is independent of the policy
being followed and only requires that all state-action pairs be continually
updated.

\subsubsection{Sarsa}
\label{sec:sarsa}
% The SARSA algorithm is an on-policy Temporal Difference control method where
% the policy is typically represented by a $M \times N$ table, where $M$ and $N$
% are arbitrary positive numbers equal to the total number of feasible states
% and actions respectively.  The action-value update for agent $j$ is defined by
% \begin{equation} Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma
% Q_j(s_{jt+1},a_{jt+1}) - Q_j(s_{jt},a_{jt})] \end{equation} While the
% Q-learning method (See Section \ref{sec:qlearning}, below) updates
% action-values using a greedy policy, which is a different policy to that being
% followed, SARSA uses the discounted future reward of the next state-action
% observation following the original policy.
Sarsa (or modified Q-learning) is an on-policy TD control method that
approximates the state-action value function in equation (\ref{eq:actionvalue})
\cite{rummery:1994}. Recall that the state-action value function for an agent
returns the total expected reward for following a particular policy for
selecting actions as a function of future states.  The function is updated
according to the rule
\begin{equation}
\label{eq:sarsa}
Q(s_t,a_t) = Q(s_t,a_t) + \alpha \bigl[r_{t+1} + \gamma
Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\bigr].
\end{equation}
This update also uses the action from the next time step $a_{t+1}$ and the
requirement to transition through state-action-reward-state-action for each
time step gives the algorithm its name.  Sarsa is referred to
as an on-policy method since it learns the same policy that it follows.

\subsubsection{Eligibility Traces}
\label{sec:eligibility}
With the TD methods described above, only the value for the immediately
preceding state or state-action pair is updated at each time step.  However, the
prediction $V(s_{t+1})$ also provides information concerning earlier predictions
and TD methods can be extended to update a set of values at each step.  An
eligibility trace $e(s)$ \cite{tanner:2005} represents how eligible the state
$s$ is to receive credit or blame for the TD error
\begin{equation}
\delta = r_{t+1} + \gamma V^\pi(s_{t+1}) - V^\pi(s_t).
\end{equation}
When extended with eligibility traces TD methods update values for all states
\begin{equation}
%V(s) \leftarrow V(s) + \gamma \delta e(s)
\Delta V_t(s) = \alpha \delta_t e_t(s)
\end{equation}
For the current state $e(s) = e(s) + 1$ and for all states
$e(s) = \gamma \lambda e(s)$ where $\lambda$ is the eligibility trace
attenuation factor from which the extended TD methods TD($\lambda$),
Q($\lambda$) and Sarsa($\lambda$) derive their names. For $\lambda = 0$ only
the preceding value is updated, as in the unextended definitions,
and for $\lambda = 1$ all preceding state-values or state-action values are
updated equally.

\subsubsection{Action Selection}
\label{sec:actionselection}
A balance between exploration of the environment and exploitation of past
experience must be struck when selecting actions.  The $\epsilon$-greedy
approach to action selection is defined by a randomness parameter $\epsilon$ and
a decay parameter $d$ \cite{rivest:1990}.  A random number $x_r$ where $0 \leq
x_r \leq 1$ is drawn for each selection.  If $x_r < \epsilon$ then a random
action is selected, otherwise the perceived optimal action is chosen. After each
selection the randomness is attenuated by $d$.

Action selection may also be accomplished using a form of the \textit{softmax}
method \cite[\S2]{suttonbarto:1998} using the Gibbs (or
Boltzmann) distribution to select action $a$ for the $t^{th}$ interaction in
state $s$ with probability
\begin{equation}
\frac{e^{Q_{t}(s,a)/\tau}}{\sum_{b=1}^n e^{Q_{t}(s,b)/\tau}}
\end{equation}
for $n$ possible actions, where $\tau$ is the \textit{temperature} parameter.
This parameter may be lowered in value over the course of an experiment since
high values give all actions similar probability and encourage exploration of
the action space, while low values promote exploitation of past experience.

% \subsubsection{Q($\lambda$)}
% \label{sec:qlambda}
% In the Q-learning formulation, described in equation \ref{eq:qlearning}, only
% the quality associated with the previous state, $s_{jt}$, is updated.  However,
% the preceding states can also, in general, be said to be associated with the
% reward $r_{jt+1}$.  Eligibility traces are a mechanism for representing this
% effect and in algorithms such as Q($\lambda$), they are what the $\lambda$
% refers to.  The eligibility trace for a state $e(s)$ represents how eligible
% the state $s$ is to receive credit or blame for the error.  The term ``trace''
% refers to fact that only recently visited states become eligible.  The
% eligibility value for the current state is increased while for all other
% states it is attenuated by a factor $\lambda$.
%
% The off-policy nature of Q-learning requires special care to be taken when
% implementing eligibility traces.  While the algorithm may learn a greedy
% policy, in which the action with the maximum value would always be taken,
% typically a policy with some degree of exploration will be followed when
% choosing actions.  If an exploratory (pseudo-random) step is taken the
% preceding states can no longer be considered eligible for credit or blame.
% Setting $\lambda$ to $0$ for non-greedy actions removes much of the benefit of
% using eligibility traces if exploratory actions are frequent.  A solution to
% this has been developed, but requires a very complex implementation
% \cite{peng:1996}.  A na\"ive approach can be taken, where the effect of
% exploratory actions is ignored, but the results of this are unexplored.

\subsection{Policy Gradient Methods}
\label{sec:policygradient}
% The value-function methods defined in Section \ref{sec:valuebased} typically
% rely upon discretisation of the sensor and action spaces so the associated
% values may be stored in tables.  The memory requirements for this restrict the
% application of these methods to small environments.  Many environments,
% particularly from real applications, exhibit continuous sensor and/or action
% spaces and require generalisation techniques to be employed to provide a more
% compact policy representation.  Policy-gradient methods use only the reward
% signal and search directly in the space of the policy parameters.  The agent's
% policy function approximator is updated according to the gradient of expected
% reward with respect to its parameters.
Value function based methods have been successfully applied with discrete
look-up table parameterisation to many problems \cite{kaelbling:1996}.  However,
the number of discrete states required increases rapidly as the dimensions of
the problem increase. Value function based methods can be used in conjunction
with function approximation techniques, such as artificial neural networks, to
allow operation with continuous state and action spaces \cite{sutton:1996}.
However, greedy action selection has been shown to cause these methods to
exhibit poor convergence or divergence characteristics, even in simple systems
\cite{tsitsiklis:94,peters:enac,gordon:95,baird:95}.

These convergence problems have motivated research into alternative learning
methods, such as policy gradient methods, that can operate successfully with
function approximators \cite{peters:enac}. Policy gradient algorithms make small
incremental changes to the parameter vector $\theta$ of a policy function
approximator \cite{sutton:2000}. Typically artificial neural networks are used
where the parameters are the weights of the network connections. However, other
forms of generalising function approximator, such as decision trees or
instance-based methods, may be used \cite{barto:policy}.  Policy gradient
methods update $\theta$ in the direction of steepest ascent of some policy
performance measure $Y$ with respect to the parameters
\begin{equation}
\theta_{t+1} = \theta_t + \alpha \frac{\partial Y}{\partial \theta_t}
\end{equation}
where $\alpha$ is a positive definite step size learning rate.  Unlike look-up
table based methods, they do not require all states to be continually updated.
Uncertainty in state data can degrade policy performance, but these methods
generally have strong convergence properties \cite{sutton:2000}.

Policy gradient methods are differentiated largely by the techniques used to
obtain an estimate of the policy gradient $\partial Y / \partial \theta$. Some
of the most successful real-world robotics results
\cite{shaal:robots,benbrahim:1996} have been yielded by likelihood ratio methods
\cite{glynn87,aleksandrov68} such as Williams' \textsc{Reinforce}
\cite{williams:reinforce} and natural policy gradient methods, such as the
Episodic Natural Actor-Critic (ENAC) \cite{peters:enac}. These algorithms have
lengthy derivations, but \citeA{petersScholar} provides a concise overview.

\subsubsection{Artificial Neural Networks}
Artificial neural networks are mathematical models that mimic aspects of
biological neural networks, such as the human brain, and are widely used in
supervised learning applications \cite{bishop96ann,fausett94}. In reinforcement
learning, the most widely used type of artificial neural network is the
multi-layer feed-forward network (or multi-layer perceptron). This model
consists of an input layer and an output layer of artificial neurons, plus any
number of optional hidden layers.  Weighted connections link neurons, but unlike
architectures such as the recurrent neural network, only neurons from adjacent
layers are connected.  Most commonly, a fully connected scheme is used in which
all neurons from one layer are connected to all neurons in the next.

\ifthenelse{\boolean{includefigures}}{\input{tikz/perceptron}}{}

\citeA{mcculloch43} first conceived of an artificial neuron $j$ that
computes a function $g$ as a weighted sum of all $n$ inputs
\begin{equation}
y_j(x) = g \left(\sum_{i=0}^n w_ix_i\right)
\end{equation}
where $(w_0 \dotsc w_n)$ are weights applied to the inputs $(x_0 \dotsc x_n)$.
In an multi-layer neural network the output $y_j$ forms part of the input
to the neurons in any following layer.  The activation function $g$ is
typically either:
\begin{itemize}
  \item Linear, where $y_j = \sum_{i=0}^n w_ix_i$,
  \item A threshold function, with $y_j \in \lbrace 0,1 \rbrace$,
  \item Sigmoidal, where $0 \leq y_j \leq 1$, or
  \item A hyperbolic tangent function, where $-1 \leq y_j \leq 1$.
\end{itemize}

The parameters of the activation functions can be adjusted along with the
connection weights to tune the transfer function between input and output that
the network provides.  To simplify this process a \textit{bias} node that always
outputs 1 may be added to a layer and connected to all neurons in the following
layer. This can be shown to allow the activation function parameters to be
removed and for network adjustment to be achieved using only connection weights.
Figure \ref{fig:perceptron} shows a fully connected three layer feed-forward
neural network, with bias nodes and separate activation functions $f$, $g$ and
$h$.

The output is obtained during the network's \textit{execution} phase by
presenting an input to the input layer that propagates through the network.  It
can be shown that a suitably configured feed-forward network with one hidden
layer can approximate any non-linear function.
% However, this requires the weights to be adjusted in the \textit{training} or
% \textit{learning} phase and it was not until the back-propagation algorithm
% was proposed by \citeA{werbos74}, 30 years after the inital period of neural
% network popularity in the 1950s, that practical methods became available.  The
% original back-propagation method is the most commonly used, but methods such
% as RProp \cite{riedmiller93} have been introduced to overcome some of its
% limitations.  In supervised learning, the error between the output and the
% expected output is propagated back though the network while the weights of the
% network are adjusted in reduce the size of the error.

% \subsubsection{REINFORCE}
% \label{sec:reinforce}
% REINFORCE is an associative reinforcement learning method that determines
% a policy by modifying the parameters of a policy function approximator, rather
% than approximating a value function \cite{williams:reinforce}.  Commonly,
% feedforward artificial neural networks are used to represent the policy, where
% the input is a representation of the state and the output is action selection
% probabilities.  In learning, a policy gradient approach is taken where the
% weights of the network are adjusted in the direction of the gradient of
% expected reinforcement.
%
% Defining the network, let $\mathbf{x}^i$ denote the vector of inputs to the
% $i$th unit and $y_i$ denote output of the unit.  In the input layer of the
% network the elements $x_j$ of $\mathbf{x}^i$ are normalised sensor values from
% the environment and in the output layer, or in any hidden layers, they are
% outputs from the $j$ unit in the preceding layer.  Let $\mathbf{w}^i$ denote
% the vector of the weights, $w_{ij}$, on the connections to the $i$th unit.  The
% output of the $i$th unit is dependant on the vector of inputs, $\mathbf{x}^i$,
% and the associated weights, $\mathbf{w}^i$.
%
% For each interaction of the agent with the environment, each parameter $w_{ij}$
% is incremented by
% \begin{equation}
% \label{eq:reinforce}
% \Delta w_{ij} = \alpha_{ij}(r - b_{ij})\frac{\partial\ln\rho_i}{\partial
% w_{ij}}
% \end{equation}
% where $\alpha_{ij}$ is the \textit{learning factor}, $b_{ij}$ is the
% \textit{reinforcement baseline} and $\rho_i$ is the performance of the policy
% (e.g., the average reward per interaction).
%
% \subsubsection{ENAC}
% \label{sec:enac}
% ToDo: Episodic Natural Actor Critic\cite{peters:enac}.


\subsection{Roth-Erev Method}
\label{sec:rotherev}
The reinforcement learning method formulated by Alvin~E.~Roth and Ido~Erev is
based on empirical results obtained from observing how humans learn decision
making strategies in games against multiple strategic players
\cite{roth:games,roth:aer}.  It learns a stateless policy in which each action
$a$ is associated with a value $q$ for the propensity of its selection.  In time
period $t$, if agent $j$ performs action $a^\prime$ and receives a reward
$r_{ja^\prime}(t)$ then the propensity value for action $a$ at time $t+1$ is
\begin{equation}
\label{eq:rotherev}
q_{ja}(t+1) =
\begin{cases}
(1-\phi)q_{ja}(t) + r_{ja^\prime}(t)(1-\epsilon), & \text{$a = a^\prime$} \\
(1-\phi)q_{ja}(t) + r_{ja^\prime}(t)(\frac{\epsilon}{A-1}), & \text{$a \ne
a^\prime$}
\end{cases}
\end{equation}
where $A$ is the total number of feasible actions, $\phi$ is the
\textit{recency} parameter and $\epsilon$ is the \textit{experimentation}
parameter.  The recency (forgetting) parameter degrades the propensities for
all actions and prevents propensity values from going unbounded.  It is
intended to represent the tendency for players to forget older action choices
and to prioritise more recent experience.  The experimentation parameter
prevents the probability of choosing an action from going to zero and
encourages exploration of the action space.

\citeA{roth:aer} proposed action selection according to a
discrete probability distribution function, where action $a$ is selected for
interaction $t+1$ with probability
\begin{equation}
\label{eq:re_prob}
p_{ja}(t+1) = \frac{q_{ja}(t+1)}{\sum_{b=0}^A q_{jb}(t+1)}
\end{equation}
Since $\sum_{b=0}^A q_{jl}(t+1)$ increases with $t$, a reward $r_{ja}(t)$ for
performing action $k$ will have a greater effect on the probability
$p_{ja}(t+1)$ during early interactions while $t$ is small.  This is intended
to represent Psychology's Power Law of Practice in which it is qualitatively
stated that with practice learning occurs at a decaying exponential rate and
that a learning curve will eventually flatten out.

\subsubsection{Modified Roth-Erev Method}
\label{sec:variant}
Two shortcomings of the basic Roth-Erev algorithm have been identified and a
modified formulation proposed by \citeA{nicolaisen:2001}.  The two issues are
that
\begin{itemize}
  \item the values by which propensities are updated can be zero or very small
  for certain combinations of the experimentation parameter $\epsilon$ and
  the total number of feasible actions $A$ and
  \item all propensity values are decreased by the same amount when the reward,
  $r_{jk^\prime}(t)$ is zero.
\end{itemize}
Under the variant algorithm, the propensity for agent $j$ to select action $a$
for interaction $t+1$ is:
\begin{equation}
\label{eq:modifiedre}
q_{ja}(t+1) =
\begin{cases}
(1-\phi)q_{ja}(t) + r_{ja^\prime}(t)(1-\epsilon), & \text{$a = a^\prime$} \\
(1-\phi)q_{ja}(t) + q_{ja}(t)(\frac{\epsilon}{A-1}), & \text{$a \ne
a^\prime$}
\end{cases}
\end{equation}
As with the original Roth-Erev algorithm, the propensity for selection of the
action that the reward is associated with is adjusted by the experimentation
parameter.  All other action propensities are adjusted by a small proportion of
their current value.

\subsubsection{Stateful Roth-Erev}
\label{sec:stateful}
The Roth-Erev technique maintains a single vector of propensities for each
action.  Action-value function based methods, such as Q-learning and Sarsa,
typically update a matrix, or look-up table, where each row corresponds to an
individual state.  In this thesis a \textit{Stateful Roth-Erev} method is
proposed.  The method is a simple extension to the original or modified version
that maintains an action propensity \textit{matrix} with a row corresponding to
each discrete state.  Updates are done according to equation (\ref{eq:rotherev})
or equation (\ref{eq:modifiedre}), but only action propensities for the current
state are updated. The method allows for differentiation between states of the
environment, but can greatly increase the number of propensity values requiring
updates.

\section{Summary}
The combination of an electricity market and an electric power system presents a
complex dynamic environment for participants.  Network power flows are
non-linear functions of the bus voltages and thus one party's generation or
consumption decisions effect all other parties.
% Substantial modifications to the design of the UK's electricity trading
% arrangements have been required since the their introduction two decades ago.
% further changes will likely be necessary if ambitious greenhouse gas emission
% reduction commitments are to be met.

The main electricity trading mechanisms can be modelled using well established
mathematical optimisation formulations.  Robust techniques exist for computing
solutions to these problems, which also provide price information that reflects
the network topology and conditions. The combination of non-linear optimisation
problems and complex participant behavioural models is likely beyond the
capabilities of conventional closed-form equilibrium approaches when analysing
large systems. An alternative is to take a ``bottom-up'' modelling approach and
examine the system dynamics that result from interactions between goal driven
individuals.

Reinforcement learning is an unsupervised machine learning technique that can be
used to model the dynamic behaviour of competing individuals.  Traditional
methods associated a \textit{value} with each state and the available actions,
but they are limited to small discrete problem representations.  Policy gradient
methods, that search directly in the space of the parameters of an action
selection policy and can operate in continuous environments, have been shown in
the literature to exhibit good convergence properties and have been successfully
applied in laboratory and operational settings.

The successful application of policy gradient methods in other fields suggests
that they may be used to model participant strategies in agent-based electricity
market simulations.  First it must be established how these methods have been
applied in similar contexts and what other methods have been used.
