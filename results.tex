\chapter{Nash Equilibrium Analysis}
\label{ch:nashanalysis}
This chapter examines the convergence to a Nash equilibrium of agents competing
with portfolios of generating plant.  Value function based and policy gradient
reinforcement learning algorithms are compared in convergence to an optimal
policy using a six bus electric power system model.

\section{Introduction}
This thesis presents the first case of
policy gradient reinforcement learning methods being applied to electricity
trading problems.  As a first step it is necessary to confirm that when using
these methods, a system of multiple agents will converge to the same Nash
equilibrium\footnote{Informally, a Nash equlibrium is a point in a
non-cooperative game at which no player is motivated to deviate from its
strategy, as it would result in lower gain \cite{nash50,nash51}.} that
traditional closed-form simulation techniques produce.

This is the same approach used by \citeA{krause:nash06} before performing the
study of congestion management techniques that is reviewed in Section
\ref{sec:related_cong}.  Nash equilibria can be difficult
to determine in complex systems so the experiment presented here utilises a
model simple enough that it can be determined through exhaustive search.

By observing the actions taken and the reward received by each agent over the
initial simulation periods it is possible to compare different algorithms in
the speed and accuracy of their convergence to an optimal policy.  In the
following sections the objectives of this experiment are explicitly defined,
the setup of the simulations is explained and simulation results, with
discussion and critical analysis, are provided.

\section{Aims and Objectives}
Some elements of the simulations reported in this chapter are similar to those
presented by \citeA{krause:nash06}.  One initial aim of this work is to
reproduce their findings as a means of validating the approach.  The additional
objectives are to show:
\begin{itemize}
  \item That policy gradient methods converge to the same Nash equilibrium as
  value function based methods and tradtional closed-form simulations,
  \item The charateristics of different learning methods by examining the
  nature of their convergence to an optimal policy.
%   \item The sensitivity of policy convergence to algorithm parameter choices
%   and policy function approximation structure.
\end{itemize}
In meeting these objectives a basis for using policy gradient methods in more
complex simulations would be created.  It would show that they are capable of
learning basic policies and provide guidance for the selection of algorithm
parameters.

\section{Method of Simulation}
% Each learning method is tested individually using a range of parameter
% configurations.  A power system model with one bus, one generator $k$ and
% one dispatchable load $l$.  In this
% context, the market clearing process is equivalent to creating offer and bids
% stacks and finding the point of intersection.  A passive agent is associated
% with the dispatchable load.  This agent bids for $-p_{g,l}^{min}$ at marginal
% cost each period regardless of environment state or reward signal.  A
% dispathcable load is used instead of a constant load to allow a price to be
% set. Generator $k$ is given sufficient capacity to supply the demand
% of the dispatchable load, $p_{g,k}^{max} > -p_{g,l}^{min}$, and the marginal of
% the $k$ is half that of the load $l$.  The generator and dispatchable load
% attributes are given in Table X.  A price cap for the market is set to twice the
% marginal cost of the $l$ at full capacity, $p_{g,l}^{min}$.  The DC optimal
% power flow formulation (See Section \ref{sec:opf}, above) is used to clear the
% market and reactive power trade is omitted.
Learning methods are compared in this experiment by repeating the same
simulation with different algorithms used by the agents.  An alternative
might be to use a combination of methods in the same simulation, but the
approach used here is intended to be an extension of the work by
\citeA{krause:nash06}.

Each simulation uses the six bus electric power system model adapted from
\citeA[pp.~104, 112, 119, 123-124, 549]{wood:pgoc}.  The model provides a simple
environment for trade with a small number of generators and branch flow
constraints that slightly increase the complexity of the Nash equilibria.  The
six buses are connected by eleven transmission lines at 230kV. The model
contains three generating units with a total capacity of 440MW and loads at
three locations, each of 70MW. The connectivity of the branches and the
locations of the generators and loads is shown in Figure~\ref{fig:case6ww}.
Data for the power system model was taken from a case provided with
\textsc{Matpower}, is listed in Appendix \ref{adx:case6ww} and is distributed
with the software developed for this thesis (See Appendix \ref{sec:pylon}).

Two sets of quadratic generator operating cost functions, of the form
$c(p_i)=ap_i^2+bp_i+c$ where $p_i$ is the out put of generator $i$, are defined
in order to create two different equilibria for investigation.  The coefficients
$a$, $b$ and $c$ for the first cost configuration are listed in Table
\ref{tbl:case6ww_gencost1}. This cost configuration defines two low cost
generators that can not offer a price greater than the marginal cost of the most
expensive generator when the maximum markup is applied.  The second set of
coefficients is listed in Table \ref{tbl:case6ww_gencost2}.  This configuration
narrows the cost differences such that offer prices may overlap and may exceed
the marginal cost of the most expensive generator.  To strengthen the penalty
for not being dispatched and shudown cost $C_{down}$ of \$100 is specified in
this configuration.

\ifthenelse{\boolean{includefigures}}{\input{tikz/case6ww}}{}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Gen &$C_{down}$ &$a$ &$b$ &$c$ \\
\hline\hline
 1 &0 &0.0 &4.0 &200.0 \\
 2 &0 &0.0 &3.0 &200.0 \\
 3 &0 &0.0 &6.0 &200.0 \\
\hline
\end{tabular}
\caption{Generator cost configuration 1.}
\label{tbl:case6ww_gencost1}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Gen &$C_{down}$ &$a$ &$b$ &$c$ \\
\hline\hline
 1 &100 &0.0 &5.1 &200.0 \\
 2 &100 &0.0 &4.5 &200.0 \\
 3 &100 &0.0 &6.0 &200.0 \\
\hline
\end{tabular}
\caption{Generator cost configuration 2.}
\label{tbl:case6ww_gencost2}
\end{center}
\end{table}

As in \citeA{krause:nash06}, no load profile is defined for the simulation.
The system load is assumed to be peak for all simulation periods, thus only one state is defined for methods
using look-up tables.  Each simulation step is assumed to be one hour long.

The minimum operating point, $P^{min}$, for all generators is zeroed so as to
simplify the experiment and avoid the need to use the unit de-commitment
algorithm.  The maximum capacity for the most expensive generator
$P^{max}_3=220$MW such that it may supply almost all of the load if dispatched.
This generator is associated with a passive agent that always offers full
capacity at marginal cost.  For the other generators $P^{max}_1=110$MW and
$P^{max}_2=110$MW.  These two generators are each associated with an active
learning agent whose activity in the market is restricted to one offer of
maximum capacity in each period, at a price representing a markup of between 0
and 30\% on marginal cost.  Methods restricted to discrete action may markup in
steps of 10\%, giving possible markup actions of 0\%, 10\%, 20\% and 30\%.  No
capacity withholding is allowed.  The market price cap is set such that it is
never reached by any markup and does not complicate the experiment.
Discriminatory pricing (pay-as-bid) is used in order to provide a clearer reward
signal to agents with low cost generators.

This simulation compares Q-learning, ENAC, REINFORCE and the variant Roth-Erev
technique.  Default algorithm parameter values from PyBrain are used and no
attempt to study parameter sensitivity or function approximator design is made.

For Q-learning $\alpha=0.3$, $\gamma=0.99$ and $\epsilon$-greedy action
selection is used with $\epsilon=0.9$ and $d=0.98$. For Roth-Erev learning
$\epsilon=0.55$, $\phi=0.3$ and Boltzmann action selection is used with
$\tau=100$ and $d=0.99$.

Both REINFORCE and ENAC use a two-layer neural network
with one linear input node, one $\tanh$ output node, no bias nodes and with
weights initialised to zero.  A two-step episode is defined for the policy
gradient methods and five episodes are performed per learning step.  The
exploration paramter $\sigma$ for these methods is initialised to zero and
adjusted manually after each episode such that:
\begin{equation}
\label{eq:sigmadecay}
\sigma_{t} = d(\sigma_{t-1}-\sigma_{n})+\sigma_{n}
\end{equation}
where $d=0.998$ is a decay parameter and $\sigma_{n}=-0.5$ specifies the
value that is converged to asymtotically.  In each simulation the learning rate
$\gamma=0.01$ for the policy gradient methods, apart from for ENAC under cost
configuration 2 where $\gamma=0.005$.  All active agents use the same parameter
values in each simulation.

As in \citeA{krause:nash06}, the point of Nash equilibrium is established by
computing each agent's reward for all possible combinations of markup.  The
rewards for Agent 1 and Agent 2 under cost configuration 1 are given in Table
\ref{tbl:nash1}.  The Nash equilibrium points are marked with a *.  It shows
that the optimal policy for each agent is to apply the maximum markup to each
offer as this never results in their generators failing to be dispatched. The
rewards under cost configuration 2 are given in Table \ref{tbl:nash2}. It
shows that the optimal point occurs when Agent 2 applies its maximum markup
and Agent 1 offers a price just below the marginal cost of the passive agent's
generator.

\begin{table}
\begin{center}
\begin{small}
\begin{tabular}{c.{2.2}|.{2.1}.{2.1}|.{2.1}.{2.1}|.{2.1}.{2.1}|.{3.1}.{2.1}|}
\cline{3-10}
 & &\multicolumn{8}{c|}{$G_1$} \\
\cline{3-10}
 & &\multicolumn{2}{c|}{0.0\%} &\multicolumn{2}{c|}{10.0\%} &\multicolumn{2}{c|}{20.0\%} &\multicolumn{2}{c|}{30.0\%} \\
 & &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$G_2$}} &0.0\% &0.0 &0.0 &40.0 &0.0 &80.0 &0.0 &120.0 &0.0 \\
\multicolumn{1}{|c|}{} &10.0\% &0.0 &33.0 &40.0 &33.0 &80.0 &33.0 &120.0 &33.0 \\
\multicolumn{1}{|c|}{} &20.0\% &0.0 &66.0 &40.0 &66.0 &80.0 &66.0 &120.0 &66.0 \\
\multicolumn{1}{|c|}{} &30.0\% &0.0 &99.0 &40.0 &99.0 &80.0 &99.0 &120.0^*
&99.0^* \\
\hline
\end{tabular}
\caption{Agent rewards under cost configuration~1}
\label{tbl:nash1}
\end{small}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{small}
\begin{tabular}{c.{2.2}|.{2.1}.{3.1}|.{2.1}.{3.1}|.{2.1}.{3.1}|.{2.1}.{3.1}|}
\cline{3-10}
 & &\multicolumn{8}{c|}{$G_1$} \\
\cline{3-10}
 & &\multicolumn{2}{c|}{0.0\%} &\multicolumn{2}{c|}{10.0\%} &\multicolumn{2}{c|}{20.0\%} &\multicolumn{2}{c|}{30.0\%} \\
 & &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$G_2$}} &0.0\% &0.0 &0.0 &51.0 &0.0 &0.0 &0.0 &0.0 &0.0 \\
\multicolumn{1}{|c|}{} &10.0\% &0.0 &49.5 &51.0 &49.5 &0.0 &49.5 &0.0 &49.5 \\
\multicolumn{1}{|c|}{} &20.0\% &0.0 &92.2 &51.0 &99.0 &0.0 &99.0 &0.0 &99.0 \\
\multicolumn{1}{|c|}{} &30.0\% &0.0 &126.8 &54.8^* &138.4^* &0.0 &148.5 &0.0
&148.5 \\
\hline
\end{tabular}
\caption{Agent rewards under cost configuration~2}
\label{tbl:nash2}
\end{small}
\end{center}
\end{table}

\section{Simulation Results}
Each action taken by an agent and the consequent reward is recorded for each
simulation.  Values are averaged over the ten simulation runs and standard
deviations are calculated using the formula
\begin{equation}
SD = \sqrt{\frac{1}{N-1}\sum_{i=0}^{N}(x_i - \bar{x})^2}
\end{equation}
where $x_i$ is the action or reward value in simulation $i$ of $N$ simulation
runs and $\bar{x}$ is the mean of the values.

Figure \ref{fig:5_1_action_a1} shows the average markup on marginal cost and
the standard deviation over the ten simulation runs for Agent 1 under price
configuration 1 using the variant Roth-Erev, Q-learning, REINFORCE and ENAC
learning methods.  The second $y$-axis in each plot realtes to the exploration
parameter for each method.  Figure \ref{fig:5_1_action_a2} plots the same
quantities for Agent 2.  Plots of reward are not given as generator prices and
the market are configured such that an agent's reward is directly proportional
to its action.  The plots are vertically aligned and have equal $x$-axis limits
to assist algorithm comparison.

\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_1_action_a1}
	  \caption{Average markup for agent 1 and standard deviation over 10 runs.}
	  \label{fig:5_1_action_a1}
	\end{figure}

	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_1_action_a2}
	  \caption{Average markup for agent 2 and standard deviation over 10 runs.}
	  \label{fig:5_1_action_a2}
	\end{figure}
}{}

Figures \ref{fig:5_2_action_a1} and \ref{fig:5_2_reward_a1} plot the average
markup and reward over ten simulation runs for Agent 1 and Agent 2,
respectively, under price configuration 2 and for the variant Roth-Erev,
Q-learning learning methods.  The figures show actual values for one simulation
run of REINFORCE and ENAC as the number of interactions and variation in values
makes the results difficult to observe.  Not all $x$-axis extents are equal in
these two figures.

\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_2_action_a1}
	  \caption{Average markup for agent 1 and standard deviation.}
	  \label{fig:5_2_action_a1}
	\end{figure}
% 	\begin{figure}
% 	  \centering
% 	  \includegraphics{figures/fig5_2_action_a2}
% 	  \caption{Average markup for agent 2 and standard deviation.}
% 	  \label{fig:5_2_action_a2}
% 	\end{figure}
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_2_reward_a1}
	  \caption{Average reward for agent 1 and standard deviation.}
	  \label{fig:5_2_reward_a1}
	\end{figure}
% 	\begin{figure}
% 	  \centering
% 	  \includegraphics{figures/fig5_2_reward_a2}
% 	  \caption{Average reward for agent 2 and standard deviation.}
% 	  \label{fig:5_2_reward_a2}
% 	\end{figure}
}{}

\section{Discussion and Critical Analysis}
Under cost configuration 1 the agents face a relatively simple control task and
receive a clear reward signal that is directly proportional to their markup. The
results show that all of the methods consistently converge to the point of Nash
equilibrium.  The variant Roth-Erev method show least variation around the mean
when converged due to the use of Boltmann exploration with a then low
temperature parameter value.  The constant variation around the mean that can be
seen for Q-learning once is has converged is due to the use of $\epsilon$-greedy
action selection and can be removed if a Boltmann explorer is used.  Empirical
studies have also shown that the speed of convergence is largely determined by
the rate at which the exploration parameter value is reduced.  However, the
episodic nature of the policy gradient methods requires them to make several
interaction per learning step and therefore a larger number of initial
exploration steps.  Policy gradient methods can also be highly sensitive to
the learning rate parameter and high values must be avoided if the policy is
to converge.

Cost configuration 2 provides a more challenging control problem in which
Agent~1 must learn to undercut the passive agent.  The results show that the
variant Roth-Erev and Q-learning methods both consistently learn their optimal
policy and converge to the Nash equilibrium.  However, there is space for
Agent~1 to markup its offer by slightly more than 10\% and still undercut the
passive agent, but methods with discrete actions are not able to exploit this
and receive the additional profit.

The results for the policy gradient methods under cost configuration 2 show that
these methods learn to reduce their markup if their offer price starts to exceed
that of the passive agent and the reward signal drops.  However, a chattering
effect below the Nash equilibrium point can be clearly seen for ENAC and the method
does not learn to always undercut the other agent.  These methods also require
a much larger number of simulation steps and for the exploration parameter to
be decayed more slowly if they are to produce this behaviour.  This is
due to the need for a lower learning rate that ensures fine policy
adjustments can be made and again for several interactions to be performed between each learning
step.
% When using REINFORCE or ENAC, Agent~2 tends also to learn to maximise its
% markup, but less consistently.  Agent~1 typically learns to undercut the
% passive agent, but does not converge to a consistent value.  The problem is
% similar to the cliff-edge walking problems often used as benchmarks in
% reinforcement learning research and may be difficult to approximate a policy
% for using a small number of $\tanh$ functions. It may be possible to improve
% the performance of these agents through more educated policy function
% approximator design. but these methods are not really intended for operation
% in such simple environments.

\section{Summary}
This experiment confirms the convergence to a Nash equilibrium of the
Q-learning methods that is published in \citeA{krause:nash06} and, to a degree,
extends the conclusion to policy gradient methods.  The results show that
while these methods do converge to the same or similar policies as the
Q-learning and Roth-Erev methods, they do not exhibit the same level of
consistency.  Value function based methods or the Roth-Erev method may be the
most suitable choice of algorithm in the simple electricity market simulations
typically found in the literature.

The simulations conducted here do not exploit any of the abilities of policy
gradient methods to utilise multi-dimensional continuous state information and
their behaviour in more complex electricity market environments deserves
investigation.

%\section{Summary}


% \chapter{Competitive Power Trade}
% Having compared the learning methods in a one-player context, this section
% describes the method used to pit them against one and other and compare their
% performance.
%
% \section{Aims \& Objectives}
% Competition is fundamental to markets and this experiment aims to compare
% learning methods in a complex dynamic market environment with multiple
% competing participants.  The objective is to compare:
% \begin{itemize}
%   \item Performance, in terms of profitability, over a finite number of
%   periods,
%   \item Profitability when trading both active and reactive power.
%   \item Consistency of profit making and,
%   \item Sensitivity to algorithm parameter changes.
% \end{itemize}
%
% \section{Method of Simulation}
% Figure X illustrates the structure of the six bus power system model, from
% \cite{wood:pgoc}, with three generators and fixed demand at three of the buses
% used to provide a dynamic environment with typical system values.  Bus,
% branch and generator attribute values are stated in Tables X, Y, Z,
% respectively.  Three learning methods are compared in six simulations
% encapsulating all method--generator combinations.
%
% A price cap $c_{cap}$ of twice the marginal cost of the most expensive generator
% at full capacity is set by the market.  The simulations are repeated for with agents
% actions composing both price and quantity and with just price.  For the
% value-function methods, the state is defined by the market clearing price from
% the previous period, divided equally into $x_s$ discrete states between $0$ and
% $c_{cap}$.  The state vector $s_t$ for the policy gradient methods consists of
% the market clearing price and generator set-point from the previous period.
% \begin{equation}
% s_t =
% \begin{bmatrix}
% c_{mcp}\\
% p_g
% \end{bmatrix}
% \end{equation}
% The script used to conduct the simulation is provided in Listing X.

\chapter{System Constraint Exploitation}
\label{ch:exploitation}
% One of the main features of agents using policy gradient learning methods and
% artifical neural networks for policy function approximation is their ability
% to accept many signals of continuous sensor data.  This section describes an
% experiment in which the power system is severly constrained for certain
% periods, resulting in elevated nodal marginal prices in particular areas.  The
% methods are tested in their ability to exploit these constraints and improve
% their total accumulated reward.
This chapter explores learning agents exploitation of constraints in electric
power system models.  Value function based and policy gradient reinforcement
learning methods are compared using a dynamic 24-bus power system model
from the IEEE Reliability Test System.

\section{Introduction}
Having examined the basic learning characterisitics of four algorithms in
Chapter~\ref{ch:nashanalysis}, this experiment extends the approach to examine
their operation in a complex dynamic environment.  It explores the ability of
policy gradient methods to operate with multi-dimensional, continuous state and
action spaces in the context of \textit{learning to trade power}.

A well established electric power system model from the IEEE Reliability Test
System \cite{ieee79rts} provides a realistic environment in which agents compete
with their portfolios of generating plant to supply dynamic loads. System
constraints change as agents adjust their behaviour and the loads follow a daily
profile that varies over the course of a simulated year.  By observing profits
at different times of day, the ability of methods to successfully observe and
exploit constraints is examined.

\section{Aims and Objectives}
This experiment aims to compare policy gradient and traditional learning methods
in a dynamic electricity trading environment.  Specifically, the objectives are
to determine:
\begin{itemize}
  \item If the policy gradient methods can achieve greater profitability
  under dynamic system constraints.
%   \item If policy gradient methods can exploit outages and the resulting system
%   constraints to further increased profit.
  \item The value of using an AC optimal power flow formulation in agent based
  electricity market simulation.
\end{itemize}
Meeting these objectives would demonstrate some of the value of using policy
gradient methods in electricity market participant modelling and determine if
they warrant further research in this domain.

\section{Method of Simulation}
In this experiment learning methods are compared by repeating simulations of
competitive electricity trade with different algorithms used by the competing
agents. Some simplification of the state and action representations for value
function based methods is required, but the portfolios of generation and
the load profiles are the same for each algorithm test.

The IEEE Reliability Test System (RTS) provides the power system model and load
profiles used in each simulation.  The model has 24 bus locations that are
connected by 32 transmission lines, 4 transformers and 2 underground cables. The
transformers tie a 230kV area to an area at 138kV.  The original model has 32
generators of 9 different types with a total capacity of 3.45GW.  To reduce the
size of the discrete action domain, five 12MW and four 20MW generators are
removed.  This is deemed reasonable as their combined capacity is only 4.1\% of
the original total generation capacity and the remaining capacity is more than
sufficient to meet demand.  To further reduce action space sizes all generators
of the same type at the same bus are aggregated into one generating unit. The
model has loads at 17 locations and the total demand at system peak is 2.85GW.

Generator costs are quadratic functions of output, defined by the parameters in
Table \ref{tbl:ieee_rts_gencosts}. Figure \ref{fig:ieee_rts_gencost_plot} shows
the cost functions for each of the seven types of generator and illustrates
their categorisation by fuel type.  Generator cost function coefficients were
taken from a website hosted by Georgia Tech Power Systems Control and Automation
Laboratory\footnote{http://pscal.ece.gatech.edu/testsys/} that assumes Coal
costs of 1.5~\$/MBtu\footnote{1 Btu $\approx$ 1055 Joules}, Oil costs of
5.5~\$/MBtu and Uranium costs of 0.46~\$/MBtu.  Data for the modified model is
provided in Appendix \ref{adx:ieee_rts} and the connectivity of branches and the
location of generators and loads is illustrated in Figure \ref{fig:ieee79rts}.

\begin{table}
\begin{center}
\begin{tabular}{c|c|.{1.5}|.{3.4}|.{3.3}|c}
\hline
Code &$C_{down}$ &\multicolumn{1}{c}{$a$} &\multicolumn{1}{|c|}{$b$}
&\multicolumn{1}{c|}{$c$} &Type \\
\hline\hline
% U12 &0.0 &0.32841 &56.564 &86.385 &Oil \\
% U20 &0.0 &0.0 &130.0 &400.685 &Oil \\
U50 &0 &0.0 &0.001 &0.001 &Hydro \\
U76	 &0	&0.01414	&16.0811	&212.308 &Coal \\
U100 &0	&0.05267	&43.6615	&781.521 &Oil \\
U155 &0	&0.00834	&12.3883	&382.239 &Coal \\
U197 &0	&0.00717	&48.5804	&832.758 &Oil \\
U350 &0	&0.00490	&11.8495	&665.109 &Coal \\
U400 &0	&0.00021	&4.4231	&395.375 &Nuclear \\
\hline
\end{tabular}
\caption{Cost parameters IEEE RTS generator types.}
\label{tbl:ieee_rts_gencosts}
\end{center}
\end{table}

\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/ieee_rts_gencosts}
	  \caption{Generator cost functions for the IEEE Reliability Test System}
	  \label{fig:ieee_rts_gencost_plot}
	\end{figure}

	\begin{figure}
	  \centering
	  \includegraphics{figures/ieee_rts_profiles}
	  \caption{Hourly, daily and weekly load profile plots from the IEEE
	  Reliability Test System}
	  \label{fig:ieee_rts_profiles}
	\end{figure}
}{}

The generating stock is divided into 4 portfolios (See Table
\ref{tbl:agent_portfolios}) that are each endowed to a learning agent.
Portfolios were chosen such that each agent has: a mix of base load and peaking
plant, approximately the same total generation capacity and generators in
different areas of the network.  The generator labels in Figure
\ref{fig:ieee79rts} specify the associated agent.  The synchronous condenser is
associated with a passive agent that always offers 0 MW at 0~\$/MWh (the
unit can be dispatched to provide or absorb reactive power).

Markups on marginal cost are restricted a maximum of 30\% and discrete markups
of 0 or 30\% are defined for value function based methods.  Upto 30\% of the
total capacity of each generator can be withheld and discrete withholds of
0~or~30\% are defined.  Agent~3 has the largest discrete action space with XX
possible actions to be explored in each state.

The state for all algoithm tests contains a forecast of the total system demand
for the period that capacity is being offered for.  The system demand follows an
hourly profile that is adjusted according to the day of the week and the time of
year.  The profiles are taken from the RTS and are shown in Figure
\ref{fig:ieee_rts_profiles}.  For tests of value function based methods or the
Roth-Erev learning algorithm, the continuous state is divided into XX discrete
states between minimum and maximum total system load.  The state vector for
agents using policy gradient methods additionally contains the voltage magnitude
at each bus.  Branch flows are not included in the state vector as the flow
limits in the RTS are high and none are reached when the system is at peak
demand.  Generator capacity limits are binding in most states of the RTS, but
the output of other generators is deemed to be hidden from the agents.

The nodal marginal pricing scheme is used in which cleared offer prices are
determined by the Lagrangian multiplier on the power balance constraint for the
bus at which the generator associated with the offer is connected.

Typical parameter values are used for each of the algorithms.  Learning rates
are set low and the exploration parameters are decayed slowly due to the length
and complexity of each simulation.  For Q-learning $\alpha=0.3$, $\gamma=0.99$
and $\epsilon$-greedy action selection is used with $\epsilon=0.9$ and $d=0.98$.
For Roth-Erev learning $\epsilon=0.55$, $\phi=0.3$ and Boltzmann action
selection is used with $\tau=100$ and $d=0.99$.

\begin{table}
\begin{center}
% \begin{tabular}{c|c|c}
% \hline
% Agent &Type &\multicolumn{1}{c}{Buses} \\
% \hline\hline
% \multirow{3}{*}{$A_1$} &U20 &1,1,2,2 \\
%  & U76 &1,1,2,2 \\
%  & U100 &7,7,7\\
% \hline
% \multirow{3}{*}{$A_2$} &U155 &23,23 \\
%  & U197 &13,13,13 \\
%  & U300 &23 \\
% \hline
% \multirow{2}{*}{$A_3$} &U12 &15,15,15,15,15 \\
%  & U155 &15,16 \\
% \hline
% \multirow{2}{*}{$A_4$} &U50 &22,22,22,22,22,22 \\
%  & U400 &18,21 \\
% \hline
% $A_5$ & Sync.~Cond. &14 \\
% \hline
% \end{tabular}

\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Agent} &U50 &U76 &U100 &U155 &U197 &U350 &U400 &Total \\
 &Hydro &Coal &Oil &Coal &Oil &Coal &Nuclear &(MW) \\
\hline
1 & &2$\times$ & &1$\times$ & & &1$\times$ &707 \\
2 & &2$\times$ & &1$\times$ & & &1$\times$ &707 \\
3 &6$\times$ & & & &3$\times$ & & &891 \\
4 & & &3$\times$ &2$\times$ & &1$\times$ & &960 \\
\hline
\end{tabular}

\caption{Agent portfolios.}
\label{tbl:agent_portfolios}
\end{center}
\end{table}

\ifthenelse{\boolean{includefigures}}{\input{tikz/ieee79rts}}{}

\section{Simulation Results}
\section{Discussion and Critical Analysis}
\label{sec:discuss}
\section{Summary}
