\chapter{Learning to Trade Power}
\label{ch:learningtotrade}
This chapter examines the convergence to a Nash equilibrium of agents
compteting with portfolios of generating plant.  Value function based and
policy gradient reinforcement learning algorithms are compared in their speed
of convergence and paramenter sensitivity using a simple 6-bus electric power
system model.

\section{Introduction}
To the best of the author's knowledge, this thesis presents the first case of
policy gradient reinforcement learning methods being applied to electricity
trading problems.  As a first step it is necessary to confirm that when using
these methods, a system of multiple agents will converge to the same Nash
equilibrium that conventional closed-form simulation techniques produce.

This is the same approach by \citeA{krause:nash06} before performing the study
of congestion management techniques that is reviewed in Section
\ref{sec:related_cong}.  Informally, a Nash equlibrium is a point at which no
player is motivated to deviate from its strategy.  This point can be difficult
to determine in complex systems so the experiment presented here utilises a
model simple enough that it can be determined through brute-force search.

By observing the actions taken and the reward received by each agent over the
initial simulation periods it is possible to compare different configurations
of the algorithms in their speed of convergence to an optimal policy.  In the
following sections the objectives of this experiment are explictly defined, the
setup of the simulations is explained and simulation results provided with
discussion and critical analysis.

\section{Aims and Objectives}
Some elements of this experiment are very similar to those presented in
\citeA{krause:nash06} and one aim is to reproduce those results.  Additionally
the specific objectives are to show that:
\begin{itemize}
  \item Policy gradient methods converge to the same Nash equilibrium as value
  function based methods.
  \item The differences in speed of convergence to an optimal policy between
  the learning methods.
  \item The sensitivity of policy convergence to algorithm parameter choices
  and policy function approximation structure.
\end{itemize}
Meeting these objectives aims to provide to provide a basis for more
complicated experiments that are less intuitively tractable.

\section{Method of Simulation}
% Each learning method is tested individually using a range of parameter
% configurations.  A power system model with one bus, one generator $k$ and
% one dispatchable load $l$.  In this
% context, the market clearing process is equivalent to creating offer and bids
% stacks and finding the point of intersection.  A passive agent is associated
% with the dispatchable load.  This agent bids for $-p_{g,l}^{min}$ at marginal
% cost each period regardless of environment state or reward signal.  A
% dispathcable load is used instead of a constant load to allow a price to be
% set. Generator $k$ is given sufficient capacity to supply the demand
% of the dispatchable load, $p_{g,k}^{max} > -p_{g,l}^{min}$, and the marginal of
% the $k$ is half that of the load $l$.  The generator and dispatchable load
% attributes are given in Table X.  A price cap for the market is set to twice the
% marginal cost of the $l$ at full capacity, $p_{g,l}^{min}$.  The DC optimal
% power flow formulation (See Section \ref{sec:opf}, above) is used to clear the
% market and reactive power trade is omitted.
Learning methods are compared in this experiment by repeating the same
simulation with agents utilising the different algorithms.  An alternative
might be to use a combination of methods in the same simulation, but the
approach used here extends the work of \citeA{krause:nash06}.

Each simulation uses the 6-bus electric power system model from \citeA[pp.~104,
112, 119, 123-124, 549]{wood:pgoc}.  The 6 buses in this model are connected by
11 transmission lines at 230kV.  It contains 3 generating units with a total
capacity of 530MW and loads a 3 locations, each of 70MW.  The generator
opertaing costs are linear functions of production, defined by the parameters
in Table \ref{tbl:case6ww_gencost} and plotted in Figure X.  The connectivity
of the branches and the locations of the generators and the load is shown in
Figure~\ref{fig:case6ww}.  Data for the power system model is provided in
Appendix \ref{adx:case6ww} and is distributed with the software developed for
this thesis (See Appendix \ref{sec:pylon}).

%\input{tikz/case6ww}

No load profile is defined, the system load is assumed to be peak for all
simulation periods, so only one system state is defined for the value function
based algorithms.  The minimum operating point $P_{min}$ for all generators is
assumed to be $0.0$ so as to simplify the experiment and avoid the need to use
the unit decommitment algorithm defined in Section \ref{sec:decommit}.

Two active agents, and one passive agent that always offers at marginal cost,
are defined in each simulation and associated with one generating unit.  Their
activity in the market is restricted to one offer in each period of maximum
capacity at a price representing a markup of between 0 and 30\% on marginal
cost.  Value function based methods are restricted to discrete markup steps of
10\%, giving possible markup actions of 0, 10, 20 and 30\%, as illustrated in
Figure X.  The market price cap is set such that it is never reached by any
markup and does not complicate the experiment.

The learning methods compared are Sarsa, Q-learning, Q($\lambda$), ENAC,
REINFORCE and the variant Roth-Erev technique.  The parameters available to
adjustment are\ldots.

As in \citeA{krause:nash06}, the point of Nash equilibrium is established by
computing each agent's reward for all possible combinations of markup.  The
value are given in Table Y.

As one would intuitively expect, the optimal policy for each agent is to apply
the maximum markup to each offer as this never results in a generator failing
to be dispatched.

They show that the optimal point is where the agent with the lowest cost
generator applies the maximum markup and the agent with the medimum priced
generator offers a proce just below the marginal cost of the passive
agent's generator.


\section{Results}
\section{Discussion}
\section{Conclusions}


% \chapter{Competitive Power Trade}
% Having compared the learning methods in a one-player context, this section
% describes the method used to pit them against one and other and compare their
% performance.
%
% \section{Aims \& Objectives}
% Competition is fundamental to markets and this experiment aims to compare
% learning methods in a complex dynamic market environment with multiple
% competing participants.  The objective is to compare:
% \begin{itemize}
%   \item Performance, in terms of profitability, over a finite number of
%   periods,
%   \item Profitability when trading both active and reactive power.
%   \item Consistency of profit making and,
%   \item Sensitivity to algorithm parameter changes.
% \end{itemize}
%
% \section{Method of Simulation}
% Figure X illustrates the structure of the six bus power system model, from
% \cite{wood:pgoc}, with three generators and fixed demand at three of the buses
% used to provide a dynamic environment with typical system values.  Bus,
% branch and generator attribute values are stated in Tables X, Y, Z,
% respectively.  Three learning methods are compared in six simulations
% encapsulating all method--generator combinations.
%
% A price cap $c_{cap}$ of twice the marginal cost of the most expensive generator
% at full capacity is set by the market.  The simulations are repeated for with agents
% actions composing both price and quantity and with just price.  For the
% value-function methods, the state is defined by the market clearing price from
% the previous period, divided equally into $x_s$ discrete states between $0$ and
% $c_{cap}$.  The state vector $s_t$ for the policy gradient methods consists of
% the market clearing price and generator set-point from the previous period.
% \begin{equation}
% s_t =
% \begin{bmatrix}
% c_{mcp}\\
% p_g
% \end{bmatrix}
% \end{equation}
% The script used to conduct the simulation is provided in Listing X.

\chapter{System Constraint Exploitation}
\label{ch:exploitation}
% One of the main features of agents using policy gradient learning methods and
% artifical neural networks for policy function approximation is their ability to
% accept many signals of continuous sensor data.  This section describes an
% experiment in which the power system is severly constrained for certain
% periods, resulting in elevated nodal marginal prices in particular areas.  The
% methods are tested in their ability to exploit these constraints and improve
% their total accumulated reward.
This chapter explores the exploitation of constraints in an electric power
system models by agents whose behaviour is determined by reinforcement learning
algorithms.  Value function based and policy gradient methods are compared
using the IEEE Reliability Test System, with dynamic loads and probibalistic
transmission line outages.

\section{Introduction}
Having explored the basic properties of various learning methods in Chapter
\ref{ch:learningtotrade}, this experiment examines them under a complex dynamic
scenario.  Policy gradient methods have been used in robotic control
applications with multi-dimensional, continuous state and action spaces, and
exhibit a degree of robustness to sensor noise.  In this experiment, these same
properties are explioited in the context of learning to trade power.  Control
of a portfolio of generators using continous sensor data from simulation of a
standard test power system model with realisitic load dynamics is examined.  To
force the system into a constrained state at certain times, transmission line
outages are simualated according to the probabilities given in Table Z.  By
observing the actions taken and the reward received by an agent during these
periods it is examined if these methods can be used to exploit such
occurances.

\section{Aims and Objectives}
This experiment aims to compare the operation of learning methods in dynamic
electric power system environments.  Specifically, the objective are to
determine:
\begin{itemize}
  \item If policy gradient methods can be used to achieve greater profit under
  dynamic loading conditions.
  \item If policy gradient methods can exploit outages and the resulting system
  constraints to further increased profit.
  \item The value of using AC optimal power flow formulations in agent base
  electricity market simulation.
\end{itemize}
Meeting these objectives aims to demonstrate the value of policy gradient
methods in electricity market participant modelling.

\section{Method}
The learning methods are compared by repeating the same simulation with
different types of algorithm in-place.  Some simplification of the state
and action domains for the value function based methods is required, but the
portfolios of generation and load profiles are constant.

The IEEE Reliability test System \cite{ieee79rts} provides the power system
model, load profiles and outage probabilities used in each simulation.  The
model has 24 bus locations, connected by 32 transmission lines, 4 transformers
and 2 underground cables.  The transformers tie together two system areas at
230kV and 138kV.  The model has 32 generators of 9 different types (See Table
X) with a total capacity of 3.45GW and load at 17 locations, totalling 2.85GW.
Generator costs are quadratic functions of output, defined by the parameters in
Table Y.  Figure X plots the cost functions for each type of generator over
their production range and illustrates their categorisation by fuel type.  Data
for the model is provided in Appendix \ref{adx:ieee_rts} and the connectivity
of branches and the location of generators and loads is illustrated in
Figure~Y.

The generating stock is divided into 5 portfolios, as listed in Table Z, that
are each endowed to a learning agent.  The synchronous generator is associated
with a passive agent that always offers at marginal cost i.e.~\$/MWh~0.0.
Markups of offer price are restricted a maximum of 30\% and discrete markup
steps of 10\% are defined for value function based methods.

%\input{tikz/ieee79rts}


\section{Results}
\section{Discussion}
\label{sec:discuss}
\section{Conclusions}
