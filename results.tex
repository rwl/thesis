\chapter{Nash Equilibrium Analysis}
\label{ch:nashanalysis}
This chapter examines the convergence to a Nash equilibrium of agents competing
with portfolios of generating plant.  Value function based and policy gradient
reinforcement learning algorithms are compared in convergence to an optimal
policy using a six bus electric power system model.

\section{Introduction}
This thesis presents the first case of
policy gradient reinforcement learning methods being applied to electricity
trading problems.  As a first step it is necessary to confirm that when using
these methods, a system of multiple agents will converge to the same Nash
equilibrium\footnote{Informally, a Nash equlibrium is a point in a
non-cooperative game at which no player is motivated to deviate from its
strategy, as it would result in lower gain \cite{nash50,nash51}.} that
traditional closed-form simulation techniques produce.

This is the same approach used by \citeA{krause:nash06} before performing the
study of congestion management techniques that is reviewed in Section
\ref{sec:related_cong}.  Nash equilibria can be difficult
to determine in complex systems so the experiment presented here utilises a
model simple enough that it can be determined through exhaustive search.

By observing the actions taken and the reward received by each agent over the
initial simulation periods it is possible to compare different algorithms in
the speed and accuracy of their convergence to an optimal policy.  In the
following sections the objectives of this experiment are explicitly defined,
the setup of the simulations is explained and simulation results, with
discussion and critical analysis, are provided.

\section{Aims and Objectives}
Some elements of the simulations reported in this chapter are similar to those
presented by \citeA{krause:nash06}.  One initial aim of this work is to
reproduce their findings as a means of validating the approach.  The additional
objectives are to show:
\begin{itemize}
  \item That policy gradient methods converge to the same Nash equilibrium as
  value function based methods and tradtional closed-form simulations,
  \item The charateristics of different learning methods by examining the
  nature of their convergence to an optimal policy.
%   \item The sensitivity of policy convergence to algorithm parameter choices
%   and policy function approximation structure.
\end{itemize}
In meeting these objectives a basis for using policy gradient methods in more
complex simulations would be created.  It would show that they are capable of
learning basic policies and provide guidance for the selection of algorithm
parameters.

\section{Method of Simulation}
% Each learning method is tested individually using a range of parameter
% configurations.  A power system model with one bus, one generator $k$ and
% one dispatchable load $l$.  In this
% context, the market clearing process is equivalent to creating offer and bids
% stacks and finding the point of intersection.  A passive agent is associated
% with the dispatchable load.  This agent bids for $-p_{g,l}^{min}$ at marginal
% cost each period regardless of environment state or reward signal.  A
% dispathcable load is used instead of a constant load to allow a price to be
% set. Generator $k$ is given sufficient capacity to supply the demand
% of the dispatchable load, $p_{g,k}^{max} > -p_{g,l}^{min}$, and the marginal of
% the $k$ is half that of the load $l$.  The generator and dispatchable load
% attributes are given in Table X.  A price cap for the market is set to twice the
% marginal cost of the $l$ at full capacity, $p_{g,l}^{min}$.  The DC optimal
% power flow formulation (See Section \ref{sec:opf}, above) is used to clear the
% market and reactive power trade is omitted.
Learning methods are compared in this experiment by repeating the same
simulation with different algorithms used by the agents.  An alternative
might be to use a combination of methods in the same simulation, but the
approach used here is intended to be an extension of the work by
\citeA{krause:nash06}.

Each simulation uses the six bus electric power system model adapted from
\citeA[pp.~104, 112, 119, 123-124, 549]{wood:pgoc}.  The model provides a simple
environment for trade with a small number of generators and branch flow
constraints that slightly increase the complexity of the Nash equilibria.  The
six buses are connected by eleven transmission lines at 230kV. The model
contains three generating units with a total capacity of 440MW and loads at
three locations, each of 70MW. The connectivity of the branches and the
locations of the generators and loads is shown in Figure~\ref{fig:case6ww}.
Data for the power system model was taken from a case provided with
\textsc{Matpower}, is listed in Appendix \ref{adx:case6ww} and is distributed
with the software developed for this thesis (See Appendix \ref{sec:pylon}).

Two sets of quadratic generator operating cost functions, of the form
$c(p_i)=ap_i^2+bp_i+c$ where $p_i$ is the out put of generator $i$, are defined
in order to create two different equilibria for investigation.  The coefficients
$a$, $b$ and $c$ for the first cost configuration are listed in Table
\ref{tbl:case6ww_gencost1}. This cost configuration defines two low cost
generators that can not offer a price greater than the marginal cost of the most
expensive generator when the maximum markup is applied.  The second set of
coefficients is listed in Table \ref{tbl:case6ww_gencost2}.  This configuration
narrows the cost differences such that offer prices may overlap and may exceed
the marginal cost of the most expensive generator.  To strengthen the penalty
for not being dispatched and shudown cost $C_{down}$ of \$100 is specified in
this configuration.

\ifthenelse{\boolean{includefigures}}{\input{tikz/case6ww}}{}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Gen &$C_{down}$ &$a$ &$b$ &$c$ \\
\hline\hline
 1 &0 &0.0 &4.0 &200.0 \\
 2 &0 &0.0 &3.0 &200.0 \\
 3 &0 &0.0 &6.0 &200.0 \\
\hline
\end{tabular}
\caption{Generator cost configuration 1.}
\label{tbl:case6ww_gencost1}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Gen &$C_{down}$ &$a$ &$b$ &$c$ \\
\hline\hline
 1 &100 &0.0 &5.1 &200.0 \\
 2 &100 &0.0 &4.5 &200.0 \\
 3 &100 &0.0 &6.0 &200.0 \\
\hline
\end{tabular}
\caption{Generator cost configuration 2.}
\label{tbl:case6ww_gencost2}
\end{center}
\end{table}

As in \citeA{krause:nash06}, no load profile is defined for the simulation.
The system load is assumed to be peak for all simulation periods, thus only one state is defined for methods
using look-up tables.  Each simulation step is assumed to be one hour long.

The minimum operating point, $P^{min}$, for all generators is zeroed so as to
simplify the experiment and avoid the need to use the unit de-commitment
algorithm.  The maximum capacity for the most expensive generator
$P^{max}_3=220$MW such that it may supply almost all of the load if dispatched.
This generator is associated with a passive agent that always offers full
capacity at marginal cost.  For the other generators $P^{max}_1=110$MW and
$P^{max}_2=110$MW.  These two generators are each associated with an active
learning agent whose activity in the market is restricted to one offer of
maximum capacity in each period, at a price representing a markup of between 0
and 30\% on marginal cost.  Methods restricted to discrete action may markup in
steps of 10\%, giving possible markup actions of 0\%, 10\%, 20\% and 30\%.  No
capacity withholding is allowed.  The market price cap is set such that it is
never reached by any markup and does not complicate the experiment.
Discriminatory pricing (pay-as-bid) is used in order to provide a clearer reward
signal to agents with low cost generators.

This simulation compares Q-learning, ENAC, REINFORCE and the variant Roth-Erev
technique.  Default algorithm parameter values from PyBrain are used and no
attempt to study parameter sensitivity or function approximator design is made.

For Q-learning $\alpha=0.3$, $\gamma=0.99$ and $\epsilon$-greedy action
selection is used with $\epsilon=0.9$ and $d=0.98$. For Roth-Erev learning
$\epsilon=0.55$, $\phi=0.3$ and Boltzmann action selection is used with
$\tau=100$ and $d=0.99$.

Both REINFORCE and ENAC use a two-layer neural network
with one linear input node, one $\tanh$ output node, no bias nodes and with
weights initialised to zero.  A two-step episode is defined for the policy
gradient methods and five episodes are performed per learning step.  The
exploration paramter $\sigma$ for these methods is initialised to zero and
adjusted manually after each episode such that:
\begin{equation}
\sigma_{t} = d(\sigma_{t-1}-\sigma_{n})+\sigma_{n}
\end{equation}
where $d=0.998$ is a decay parameter and $\sigma_{n}=-0.5$ specifies the
value that is converged to asymtotically.  In each simulation the learning rate
$\gamma=0.01$ for the policy gradient methods, apart from for ENAC under cost
configuration 2 where $\gamma=0.005$.  All active agents use the same parameter
values in each simulation.

As in \citeA{krause:nash06}, the point of Nash equilibrium is established by
computing each agent's reward for all possible combinations of markup.  The
rewards for Agent 1 and Agent 2 under cost configuration 1 are given in Table
\ref{tbl:nash1}.  The Nash equilibrium points are marked with a *.  It shows
that the optimal policy for each agent is to apply the maximum markup to each
offer as this never results in their generators failing to be dispatched. The
rewards under cost configuration 2 are given in Table \ref{tbl:nash2}. It
shows that the optimal point occurs when Agent 2 applies its maximum markup
and Agent 1 offers a price just below the marginal cost of the passive agent's
generator.

\begin{table}
\begin{center}
\begin{small}
\begin{tabular}{c.{2.2}|.{2.1}.{2.1}|.{2.1}.{2.1}|.{2.1}.{2.1}|.{3.1}.{2.1}|}
\cline{3-10}
 & &\multicolumn{8}{c|}{$G_1$} \\
\cline{3-10}
 & &\multicolumn{2}{c|}{0.0\%} &\multicolumn{2}{c|}{10.0\%} &\multicolumn{2}{c|}{20.0\%} &\multicolumn{2}{c|}{30.0\%} \\
 & &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$G_2$}} &0.0\% &0.0 &0.0 &40.0 &0.0 &80.0 &0.0 &120.0 &0.0 \\
\multicolumn{1}{|c|}{} &10.0\% &0.0 &33.0 &40.0 &33.0 &80.0 &33.0 &120.0 &33.0 \\
\multicolumn{1}{|c|}{} &20.0\% &0.0 &66.0 &40.0 &66.0 &80.0 &66.0 &120.0 &66.0 \\
\multicolumn{1}{|c|}{} &30.0\% &0.0 &99.0 &40.0 &99.0 &80.0 &99.0 &120.0^*
&99.0^* \\
\hline
\end{tabular}
\caption{Agent rewards under cost configuration~1}
\label{tbl:nash1}
\end{small}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{small}
\begin{tabular}{c.{2.2}|.{2.1}.{3.1}|.{2.1}.{3.1}|.{2.1}.{3.1}|.{2.1}.{3.1}|}
\cline{3-10}
 & &\multicolumn{8}{c|}{$G_1$} \\
\cline{3-10}
 & &\multicolumn{2}{c|}{0.0\%} &\multicolumn{2}{c|}{10.0\%} &\multicolumn{2}{c|}{20.0\%} &\multicolumn{2}{c|}{30.0\%} \\
 & &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$G_2$}} &0.0\% &0.0 &0.0 &51.0 &0.0 &0.0 &0.0 &0.0 &0.0 \\
\multicolumn{1}{|c|}{} &10.0\% &0.0 &49.5 &51.0 &49.5 &0.0 &49.5 &0.0 &49.5 \\
\multicolumn{1}{|c|}{} &20.0\% &0.0 &92.2 &51.0 &99.0 &0.0 &99.0 &0.0 &99.0 \\
\multicolumn{1}{|c|}{} &30.0\% &0.0 &126.8 &54.8^* &138.4^* &0.0 &148.5 &0.0
&148.5 \\
\hline
\end{tabular}
\caption{Agent rewards under cost configuration~2}
\label{tbl:nash2}
\end{small}
\end{center}
\end{table}

\section{Simulation Results}
Each action taken by an agent and the consequent reward is recorded for each
simulation.  Values are averaged over the ten simulation runs and standard
deviations are calculated using the formula
\begin{equation}
SD = \sqrt{\frac{1}{N-1}\sum_{i=0}^{N}(x_i - \bar{x})^2}
\end{equation}
where $x_i$ is the action or reward value in simulation $i$ of $N$ simulation
runs and $\bar{x}$ is the mean of the values.

Figure \ref{fig:5_1_action_a1} shows the average markup on marginal cost and
the standard deviation over the ten simulation runs for Agent 1 under price
configuration 1 using the variant Roth-Erev, Q-learning, REINFORCE and ENAC
learning methods.  The second $y$-axis in each plot realtes to the exploration
parameter for each method.  Figure \ref{fig:5_1_action_a2} plots the same
quantities for Agent 2.  Plots of reward are not given as generator prices and
the market are configured such that an agent's reward is directly proportional
to its action.  The plots are vertically aligned and have equal $x$-axis limits
to assist algorithm comparison.

\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_1_action_a1}
	  \caption{Average markup for agent 1 and standard deviation over 10 runs.}
	  \label{fig:5_1_action_a1}
	\end{figure}

	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_1_action_a2}
	  \caption{Average markup for agent 2 and standard deviation over 10 runs.}
	  \label{fig:5_1_action_a2}
	\end{figure}
}{}

Figures \ref{fig:5_2_action_a1} and \ref{fig:5_2_reward_a1} plot the average
markup and reward over ten simulation runs for Agent 1 and Agent 2,
respectively, under price configuration 2 and for the variant Roth-Erev,
Q-learning learning methods.  The figures show actual values for one simulation
run of REINFORCE and ENAC as the number of interactions and variation in values
makes the results difficult to observe.  Not all $x$-axis extents are equal in
these two figures.

\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_2_action_a1}
	  \caption{Average markup for agent 1 and standard deviation.}
	  \label{fig:5_2_action_a1}
	\end{figure}
% 	\begin{figure}
% 	  \centering
% 	  \includegraphics{figures/fig5_2_action_a2}
% 	  \caption{Average markup for agent 2 and standard deviation.}
% 	  \label{fig:5_2_action_a2}
% 	\end{figure}
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_2_reward_a1}
	  \caption{Average reward for agent 1 and standard deviation.}
	  \label{fig:5_2_reward_a1}
	\end{figure}
% 	\begin{figure}
% 	  \centering
% 	  \includegraphics{figures/fig5_2_reward_a2}
% 	  \caption{Average reward for agent 2 and standard deviation.}
% 	  \label{fig:5_2_reward_a2}
% 	\end{figure}
}{}

\section{Discussion and Critical Analysis}
Under cost configuration 1 the agents face a relatively simple control task and
receive a clear reward signal that is directly proportional to their markup. The
results show that all of the methods consistently converge to the point of Nash
equilibrium.  The variant Roth-Erev method show least variation around the mean
when converged due to the use of Boltmann exploration with a then low
temperature parameter value.  The constant variation around the mean that can be
seen for Q-learning once is has converged is due to the use of $\epsilon$-greedy
action selection and can be removed if a Boltmann explorer is used.  Empirical
studies have also shown that the speed of convergence is largely determined by
the rate at which the exploration parameter value is reduced.  However, the
episodic nature of the policy gradient methods requires them to make several
interaction per learning step and therefore a larger number of initial
exploration steps.  Policy gradient methods can also be highly sensitive to
the learning rate parameter and high values must be avoided if the policy is
to converge.

Cost configuration 2 provides a more challenging control problem in which
Agent~1 must learn to undercut the passive agent.  The results show that the
variant Roth-Erev and Q-learning methods both consistently learn their optimal
policy and converge to the Nash equilibrium.  However, there is space for
Agent~1 to markup its offer by slightly more than 10\% and still undercut the
passive agent, but methods with discrete actions are not able to exploit this
and receive the additional profit.

The results for the policy gradient methods under cost configuration 2 show that
these methods learn to reduce their markup if their offer price starts to exceed
that of the passive agent and the reward signal drops.  However, a chattering
effect below the Nash equilibrium point can be clearly seen for ENAC and the method
does not learn to always undercut the other agent.  These methods also require
a much larger number of simulation steps and for the exploration parameter to
be decayed more slowly if they are to produce this behaviour.  This is
due to the need for a lower learning rate that ensures fine policy
adjustments can be made and again for several interactions to be performed between each learning
step.
% When using REINFORCE or ENAC, Agent~2 tends also to learn to maximise its
% markup, but less consistently.  Agent~1 typically learns to undercut the
% passive agent, but does not converge to a consistent value.  The problem is
% similar to the cliff-edge walking problems often used as benchmarks in
% reinforcement learning research and may be difficult to approximate a policy
% for using a small number of $\tanh$ functions. It may be possible to improve
% the performance of these agents through more educated policy function
% approximator design. but these methods are not really intended for operation
% in such simple environments.

\section{Summary}
This experiment confirms the convergence to a Nash equilibrium of the
Q-learning methods that is published in \citeA{krause:nash06} and, to a degree,
extends the conclusion to policy gradient methods.  The results show that
while these methods do converge to the same or similar policies as the
Q-learning and Roth-Erev methods, they do not exhibit the same level of
consistency.  Value function based methods or the Roth-Erev method may be the
most suitable choice of algorithm in the simple electricity market simulations
typically found in the literature.

The simulations conducted here do not exploit any of the abilities of policy
gradient methods to utilise multi-dimensional continuous state information and
their behaviour in more complex electricity market environments deserves
investigation.

%\section{Summary}


% \chapter{Competitive Power Trade}
% Having compared the learning methods in a one-player context, this section
% describes the method used to pit them against one and other and compare their
% performance.
%
% \section{Aims \& Objectives}
% Competition is fundamental to markets and this experiment aims to compare
% learning methods in a complex dynamic market environment with multiple
% competing participants.  The objective is to compare:
% \begin{itemize}
%   \item Performance, in terms of profitability, over a finite number of
%   periods,
%   \item Profitability when trading both active and reactive power.
%   \item Consistency of profit making and,
%   \item Sensitivity to algorithm parameter changes.
% \end{itemize}
%
% \section{Method of Simulation}
% Figure X illustrates the structure of the six bus power system model, from
% \cite{wood:pgoc}, with three generators and fixed demand at three of the buses
% used to provide a dynamic environment with typical system values.  Bus,
% branch and generator attribute values are stated in Tables X, Y, Z,
% respectively.  Three learning methods are compared in six simulations
% encapsulating all method--generator combinations.
%
% A price cap $c_{cap}$ of twice the marginal cost of the most expensive generator
% at full capacity is set by the market.  The simulations are repeated for with agents
% actions composing both price and quantity and with just price.  For the
% value-function methods, the state is defined by the market clearing price from
% the previous period, divided equally into $x_s$ discrete states between $0$ and
% $c_{cap}$.  The state vector $s_t$ for the policy gradient methods consists of
% the market clearing price and generator set-point from the previous period.
% \begin{equation}
% s_t =
% \begin{bmatrix}
% c_{mcp}\\
% p_g
% \end{bmatrix}
% \end{equation}
% The script used to conduct the simulation is provided in Listing X.

\chapter{System Constraint Exploitation}
\label{ch:exploitation}
% One of the main features of agents using policy gradient learning methods and
% artifical neural networks for policy function approximation is their ability to
% accept many signals of continuous sensor data.  This section describes an
% experiment in which the power system is severly constrained for certain
% periods, resulting in elevated nodal marginal prices in particular areas.  The
% methods are tested in their ability to exploit these constraints and improve
% their total accumulated reward.
This chapter explores the exploitation of constraints in electric power
system models by agents whose behaviour is determined by reinforcement learning
algorithms.  Value function based and policy gradient methods are compared
using the 24-bus IEEE Reliability Test System with dynamic load.

\section{Introduction}
Having explored the basic properties of four learning methods in
Chapter~\ref{ch:nashanalysis}, this experiment examines them under a
more complex dynamic scenario.  The experiment explores the multi-dimensional,
continuous state space handling abilities of policy gradient methods in the
context of \textit{learning to trade power}.

Control of a portfolio of generators using continuous sensor data from
simulations of the IEEE Reliability Test System (RTS) \cite{ieee79rts} is
investigated.  The system is constrained by bus voltage and generator capacity
limits as the system demand cycles through daily load profiles. By
observing the actions taken and the rewards received by the agents during these
periods it is examined if policy gradient methods can successfully observe and
exploit the constraints.

\section{Aims and Objectives}
This experiment aims to compare the operation of learning methods in dynamic
electric power system environments.  Specifically, the objectives are to
determine:
\begin{itemize}
  \item If policy gradient methods can be used to achieve greater profit under
  dynamic loading conditions.
%   \item If policy gradient methods can exploit outages and the resulting system
%   constraints to further increased profit.
  \item The value of using AC optimal power flow formulations in agent base
  electricity market simulation.
\end{itemize}
Meeting these objectives aims to demonstrate the value of policy gradient
methods in electricity market participant modelling and determine if they
warrant further research in this domain.

\section{Method of Simulation}
In this experiment learning methods are compared by repeating simulations
of competitive electricity trade with agents using different types of
algorithm. Some simplification of the state and action domains for the value
function based methods is required, but the portfolios of generation and load
profiles are constant.

The IEEE RTS provides the power system model and load profiles used in each
simulation.  The model has 24 bus locations, connected by 32 transmission lines,
4 transformers and 2 underground cables.  The transformers tie together two
areas at 230kV and 138kV.  The model has 32 generators of 9 different types (See
Table \ref{tbl:ieee_rts_gencosts}) with a total capacity of 3.45GW and load at
17 locations, totalling 2.85GW. Generator costs are quadratic functions of
output, defined by the parameters in Table \ref{tbl:ieee_rts_gencosts}. Figure
\ref{fig:ieee_rts_gencost_plot} plots the cost functions for each type of
generator over their production range and illustrates their categorisation by
fuel type.  The generator cost data was provided by Georgia Tech Power Systems
Control and Automation Laboratory.  All of the data for the model is provided in
Appendix \ref{adx:ieee_rts} and the connectivity of branches and the location of
generators and loads is illustrated in Figure \ref{fig:ieee79rts}.

\begin{table}
\begin{center}
\begin{tabular}{r|c|.{1.5}|.{3.4}|.{3.3}|l}
\hline
Code &$C_{down}$ &\multicolumn{1}{c}{$a$} &\multicolumn{1}{|c|}{$b$}
&\multicolumn{1}{c|}{$c$} &Type \\
\hline\hline
% U12 &0.0 &0.32841 &56.564 &86.385 &Oil \\
% U20 &0.0 &0.0 &130.0 &400.685 &Oil \\
U50 &0 &0.0 &0.001 &0.001 &Hydro \\
U76	 &0	&0.01414	&16.0811	&212.308 &Coal \\
U100 &0	&0.05267	&43.6615	&781.521 &Oil \\
U155 &0	&0.00834	&12.3883	&382.239 &Coal \\
U197 &0	&0.00717	&48.5804	&832.758 &Oil \\
U350 &0	&0.00490	&11.8495	&665.109 &Coal \\
U400 &0	&0.00021	&4.4231	&395.375 &Nuclear \\
\hline
\end{tabular}
\caption{Cost parameters IEEE RTS generator types.}
\label{tbl:ieee_rts_gencosts}
\end{center}
\end{table}

\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/ieee_rts_gencosts}
	  \caption{Generator cost functions for the IEEE Reliability Test System}
	  \label{fig:ieee_rts_gencost_plot}
	\end{figure}

	\begin{figure}
	  \centering
	  \includegraphics{figures/ieee_rts_profiles}
	  \caption{Hourly, daily and weekly load profile plots from the IEEE
	  Reliability Test System}
	  \label{fig:ieee_rts_profiles}
	\end{figure}
}{}

The generating stock is divided into 5 portfolios (See Table
\ref{tbl:agent_portfolios}) that are each endowed to a learning agent.  The
synchronous condenser is associated with a passive agent that always offers
at marginal cost i.e.~\$/MWh~$0.0$

Markups on marginal cost are restricted a maximum of 30\% and discrete markup
steps of 10\% are defined for value function based methods.  Agents using
policy gradient learning methods can markdown the capacity offered for each of
its generators by a maximum of 30\%.

The environment state for all agents contains a forecast of total system
demand.  The system demand follows an hourly profile that is adjusted according
to the day of the week and the time of year.  The profiles are provided by the
IEEE RTS and are shown in Figure \ref{fig:ieee_rts_profiles}.  When using
value function based methods or the Roth-Erev learning algorithm, the
continuous state is divided into 10 discrete states between minimum and maximum
total system load.  The state vector for agents with policy gradient methods
additionally contains the voltage magnitude at each bus.  Branch flow limits in
the RTS are high and are not reached at peak demand.  Otherwise, the state
vector might also contain branch power flow values.  Generator capacity limits
are often binding in the RTS, but the output of other generators is deemed to
be hidden from the agents.

\begin{table}
\begin{center}
% \begin{tabular}{c|c|c}
% \hline
% Agent &Type &\multicolumn{1}{c}{Buses} \\
% \hline\hline
% \multirow{3}{*}{$A_1$} &U20 &1,1,2,2 \\
%  & U76 &1,1,2,2 \\
%  & U100 &7,7,7\\
% \hline
% \multirow{3}{*}{$A_2$} &U155 &23,23 \\
%  & U197 &13,13,13 \\
%  & U300 &23 \\
% \hline
% \multirow{2}{*}{$A_3$} &U12 &15,15,15,15,15 \\
%  & U155 &15,16 \\
% \hline
% \multirow{2}{*}{$A_4$} &U50 &22,22,22,22,22,22 \\
%  & U400 &18,21 \\
% \hline
% $A_5$ & Sync.~Cond. &14 \\
% \hline
% \end{tabular}

\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Agent} &U50 &U76 &U100 &U155 &U197 &U350 &U400 &Total \\
 &Hydro &Coal &Oil &Coal &Oil &Coal &Nuclear &(MW) \\
\hline
1 & &2$\times$ & &1$\times$ & & &1$\times$ &707 \\
2 & &2$\times$ & &1$\times$ & & &1$\times$ &707 \\
3 &6$\times$ & & & &3$\times$ & & &891 \\
4 & & &3$\times$ &2$\times$ & &1$\times$ & &960 \\
\hline
\end{tabular}

\caption{Agent portfolios.}
\label{tbl:agent_portfolios}
\end{center}
\end{table}

\ifthenelse{\boolean{includefigures}}{\input{tikz/ieee79rts}}{}

\section{Simulation Results}
\section{Discussion and Critical Analysis}
\label{sec:discuss}
\section{Summary}
