\chapter{Nash Equilibrium Analysis}
\label{ch:nashanalysis}
This chapter examines the convergence to Nash
equilibria of agents competing with portfolios of generating plant.  Value
function based and policy gradient reinforcement learning algorithms are
compared in their ability to converge to an optimal policy using a six bus
electric power system model.

\section{Introduction}
To the best of the author's knowledge, this thesis presents the first case of
policy gradient reinforcement learning methods being applied to electricity
trading problems.  As a first step it is necessary to confirm that when using
these methods, a system of multiple agents will converge to the same Nash
equilibrium\footnote{Informally, a Nash equlibrium is a point at which no
player is motivated to deviate from its strategy as it would result in a lower
gain.} that conventional closed-form simulation techniques produce.

This is the same approach used by \citeA{krause:nash06} before performing the
study of congestion management techniques that is reviewed in Section
\ref{sec:related_cong}.  Nash equilibria can be difficult
to determine in complex systems so the experiment presented here utilises a
model simple enough that it can be determined through exhaustive search.

By observing the actions taken and the reward received by each agent over the
initial simulation periods it is possible to compare different configurations
of the algorithms in their speed of convergence to an optimal policy.  In the
following sections the objectives of this experiment are explicitly defined,
the setup of the simulations is explained and simulation results, with discussion
and critical analysis, are provided.

\section{Aims and Objectives}
Some elements of this experiment are very similar to those presented in
\citeA{krause:nash06} and one initial aim is to reproduce those results.
The additional objectives are to show that:
\begin{itemize}
  \item Policy gradient methods converge to the same Nash equilibrium as value
  function based methods,
  \item The differences in speed of convergence to an optimal policy between
  the learning methods.
%   \item The sensitivity of policy convergence to algorithm parameter choices
%   and policy function approximation structure.
\end{itemize}
Meeting these objectives aims to provide a basis for more complicated
experiments that are less intuitively tractable.

\section{Method of Simulation}
% Each learning method is tested individually using a range of parameter
% configurations.  A power system model with one bus, one generator $k$ and
% one dispatchable load $l$.  In this
% context, the market clearing process is equivalent to creating offer and bids
% stacks and finding the point of intersection.  A passive agent is associated
% with the dispatchable load.  This agent bids for $-p_{g,l}^{min}$ at marginal
% cost each period regardless of environment state or reward signal.  A
% dispathcable load is used instead of a constant load to allow a price to be
% set. Generator $k$ is given sufficient capacity to supply the demand
% of the dispatchable load, $p_{g,k}^{max} > -p_{g,l}^{min}$, and the marginal of
% the $k$ is half that of the load $l$.  The generator and dispatchable load
% attributes are given in Table X.  A price cap for the market is set to twice the
% marginal cost of the $l$ at full capacity, $p_{g,l}^{min}$.  The DC optimal
% power flow formulation (See Section \ref{sec:opf}, above) is used to clear the
% market and reactive power trade is omitted.
Learning methods are compared in this experiment by repeating the same
simulation with the agents using different algorithms.  An alternative
might be to use a combination of methods in the same simulation, but the
approach used here is intended to be an extension of the work by
\citeA{krause:nash06}.

Each simulation uses the six bus electric power system model adapted from
\citeA[pp.~104, 112, 119, 123-124, 549]{wood:pgoc}.  The six buses are
connected by eleven transmission lines at 230kV.  The model contains three
generating units with a total capacity of 440MW and loads at three locations, each of
70MW. The connectivity of the branches and the locations of the generators and
loads is shown in Figure~\ref{fig:case6ww}.  Data for the power system model is
provided in Appendix \ref{adx:case6ww} and is distributed with the software
developed for this thesis (See Appendix \ref{sec:pylon}).

Two sets of generator operating costs are defined in order to create two
different equilibria for investigation. The first set is listed in Table
\ref{tbl:case6ww_gencost1}. It defines two low cost generators that can not
offer a price greater than the marginal cost of the most expensive generator
when the maximum markup is applied.  The second configuration is listed in
Table \ref{tbl:case6ww_gencost2} and narrows the cost differences such that
offer prices overlap and may exceed the marginal cost of the most expensive
generator.

\ifthenelse{\boolean{includefigures}}{\input{tikz/case6ww}}{}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Gen &$C_{down}$ &$a$ &$b$ &$c$ \\
\hline\hline
 1 &100 &0.0 &4.0 &200.0 \\
 2 &100 &0.0 &3.0 &200.0 \\
 3 &100 &0.0 &6.0 &200.0 \\
\hline
\end{tabular}
\caption{Generator cost configuration 1 for 6-bus case.}
\label{tbl:case6ww_gencost1}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Gen &$C_{down}$ &$a$ &$b$ &$c$ \\
\hline\hline
 1 &100 &0.0 &5.1 &200.0 \\
 2 &100 &0.0 &4.5 &200.0 \\
 3 &100 &0.0 &6.0 &200.0 \\
\hline
\end{tabular}
\caption{Generator cost configuration 2 for 6-bus case.}
\label{tbl:case6ww_gencost2}
\end{center}
\end{table}

No load profile is defined, the system load is assumed to be peak for all
simulation periods, so only one system state is defined for the value function
based algorithms.  The minimum operating point, $P^{min}$, for all generators
is made to be zero so as to simplify the experiment and avoid the need to
use the unit de-commitment algorithm.

The maximum capacity for the most expensive generator $P^{max}_3=220$MW such
that it may supply almost all of the load if it is dispatched.  This
generator is associated with a passive agent that always offers a marginal
cost.  For the other generators $P^{max}_1=110$MW and $P^{max}_2=110$MW.  These
two generators are each associated with an active learning agent whose activity
in the market is restricted to one offer of maximum capacity in each period,
at a price representing a markup of between 0 and 30\% on marginal cost.
Value function based methods are restricted to discrete markup steps of 10\%,
giving possible markup actions of 0, 10, 20 and 30\%. The market price cap is
set such that it is never reached by any markup and does not complicate the
experiment.  Discriminatory pricing (pay-as-bid) is used in order to provide a
clearer reward signal to agents with low cost generators.

The learning methods compared are Q-learning, ENAC, REINFORCE and the variant
Roth-Erev technique.  For Q-learning $\alpha=0.3$, $\gamma=0.99$ and
$\epsilon$-greedy action selection is used with $\epsilon=0.9$ and $d=0.97$.
For Roth-Erev learning $\epsilon=0.55$, $\phi=0.3$ and Boltzmann action
selection is used with $\tau=100$ and $d=0.98$.  Both REINFORCE and ENAC use a
three-layer neural network with one linear input node, two hidden $\tanh$
nodes, one output $\tanh$ node and bias nodes in the hidden and output layers.

As in \citeA{krause:nash06}, the point of Nash equilibrium is established by
computing each agent's reward for all possible combinations of markup.  The
rewards for Agent 1 and Agent 2 under cost configuration 1 are given in Table
\ref{tbl:nash1}.  The Nash equilibrium points are marked with a *.  It shows
that the optimal policy for each agent is to apply the maximum markup to each
offer as this never results in their generators failing to be dispatched. The
rewards under cost configuration 2 are given in Table \ref{tbl:nash2}. It
shows that the optimal point occurs when Agent 2 applies its maximum markup
and Agent 1 offers a price just below the marginal cost of the passive agent's
generator.

\begin{table}
\begin{center}
\begin{small}
\begin{tabular}{c.{2.2}|.{2.1}.{2.1}|.{2.1}.{2.1}|.{2.1}.{2.1}|.{3.1}.{2.1}|}
\cline{3-10}
 & &\multicolumn{8}{c|}{$G_1$} \\
\cline{3-10}
 & &\multicolumn{2}{c|}{0.0\%} &\multicolumn{2}{c|}{10.0\%} &\multicolumn{2}{c|}{20.0\%} &\multicolumn{2}{c|}{30.0\%} \\
 & &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$G_2$}} &0.0\% &0.0 &0.0 &40.0 &0.0 &80.0 &0.0 &120.0 &0.0 \\
\multicolumn{1}{|c|}{} &10.0\% &0.0 &33.0 &40.0 &33.0 &80.0 &33.0 &120.0 &33.0 \\
\multicolumn{1}{|c|}{} &20.0\% &0.0 &66.0 &40.0 &66.0 &80.0 &66.0 &120.0 &66.0 \\
\multicolumn{1}{|c|}{} &30.0\% &0.0 &99.0 &40.0 &99.0 &80.0 &99.0 &120.0^*
&99.0^* \\
\hline
\end{tabular}
\caption{Agent rewards under cost configuration~1}
\label{tbl:nash1}
\end{small}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{small}
\begin{tabular}{c.{2.2}|.{2.1}.{3.1}|.{2.1}.{3.1}|.{2.1}.{3.1}|.{2.1}.{3.1}|}
\cline{3-10}
 & &\multicolumn{8}{c|}{$G_1$} \\
\cline{3-10}
 & &\multicolumn{2}{c|}{0.0\%} &\multicolumn{2}{c|}{10.0\%} &\multicolumn{2}{c|}{20.0\%} &\multicolumn{2}{c|}{30.0\%} \\
 & &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$G_2$}} &0.0\% &0.0 &0.0 &51.0 &0.0 &0.0 &0.0 &0.0 &0.0 \\
\multicolumn{1}{|c|}{} &10.0\% &0.0 &49.5 &51.0 &49.5 &0.0 &49.5 &0.0 &49.5 \\
\multicolumn{1}{|c|}{} &20.0\% &0.0 &92.2 &51.0 &99.0 &0.0 &99.0 &0.0 &99.0 \\
\multicolumn{1}{|c|}{} &30.0\% &0.0 &126.8 &54.8^* &138.4^* &0.0 &148.5 &0.0
&148.5 \\
\hline
\end{tabular}
\caption{Agent rewards under cost configuration~2}
\label{tbl:nash2}
\end{small}
\end{center}
\end{table}

\section{Simulation Results}
Each action taken by an agent and the consequent reward is recorded for each
simulation.  Values are averaged over the 10 simulation runs and standard
deviations are calculated using the formula
\begin{equation}
SD = \sum_{i=0}^{N}\frac{(x_i - m)^2}{N-1}
\end{equation}
where $x_i$ is the action or reward value in simulation $i$ of $N$ simulation
runs and $m$ is the mean of the values.

Figure \ref{fig:5_1_action_a1} plots the average markup on marginal cost and
the standard deviation over the 10 simulation runs for Agent 1 under price
configuration 1 using the variant Roth-Erev, Q-learning, REINFORCE and ENAC
learning methods.  The second $y$-axis in each plot gives the value of the exploration parameter for each method.  Figure \ref{fig:5_1_action_a2}
plots the same quantities for Agent 2.  Plots of reward are not given as
generator prices and the market are configured such that an agent's reward is
directly proportional to its action.

\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_1_action_a1}
	  \caption{Average markup for agent 1 and standard deviation over 10 runs.}
	  \label{fig:5_1_action_a1}
	\end{figure}

	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_1_action_a2}
	  \caption{Average markup for agent 2 and standard deviation over 10 runs.}
	  \label{fig:5_1_action_a2}
	\end{figure}
}{}

Figures \ref{fig:5_2_action_a1} and \ref{fig:5_2_action_a2} plot the average
markup for Agent 1 and Agent 2, respectively, under price
configuration 2 and again for the variant Roth-Erev, Q-learning, REINFORCE and
ENAC learning methods.  Figures \ref{fig:5_2_reward_a1} and
\ref{fig:5_2_reward_a2} plot the associated average \textit{rewards} for
Agent 1 and Agent 2.  Again the standard deviation and exploration parameter
values are plotted. The plots are vertically aligned and have equal $x$-axis
limits to assist algorithm comparison.
\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_2_action_a1}
	  \caption{Average markup for agent 1 and standard deviation.}
	  \label{fig:5_2_action_a1}
	\end{figure}
% 	\begin{figure}
% 	  \centering
% 	  \includegraphics{figures/fig5_2_action_a2}
% 	  \caption{Average markup for agent 2 and standard deviation.}
% 	  \label{fig:5_2_action_a2}
% 	\end{figure}
	\begin{figure}
	  \centering
	  \includegraphics{figures/fig5_2_reward_a1}
	  \caption{Average reward for agent 1 and standard deviation.}
	  \label{fig:5_2_reward_a1}
	\end{figure}
% 	\begin{figure}
% 	  \centering
% 	  \includegraphics{figures/fig5_2_reward_a2}
% 	  \caption{Average reward for agent 2 and standard deviation.}
% 	  \label{fig:5_2_reward_a2}
% 	\end{figure}
}{}

\section{Discussion and Critical Analysis}
Under the first generator price configuration the agents face a
relatively simple control task and receive a clear reward signal that is
directly proportional to their markup action.  The results show that all of
the methods consistently converge to the point of Nash equilibrium.  A
multitude of parameter and neural network structure variations could be
investigated and a sea of similar plots would be produced.  However, the
author's experience is that the speed of convergence is largely determined by
the rate at which the exploration parameter value is reduced. The policy
gradient methods are sensitive to high learning rate parameter values, but
make only very small policy adjustments if this parameter is set too low. All
of the plots for REINFORCE and ENAC show that the methods only converge to a
stable policy if the exploration parameter $\sigma$ is manually reduced to
below approximately~$-2$.

The second pricing configuration provides a more challenging control problem in
which there is some interdependence between the agent's rewards and where
Agent~1 must learn to undercut the passive agent.  The results show that the
variant Roth-Erev and Q-learning methods both consistently learn their optimal
policy and converge to the Nash equilibrium.  It should be noted that Agent~1
can markup its marginal price by slightly more than 10\% and still undercut
the passive agent, but these methods are restricted to discrete actions.

When using REINFORCE or ENAC, Agent~2 tends also to learn to maximise its
markup, but less consistently.  Agent~1 typically learns to undercut the
passive agent, but does not converge to a consistent value.  The problem is
similar to the cliff-edge walking problems often used as benchmarks in
reinforcement learning research and may be difficult to approximate a policy
for using a small number of $\tanh$ functions. It may be possible to improve
the performance of these agents through more educated policy function
approximator design.
%but these methods are not really intended for operation in
%such simple environments.

This experiment confirms the convergence to a Nash equilibrium of the
Q-learning methods that is published in \citeA{krause:nash06} and, to a degree,
extends the conclusion to policy gradient methods.  The results show that
while these methods do converge to the same or similar policies as the
Q-learning and Roth-Erev methods, they do not exhibit the same level of
consistency.  Value function based methods or the Roth-Erev method may be the
most suitable choice of algorithm in the simple electricity market simulations
typically found in the literature.

\section{Summary}
The simulations conducted here do not exploit any of
the abilities of policy gradient methods to utilise multi-dimensional
continuous state information and their behaviour in more complex environments
must be examined.

%\section{Summary}


% \chapter{Competitive Power Trade}
% Having compared the learning methods in a one-player context, this section
% describes the method used to pit them against one and other and compare their
% performance.
%
% \section{Aims \& Objectives}
% Competition is fundamental to markets and this experiment aims to compare
% learning methods in a complex dynamic market environment with multiple
% competing participants.  The objective is to compare:
% \begin{itemize}
%   \item Performance, in terms of profitability, over a finite number of
%   periods,
%   \item Profitability when trading both active and reactive power.
%   \item Consistency of profit making and,
%   \item Sensitivity to algorithm parameter changes.
% \end{itemize}
%
% \section{Method of Simulation}
% Figure X illustrates the structure of the six bus power system model, from
% \cite{wood:pgoc}, with three generators and fixed demand at three of the buses
% used to provide a dynamic environment with typical system values.  Bus,
% branch and generator attribute values are stated in Tables X, Y, Z,
% respectively.  Three learning methods are compared in six simulations
% encapsulating all method--generator combinations.
%
% A price cap $c_{cap}$ of twice the marginal cost of the most expensive generator
% at full capacity is set by the market.  The simulations are repeated for with agents
% actions composing both price and quantity and with just price.  For the
% value-function methods, the state is defined by the market clearing price from
% the previous period, divided equally into $x_s$ discrete states between $0$ and
% $c_{cap}$.  The state vector $s_t$ for the policy gradient methods consists of
% the market clearing price and generator set-point from the previous period.
% \begin{equation}
% s_t =
% \begin{bmatrix}
% c_{mcp}\\
% p_g
% \end{bmatrix}
% \end{equation}
% The script used to conduct the simulation is provided in Listing X.

\chapter{System Constraint Exploitation}
\label{ch:exploitation}
% One of the main features of agents using policy gradient learning methods and
% artifical neural networks for policy function approximation is their ability to
% accept many signals of continuous sensor data.  This section describes an
% experiment in which the power system is severly constrained for certain
% periods, resulting in elevated nodal marginal prices in particular areas.  The
% methods are tested in their ability to exploit these constraints and improve
% their total accumulated reward.
This chapter explores the exploitation of constraints in electric power
system models by agents whose behaviour is determined by reinforcement learning
algorithms.  Value function based and policy gradient methods are compared
using the 24-bus IEEE Reliability Test System with dynamic load.

\section{Introduction}
Having explored the basic properties of four learning methods in
Chapter~\ref{ch:nashanalysis}, this experiment examines them under a
more complex dynamic scenario.  The experiment explores the multi-dimensional,
continuous state space handling abilities of policy gradient methods in the
context of \textit{learning to trade power}.

Control of a portfolio of generators using continuous sensor data from
simulations of the IEEE Reliability Test System (RTS) \cite{ieee79rts} is
investigated.  The system is constrained by bus voltage and generator capacity
limits as the system demand cycles through daily load profiles. By
observing the actions taken and the rewards received by the agents during these
periods it is examined if policy gradient methods can successfully observe and
exploit the constraints.

\section{Aims and Objectives}
This experiment aims to compare the operation of learning methods in dynamic
electric power system environments.  Specifically, the objectives are to
determine:
\begin{itemize}
  \item If policy gradient methods can be used to achieve greater profit under
  dynamic loading conditions.
%   \item If policy gradient methods can exploit outages and the resulting system
%   constraints to further increased profit.
  \item The value of using AC optimal power flow formulations in agent base
  electricity market simulation.
\end{itemize}
Meeting these objectives aims to demonstrate the value of policy gradient
methods in electricity market participant modelling and determine if they
warrant further research in this domain.

\section{Method of Simulation}
In this experiment learning methods are compared by repeating simulations
of competitive electricity trade with agents using different types of
algorithm. Some simplification of the state and action domains for the value
function based methods is required, but the portfolios of generation and load
profiles are constant.

The IEEE RTS provides the power system model and load profiles used in each
simulation.  The model has 24 bus locations, connected by 32 transmission lines,
4 transformers and 2 underground cables.  The transformers tie together two
areas at 230kV and 138kV.  The model has 32 generators of 9 different types (See
Table \ref{tbl:ieee_rts_gencosts}) with a total capacity of 3.45GW and load at
17 locations, totalling 2.85GW. Generator costs are quadratic functions of
output, defined by the parameters in Table \ref{tbl:ieee_rts_gencosts}. Figure
\ref{fig:ieee_rts_gencost_plot} plots the cost functions for each type of
generator over their production range and illustrates their categorisation by
fuel type.  The generator cost data was provided by Georgia Tech Power Systems
Control and Automation Laboratory.  All of the data for the model is provided in
Appendix \ref{adx:ieee_rts} and the connectivity of branches and the location of
generators and loads is illustrated in Figure \ref{fig:ieee79rts}.

\begin{table}
\begin{center}
\begin{tabular}{r|c|.{1.5}|.{3.4}|.{3.3}|l}
\hline
Code &$C_{down}$ &\multicolumn{1}{c}{$a$} &\multicolumn{1}{|c|}{$b$}
&\multicolumn{1}{c|}{$c$} &Type \\
\hline\hline
% U12 &0.0 &0.32841 &56.564 &86.385 &Oil \\
% U20 &0.0 &0.0 &130.0 &400.685 &Oil \\
U50 &0 &0.0 &0.001 &0.001 &Hydro \\
U76	 &0	&0.01414	&16.0811	&212.308 &Coal \\
U100 &0	&0.05267	&43.6615	&781.521 &Oil \\
U155 &0	&0.00834	&12.3883	&382.239 &Coal \\
U197 &0	&0.00717	&48.5804	&832.758 &Oil \\
U350 &0	&0.00490	&11.8495	&665.109 &Coal \\
U400 &0	&0.00021	&4.4231	&395.375 &Nuclear \\
\hline
\end{tabular}
\caption{Cost parameters IEEE RTS generator types.}
\label{tbl:ieee_rts_gencosts}
\end{center}
\end{table}

\ifthenelse{\boolean{includefigures}}{
	\begin{figure}
	  \centering
	  \includegraphics{figures/ieee_rts_gencosts}
	  \caption{Generator cost functions for the IEEE Reliability Test System}
	  \label{fig:ieee_rts_gencost_plot}
	\end{figure}

	\begin{figure}
	  \centering
	  \includegraphics{figures/ieee_rts_profiles}
	  \caption{Hourly, daily and weekly load profile plots from the IEEE
	  Reliability Test System}
	  \label{fig:ieee_rts_profiles}
	\end{figure}
}{}

The generating stock is divided into 5 portfolios (See Table
\ref{tbl:agent_portfolios}) that are each endowed to a learning agent.  The
synchronous condenser is associated with a passive agent that always offers
at marginal cost i.e.~\$/MWh~$0.0$

Markups on marginal cost are restricted a maximum of 30\% and discrete markup
steps of 10\% are defined for value function based methods.  Agents using
policy gradient learning methods can markdown the capacity offered for each of
its generators by a maximum of 30\%.

The environment state for all agents contains a forecast of total system
demand.  The system demand follows an hourly profile that is adjusted according
to the day of the week and the time of year.  The profiles are provided by the
IEEE RTS and are shown in Figure \ref{fig:ieee_rts_profiles}.  When using
value function based methods or the Roth-Erev learning algorithm, the
continuous state is divided into 10 discrete states between minimum and maximum
total system load.  The state vector for agents with policy gradient methods
additionally contains the voltage magnitude at each bus.  Branch flow limits in
the RTS are high and are not reached at peak demand.  Otherwise, the state
vector might also contain branch power flow values.  Generator capacity limits
are often binding in the RTS, but the output of other generators is deemed to
be hidden from the agents.

\begin{table}
\begin{center}
% \begin{tabular}{c|c|c}
% \hline
% Agent &Type &\multicolumn{1}{c}{Buses} \\
% \hline\hline
% \multirow{3}{*}{$A_1$} &U20 &1,1,2,2 \\
%  & U76 &1,1,2,2 \\
%  & U100 &7,7,7\\
% \hline
% \multirow{3}{*}{$A_2$} &U155 &23,23 \\
%  & U197 &13,13,13 \\
%  & U300 &23 \\
% \hline
% \multirow{2}{*}{$A_3$} &U12 &15,15,15,15,15 \\
%  & U155 &15,16 \\
% \hline
% \multirow{2}{*}{$A_4$} &U50 &22,22,22,22,22,22 \\
%  & U400 &18,21 \\
% \hline
% $A_5$ & Sync.~Cond. &14 \\
% \hline
% \end{tabular}

\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Agent} &U50 &U76 &U100 &U155 &U197 &U350 &U400 &Total \\
 &Hydro &Coal &Oil &Coal &Oil &Coal &Nuclear &(MW) \\
\hline
1 & &2$\times$ & &1$\times$ & & &1$\times$ &707 \\
2 & &2$\times$ & &1$\times$ & & &1$\times$ &707 \\
3 &6$\times$ & & & &3$\times$ & & &891 \\
4 & & &3$\times$ &2$\times$ & &1$\times$ & &960 \\
\hline
\end{tabular}

\caption{Agent portfolios.}
\label{tbl:agent_portfolios}
\end{center}
\end{table}

\ifthenelse{\boolean{includefigures}}{\input{tikz/ieee79rts}}{}

\section{Simulation Results}
\section{Discussion and Critical Analysis}
\label{sec:discuss}
\section{Summary}
