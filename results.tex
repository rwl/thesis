\chapter{Nash Equilibrium Analysis}
\label{ch:learningtotrade}
This chapter examines the convergence to a Nash equilibrium of agents
compteting with portfolios of generating plant.  Value function based and
policy gradient reinforcement learning algorithms are compared in their speed
of convergence and sensitivity to parameter changes using a six bus electric
power system model.

\section{Introduction}
To the best of the author's knowledge, this thesis presents the first case of
policy gradient reinforcement learning methods being applied to electricity
trading problems.  As a first step it is necessary to confirm that when using
these methods, a system of multiple agents will converge to the same Nash
equilibrium that conventional closed-form simulation techniques produce.

This is the same approach by \citeA{krause:nash06} before performing the study
of congestion management techniques that is reviewed in Section
\ref{sec:related_cong}.  Informally, a Nash equlibrium is a point at which no
player is motivated to deviate from its strategy.  This can be difficult
to determine in complex systems so the experiment presented here utilises a
model simple enough that it can be determined through exhaustive search.

By observing the actions taken and the reward received by each agent over the
initial simulation periods it is possible to compare different configurations
of the algorithms in their speed of convergence to an optimal policy.  In the
following sections the objectives of this experiment are explictly defined, the
setup of the simulations is explained and simulation results are provided with
discussion and critical analysis.

\section{Aims and Objectives}
Some elements of this experiment are very similar to those presented in
\citeA{krause:nash06} and one aim is to reproduce those results.  Additionally,
the specific objectives are to show that:
\begin{itemize}
  \item Policy gradient methods converge to the same Nash equilibrium as value
  function based methods.
  \item The differences in speed of convergence to an optimal policy between
  the learning methods.
  \item The sensitivity of policy convergence to algorithm parameter choices
  and policy function approximation structure.
\end{itemize}
Meeting these objectives aims to provide a basis for more complicated
experiments that are less intuitively tractable.

\section{Method of Simulation}
% Each learning method is tested individually using a range of parameter
% configurations.  A power system model with one bus, one generator $k$ and
% one dispatchable load $l$.  In this
% context, the market clearing process is equivalent to creating offer and bids
% stacks and finding the point of intersection.  A passive agent is associated
% with the dispatchable load.  This agent bids for $-p_{g,l}^{min}$ at marginal
% cost each period regardless of environment state or reward signal.  A
% dispathcable load is used instead of a constant load to allow a price to be
% set. Generator $k$ is given sufficient capacity to supply the demand
% of the dispatchable load, $p_{g,k}^{max} > -p_{g,l}^{min}$, and the marginal of
% the $k$ is half that of the load $l$.  The generator and dispatchable load
% attributes are given in Table X.  A price cap for the market is set to twice the
% marginal cost of the $l$ at full capacity, $p_{g,l}^{min}$.  The DC optimal
% power flow formulation (See Section \ref{sec:opf}, above) is used to clear the
% market and reactive power trade is omitted.
Learning methods are compared in this experiment by repeating the same
simulation with the different algorithms in-place.  An alternative
might be to use a combination of methods in the same simulation, but the
approach used here is intended to be an extension of the work in
\citeA{krause:nash06}.

Each simulation uses the 6-bus electric power system model from \citeA[pp.~104,
112, 119, 123-124, 549]{wood:pgoc}.  The six buses in this model are at 230kV
and are connected by eleven transmission lines.  It contains three generating
units with a total capacity of 530MW and loads at three locations, each of
70MW. The generator opertaing costs are linear functions of production, defined by
the parameters in Table \ref{tbl:case6ww_gencost} and plotted in Figure X.
The connectivity of the branches and the locations of the generators and the
load is shown in Figure~\ref{fig:case6ww}.  Data for the power system model is
provided in Appendix \ref{adx:case6ww} and is distributed with the software
developed for this thesis (See Appendix \ref{sec:pylon}).

%\input{tikz/case6ww}

No load profile is defined, the system load is assumed to be peak for all
simulation periods, so only one system state is defined for the value function
based algorithms.  The minimum operating point, $P^{min}$, for all generators
is made to be zero so as to simplify the experiment and avoid the need to
use the unit decommitment algorithm defined in Section \ref{sec:decommit}.

Two active agents, and one passive agent that always offers at marginal cost,
are defined in each simulation and are each associated with one generating
unit. Their activity in the market is restricted to one offer of maximum
capacity in each period, at a price representing a markup of between 0 and 30\%
on marginal cost.  Value function based methods are restricted to discrete markup steps of
10\%, giving possible markup actions of 0, 10, 20 and 30\%, as illustrated in
Figure X.  The market price cap is set such that it is never reached by any
markup and does not complicate the experiment.

The learning methods compared are Sarsa, Q-learning, Q($\lambda$), ENAC,
REINFORCE and the variant Roth-Erev technique.  The parameters available to
adjustment are\ldots.

As in \citeA{krause:nash06}, the point of Nash equilibrium is established by
computing each agent's reward for all possible combinations of markup.  The
value are given in Table \ref{tbl:nash1}.

\begin{table}
\begin{center}
\begin{tabular}{c.{2.2}|.{2.1}.{3.1}|.{2.1}.{3.1}|.{2.1}.{3.1}|.{2.1}.{3.1}|}
\cline{3-10}
 & &\multicolumn{8}{c|}{$G_1$} \\
\cline{3-10}
 & &\multicolumn{2}{c|}{0.0\%} &\multicolumn{2}{c|}{10.0\%} &\multicolumn{2}{c|}{20.0\%} &\multicolumn{2}{c|}{30.0\%} \\
 & &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$G_2$}} &0.0\% &0.0 &0.0 &51.0 &0.0 &0.0 &0.0 &0.0 &0.0 \\
\multicolumn{1}{|c|}{} &10.0\% &0.0 &49.5 &51.0 &49.5 &0.0 &49.5 &0.0 &49.5 \\
\multicolumn{1}{|c|}{} &20.0\% &0.0 &92.2 &51.0 &99.0 &0.0 &99.0 &0.0 &99.0 \\
\multicolumn{1}{|c|}{} &30.0\% &0.0 &126.8 &54.8 &138.4 &0.0 &148.5 &0.0 &148.5 \\
\hline
\end{tabular}
\caption{Agent rewards for Nash equilibrium analysis under cost
configuration~1}
\label{tbl:nash1}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{c.{2.2}|.{2.1}.{2.1}|.{2.1}.{2.1}|.{2.1}.{2.1}|.{3.1}.{2.1}|}
\cline{3-10}
 & &\multicolumn{8}{c|}{$G_1$} \\
\cline{3-10}
 & &\multicolumn{2}{c|}{0.0\%} &\multicolumn{2}{c|}{10.0\%} &\multicolumn{2}{c|}{20.0\%} &\multicolumn{2}{c|}{30.0\%} \\
 & &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 &r_1 &r_2 \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$G_2$}} &0.0\% &0.0 &0.0 &40.0 &0.0 &80.0 &0.0 &120.0 &0.0 \\
\multicolumn{1}{|c|}{} &10.0\% &0.0 &33.0 &40.0 &33.0 &80.0 &33.0 &120.0 &33.0 \\
\multicolumn{1}{|c|}{} &20.0\% &0.0 &66.0 &40.0 &66.0 &80.0 &66.0 &120.0 &66.0 \\
\multicolumn{1}{|c|}{} &30.0\% &0.0 &99.0 &40.0 &99.0 &80.0 &99.0 &120.0 &99.0 \\
\hline
\end{tabular}
\caption{Agent rewards for Nash equilibrium analysis under cost
configuration~2}
\label{tbl:nash2}
\end{center}
\end{table}

As one would intuitively expect, the optimal policy for each agent is to apply
the maximum markup to each offer as this never results in a generator failing
to be dispatched.

They show that the optimal point is where the agent with the lowest cost
generator applies the maximum markup and the agent with the medimum priced
generator offers a price just below the marginal cost of the passive
agent's generator.

\section{Simulation Results}
Each action taken by an agent and the consequent reward is recorded for each
simulation.  Values are averaged over the 10 simulation runs and standard
deviations calculated using the conventional formula
\begin{equation}
SD = \sum_{i=0}^{N}\frac{(x_i - m)^2}{N-1}
\end{equation}
where $x_i$ is the action or reward value in simulation $i$ of $N$ simulation
runs and $m$ is the mean of the values.

Figure \ref{fig:5_1_action_a1} plots the average markup on marginal cost and
the standard deviation over the 10 simulation runs for Agent 1 under the first
price configuration using the variant Roth-Erev, Q-learning, REINFORCE and ENAC
learning methods.  The second $y$-axis in each plot concerns the value of
the exploration parameter for each method.  Figure \ref{fig:5_1_action_a2}
plots the same quantities for Agent 2.  Generator prices and the market are
configured such that an agent's reward is directly proportional to its action,
so plots of reward are not given.

\begin{figure}
  \centering
  \includegraphics{figures/fig5_1_action_a1}
  \caption{Average markup for agent 1 and standard deviation over 10 runs.}
  \label{fig:5_1_action_a1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{figures/fig5_1_action_a2}
  \caption{Average markup for agent 2 and standard deviation over 10 runs.}
  \label{fig:5_1_action_a2}
\end{figure}

Figures \ref{fig:5_2_action_a1} and \ref{fig:5_2_action_a2} plot the average
markup for Agent 1 and Agent 2, respectively, under the second price
configuration and again for the variant Roth-Erev, Q-learning, REINFORCE and
ENAC learning methods.  Figures \ref{fig:5_2_reward_a1} and
\ref{fig:5_2_reward_a2} plot the associated average \textit{rewards} for
Agent 1 and Agent 2.  Again the standard deviation and exploration parameter
values are plotted. The plots are vertically aligned and have equal $x$-axis
limits to aid algorithm comparison.
\begin{figure}
  \centering
  \includegraphics{figures/fig5_2_action_a1}
  \caption{Average markup for agent 1 and standard deviation.}
  \label{fig:5_2_action_a1}
\end{figure}
\begin{figure}
  \centering
  \includegraphics{figures/fig5_2_action_a2}
  \caption{Average markup for agent 2 and standard deviation.}
  \label{fig:5_2_action_a2}
\end{figure}
\begin{figure}
  \centering
  \includegraphics{figures/fig5_2_reward_a1}
  \caption{Average reward for agent 1 and standard deviation.}
  \label{fig:5_2_reward_a1}
\end{figure}
\begin{figure}
  \centering
  \includegraphics{figures/fig5_2_reward_a2}
  \caption{Average reward for agent 2 and standard deviation.}
  \label{fig:5_2_reward_a2}
\end{figure}

\section{Discussion and Critical Analysis}
Under the first generator price configuration the agents face a very simple
control task and receive a clear reward signal that is directly proportional to
their markup action.  The results show that all of the methods consistently
converge to the Nash equilibrium point.  A multitude of parameter and nerual
network structures could be investigated and would produce a sea of similar
plots.  The author's experience is that the speed of convergence is largely
determined by the rate at which the exploration parameter value is reduced.
The policy gradient methods are sensitive to high learning rate parameter
values, but make only very small policy adjustments if this parameter is set
too low. All of the plots for REINFORCE and ENAC show that the methods only converge to a
stable policy if the exploration parameter $\sigma$ is manually reduced to
below approximately minus two.

The second pricing configuration provides a more challenging control problem in
which there is some interdependence between the agent's rewards and where
Agent~1 must learn to undercut the passive agent.  The results show that the
variant Roth-Erev and Q-learning methods both consistantly learn their optimal
policy and converge to the Nash equilibrium.  It should be noted that Agent~1
can markup its marginal price by slightly more than 10\% and still undercut
the passive agent, but these methods are restricted to discrete actions.

When using REINFORCE or ENAC, Agent~2 tends also to learn to maximise its
markup, but less consistantly.  Agent~1 typically learns to undercut the
passive agent, but does not converge to a consistant value.  The problem is
similar to the cliff-edge walking problems often used as benchmarks in
reinforcement learning research and may be difficult to approximate a policy
for using a small number of $\tanh$ functions. It may be possible to improve
the performance of these agents through more educated policy function
approximator design, but their methods are not really intended for operation in
such simple environments.

This experiment confirms the convergence to a Nash equilibrium of the
Q-learning methods that is published in \citeA{krause:nash06} and, to a degree,
extends the conclusion to policy gradient methods.  The results show that
while these methods do converge to the same or similar policies as the
Q-learning and Roth-Erev methods, they do not exhibit the same level of
consistancy.

Value function based methods or the Roth-Erev method may be the
most suitable choice of algorithm in the simple electricity market simulations
typlically found in the literature.  The simulations conducted here do not
exploit any of the abilities of policy gradient methods to utilise
multi-dimensional continuous state information and it would be desirable to
investigate their behaviour in a more complex environment.

%\section{Summary}


% \chapter{Competitive Power Trade}
% Having compared the learning methods in a one-player context, this section
% describes the method used to pit them against one and other and compare their
% performance.
%
% \section{Aims \& Objectives}
% Competition is fundamental to markets and this experiment aims to compare
% learning methods in a complex dynamic market environment with multiple
% competing participants.  The objective is to compare:
% \begin{itemize}
%   \item Performance, in terms of profitability, over a finite number of
%   periods,
%   \item Profitability when trading both active and reactive power.
%   \item Consistency of profit making and,
%   \item Sensitivity to algorithm parameter changes.
% \end{itemize}
%
% \section{Method of Simulation}
% Figure X illustrates the structure of the six bus power system model, from
% \cite{wood:pgoc}, with three generators and fixed demand at three of the buses
% used to provide a dynamic environment with typical system values.  Bus,
% branch and generator attribute values are stated in Tables X, Y, Z,
% respectively.  Three learning methods are compared in six simulations
% encapsulating all method--generator combinations.
%
% A price cap $c_{cap}$ of twice the marginal cost of the most expensive generator
% at full capacity is set by the market.  The simulations are repeated for with agents
% actions composing both price and quantity and with just price.  For the
% value-function methods, the state is defined by the market clearing price from
% the previous period, divided equally into $x_s$ discrete states between $0$ and
% $c_{cap}$.  The state vector $s_t$ for the policy gradient methods consists of
% the market clearing price and generator set-point from the previous period.
% \begin{equation}
% s_t =
% \begin{bmatrix}
% c_{mcp}\\
% p_g
% \end{bmatrix}
% \end{equation}
% The script used to conduct the simulation is provided in Listing X.

\chapter{System Constraint Exploitation}
\label{ch:exploitation}
% One of the main features of agents using policy gradient learning methods and
% artifical neural networks for policy function approximation is their ability to
% accept many signals of continuous sensor data.  This section describes an
% experiment in which the power system is severly constrained for certain
% periods, resulting in elevated nodal marginal prices in particular areas.  The
% methods are tested in their ability to exploit these constraints and improve
% their total accumulated reward.
This chapter explores the exploitation of constraints in electric power
system models by agents whose behaviour is determined by reinforcement learning
algorithms.  Value function based and policy gradient methods are compared
using the IEEE Reliability Test System, with dynamic loads and probabilistic
transmission line outages.

\section{Introduction}
Having explored the basic properties of various learning methods in
Chapter~\ref{ch:learningtotrade}, this experiment examines them under a complex dynamic scenario.  Policy gradient methods have been used in robotic control
applications with multi-dimensional, continuous state and action spaces, and
exhibit a degree of robustness to sensor noise.  In this experiment,
these features are explored in the context of learning to trade power.

Control of a portfolio of generators using continous sensor data from
simulations of a standard test power system model with realisitic load dynamics
is examined.  To force the system into a constrained state at certain times, transmission line outages are simualated according to the probabilities given in Table Z.  By
observing the actions taken and the reward received by an agent during these
periods it is examined if these methods can be used to exploit such
occurances.

\section{Aims and Objectives}
This experiment aims to compare the operation of learning methods in dynamic
electric power system environments.  Specifically, the objective are to
determine:
\begin{itemize}
  \item If policy gradient methods can be used to achieve greater profit under
  dynamic loading conditions.
  \item If policy gradient methods can exploit outages and the resulting system
  constraints to further increased profit.
  \item The value of using AC optimal power flow formulations in agent base
  electricity market simulation.
\end{itemize}
Meeting these objectives aims to demonstrate the value of policy gradient
methods in electricity market participant modelling.

\section{Method of Simulation}
The learning methods are compared by repeating the same simulation with
different types of algorithm in-place.  Some simplification of the state
and action domains for the value function based methods is required, but the
portfolios of generation and load profiles are constant.

The IEEE Reliability Test System \cite{ieee79rts} provides the power system
model, load profiles and outage probabilities used in each simulation.  The
model has 24 bus locations, connected by 32 transmission lines, 4 transformers
and 2 underground cables.  The transformers tie together two system areas at
230kV and 138kV.  The model has 32 generators of 9 different types (See Table
X) with a total capacity of 3.45GW and load at 17 locations, totalling 2.85GW.
Generator costs are quadratic functions of output, defined by the parameters in
Table Y.  Figure X plots the cost functions for each type of generator over
their production range and illustrates their categorisation by fuel type.  Data
for the model is provided in Appendix \ref{adx:ieee_rts} and the connectivity
of branches and the location of generators and loads is illustrated in
Figure~Y.

\begin{figure}
  \centering
  \includegraphics{figures/ieee_rts_profiles}
  \caption{Hourly, daily and weekly load profile plots from the IEEE
  Reliability Test System}
  \label{fig:iee_rts_profiles}
\end{figure}

The generating stock is divided into 5 portfolios, as listed in Table Z, that
are each endowed to a learning agent.  The synchronous generator is associated
with a passive agent that always offers at marginal cost i.e.~\$/MWh~0.
Markups of offer price are restricted a maximum of 30\% and discrete markup
steps of 10\% are defined for value function based methods.

%\input{tikz/ieee79rts}

\section{Simulation Results}
\section{Discussion and Critical Analysis}
\label{sec:discuss}
\section{Summary}
