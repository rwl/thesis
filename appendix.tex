% \chapter{Background Theory}
% \label{ch:background}
% This chapter provides an introduction to optimal power flow and reinforcement
% learning.  The methods described are used in Chapter \ref{ch:method} below to
% model electricity markets and market participant behaviour.  Optimal power
% flow is one of the most widely studied subjects in electric power Engineering
% and a comprehensive literature review is available in
% \cite{momoh:part1,momoh:part2}.  For detailed definitions and analysis
% reinforcement learning methods the interested reader is referred to
% \cite{suttonbarto:1998,bertsekas:96}.

\chapter{Optimal Power Flow}
\label{sec:opf}
Nationalised electricity supply industries are typically planned operated and
controlled centrally.  A system operator determines which generators must
operate and the required output of the operating units such that demand and
reserve requirements are met and the overall cost of production is minimised.
In electric power Engineering, this is termed the \textit{unit commitment} and
\textit{economic dispatch} problem.

In 1962 a unit commitment formulation was published with network constraints
included \cite{carpentier:opf}.  \textit{Optimal power flow} is this
integration of economic and power flow aspects of power systems into a
mathematical optimisation problem.  Its ability to solve centralised power
system operation problems and determine prices in power pool markets has led
to optimal power flow being one of the most widely studied subjects in the
power systems community.

\section{Power Flow Formulation}
\label{sec:pf_form}
Optimal power flow derives its name from the \textit{power flow}, or load flow,
steady-state power system analysis technique.  Given sets of generator data,
load data and a nodal admittance matrix $Y_{bus}$, a power flow study
determines the voltage
\begin{equation}
V_i = \vert V_i \vert \angle\delta_i = \vert
V_i\vert(\cos\delta_i + j\sin\delta_i)
\end{equation}
at each node $i$ in the power system \cite{grainger:psa}. $Y_{bus}$ describes
the electrical network and its formulation is dependant upon the transmission
line, transformer and shunt models employed\footnote{The $Y_{bus}$
formulation used in this thesis is defined in Section \ref{sec:acpf}.}.
Importantly, the relationship between nodal voltages and power entering the
network is non-linear.  For a network of $n_b$ nodes, the current injected at
node $i$ is
\begin{equation}
I_i = \sum_{j=1}^{n_b} Y_{ij} V_j
\end{equation}
where $Y_{ij} = \vert Y_{ij}\vert \angle\theta_{ij}$ is the $(i,j)^{th}$ element
if the, $n_b \times n_b$, $Y_{bus}$ matrix.  Hence, the apparent power entering
the network at bus $i$ is
\begin{equation}
S_i = P_i+Q_i = V_iI_i^* = \sum_{n=1}^{n_b} \vert Y_{ij}V_iV_j \vert \angle
(\delta_i - \delta_j - \theta_{ij})
\end{equation}
Converting to polar coordinates and separating the real and imaginary parts,
the active power
\begin{equation}
P_i = \sum_{n=1}^{n_b} \vert Y_{ij}V_iV_j \vert \cos(\delta_i - \delta_j -
\theta_{ij})
\end{equation}
and the reactive power entering the network
\begin{equation}
Q_i = \sum_{n=1}^{n_b} \vert Y_{ij}V_iV_j \vert \sin(\delta_i - \delta_j -
\theta_{ij})
\end{equation}
at bus $i$ are non-linear functions of $V_i$, as indicated by the presence of
the sine and cosine terms.  Kirchoff's Current Law requires that the net
complex power injection (generation - load) at each bus equals the sum of
complex power flows on each branch connected to the bus.  The power balance
equations
\begin{equation}
\label{eq:p_balance}
P_g^i - P_d^i = P^i
\end{equation}
and
\begin{equation}
\label{eq:q_balance}
Q_g^i - Q_d^i = Q^i
\end{equation}
where the subscripts $g$ and $d$ indicate generation and demand
respectively, form a key constraint in the optimal power flow problem.

\section{Optimal Power Flow Formulation}
Optimal power flow is a mathematical optimisation problem in which the complex
power balance equations (\ref{eq:p_balance}) and (\ref{eq:q_balance}) must be
satisfied.  Optimisation problems have the general form
\begin{equation}
\min_x f(x)
\end{equation}
subject to
\begin{eqnarray}
\label{eq:equality}
g(x)& =& 0\\
\label{eq:inequality}
h(x)& \leq& 0
\end{eqnarray}
where $x$ is the optimisation variable, $f$ is the objective function and
equations (\ref{eq:equality}) and (\ref{eq:inequality}) are sets of equality
and inequality constraints on $x$, respectively.  Typical inequality
constraints are bus voltage magnitude contingency state limits, generator
output limits and branch power or current flow limits.  The optimisation
variable $x$ may be made up of generator set-points, bus voltages, transformer
tap settings etc.  If the optimisation variable $x$ is empty then the
formulation reduces to a power flow problem as described in Section
\ref{sec:pf_form}, above.

A common objective of optimal power flow is total system cost minimisation.
For and network of $n_g$ generators the objective function is
\begin{equation}
\min \sum_{k=1}^{n_g} C_k(P_{g,k})
\end{equation}
where $C_k$ is a cost function (typically quadratic) of the set-point $P_{g,k}$
for generator $k$.  Alternative objectives may be to minimise losses, maxmimise
the voltage stability margin or minimise deviation of an optimisation variable
from a particular schedule.

\section{Interior-Point Methods}


\chapter{Reinforcement Learning}
\label{sec:rl}
% The problem of learning how best to interact with an environment so as to
% maximise some long-term reward is a general one.  Reinforcement learning is a
% term from the field of machine learning that is typically applied to
% understanding, automating and solving this problem through adaptive
% computational approaches.  Unlike many machine learning techinques, the
% algorithms are not instructed as to which actions to take, but must learn to
% maximise the long-term reward through trial-and-error.

% Reinforcement learning starts with an interactive, goal-seeking individual
% agent existing in an environment.  The agent requires the ability to;
% \begin{itemize}
%   \item Sense aspects of its environment,
%   \item Perform actions that influence the state of its environment and
%   \item be assigned rewards as a response to their chosen action.
% \end{itemize}
% An agent follows a particular \textit{policy} when mapping the perceived state
% of its environment to an action choice.  Reinforcement learning methods adjust
% the agent's policy.
Reinforcement learning is learning from reward by mapping situations to actions
when interating with an uncertain environment \cite{suttonbarto:1998}.  An
agent learns \textit{what} to do in order to achieve a task through
trial-and-error using a numerical reward or penalty signal without being
instructed \textit{how} to achieve it.  In challenging cases, actions may not
yield immediate reward or may affect the next situation and all subsequent
rewards.  A compromise must be made between exploitation of past experiences
and exploration of the environment through new action choices.  A
reinforcement learning agent must be able to:
\begin{itemize}
  \item sense aspects of its environment,
  \item take actions that influence its environment and,
  \item have an explicit goal or set of goals relating to the state of its
  environment.
\end{itemize}
In the classical model of agent-environment interaction, at each time step $t$
in a sequence of discrete time steps $t = 1,2,3\dotsc$ an agent receives as
input some form of the environment's state $s_t \in \mathscr{S}$, where
$\mathscr{S}$ is the set of possible states.  From a set of actions
$\mathscr{A}(s_t)$ available to the agent in state $s_t$, the agent selects an
action $a_t$ and performs it upon its environment.  The environment enters a
new state $s_{t+1}$ in the next time step and the agent receives a scalar
numerical reward $r_{t+1} \in \mathbb{R}$ in part as a result of its action.
The agent then learns from the state representations $s_t$ and $s_{t+1}$, the
chosen action $a_t$ and the reinforcement signal $r_{t+1}$ before beginning
its next interaction.  Figure X diagrams the agent-environment interaction
event sequence.

\section{Markov Decision Processes}
For a finite number of states $\mathscr{S}$, if all states are Markov, the
agent interacts with a finite Markov decision process (MDP).  Informally,
for a state to be Markov it must retain all relevant information about the
complete sequence of positions leading up to the state, such that all future
states and expected rewards can be predicted as well as would be possible given
a complete history.  A particular MDP is defined for a discrete set of time
steps by a state set $\mathscr{S}$, an action set $\mathscr{A}$, a set of state
transition probabilities $\mathscr{P}$ and a set of expected reward values
$\mathscr{R}$.  Given a state $s$ and an action $a$, the probability of
transitioning to each possible next state $s^\prime$ is
\begin{equation}
\mathscr{P}^a_{ss^\prime} = \Pr \bigl\lbrace s_{t+1} = s^\prime \vert s_t=s,
a_t=a \bigr\rbrace .
\end{equation}
Given the next state $s^\prime$, the expected value of the next reward is
\begin{equation}
\mathscr{R}^a_{ss^\prime} = E \bigl\lbrace r_{t+1} \vert s_t=s, a_t=a,
s_{t+1}=s^\prime \bigr\rbrace .
\end{equation}
In practice not all state signals are Markov, but should provide a good basis
for predicting subsequent states, future rewards and selecting actions.

If the state transition probabilities and expected reward values are not known,
only the states and actions, then samples from the MDP must be taken and a value
function approximated iteratively based on new experiences generated by
performing actions.

\section{Value Function Methods}
\label{sec:valuebased}
% Value-based methods attempt to find the optimal policy by
% approximating a \textit{value-function} which returns the total reward an
% agent can expect to accumulate, given an initial state and following the
% current policy thereafter.  The policy is adjusted using the reinforcing
% signal and information from previous action choices.
Any method that can optimise control of a MDP may be considered a reinforcement
learning method.  All search for an optimal policy $\pi^*$ that maps state
$s \in \mathscr{S}$ and action $a \in \mathscr{A}$ to the probability
$\pi^*(s,a)$ of taking $a$ in $s$ and maxmimises the sum of rewards over the
agents lifetime.

Each state $s$ under policy $\pi$ may be associated with a \textit{value}
$V^\pi(s)$ equal to the expected return from following policy $\pi$ from state
$s$.  Most reinforcement learning methods are based on estimating the
state-value function
\begin{equation}
\label{eq:statevalue}
V^\pi(s) = E \Bigg\lbrace \sum^\infty_{t=0} \gamma^t r_t \Bigg\vert s_0 = s
\Bigg\rbrace
\end{equation}
where $\gamma$ is a discount factor, with $0\leq \gamma \leq 1$.
Performing certain actions may result in no state change, creating a loop and
causing the value of that action to be infinite for certain policies.
The discount factor $\gamma$ prevents values from going unbounded and
represents reduced trust in the reward $r_t$ as discrete time $t$
increases.  Many reinforcement learning methods estimate the action-value
function
\begin{equation}
\label{eq:actionvalue}
Q^\pi(s,a) = E \Bigg\lbrace \sum^\infty_{t=0} \gamma^t r_t \Bigg\vert s_0 = s,
a_0 = a \Bigg\rbrace
\end{equation}
which defines the value of taking action $a$ in state $s$ under fixed policy
$\pi$.

\subsection{Temporal-Difference Learning}
Temporal Difference (TD) learning is a central idea in reinforcement learning.
TD methods do not attempt to estimate the state transition probabilities and
expected rewards of the finite MDP, but estimate the value function directly.
They learn to \textit{predict} the expected value of total reward returned by
the state-value function (\ref{eq:statevalue}).  For an exploratory policy $\pi$
and a non-terminal state $s$, an estimate of $V^\pi(s_t)$ at any given time step
$t$ is updated using the estimate at the next time step $V^\pi(s_{t+1})$ and the
observed reward $r_{t+1}$
\begin{equation}
V^\pi(s_t) \leftarrow V^\pi(s_t) + \alpha \bigl[r_{t+1} + \gamma
V^\pi(s_{t+1}) - V^\pi(s_t) \bigr]
\end{equation}
where $\alpha$ is the learning rate, with $0 \leq \alpha \leq 1$, which controls
how much attention is paid to new data when updating $V^\pi$.  TD learning
evaluates a particular policy and offers strong convergence guarantees, but does
not learn better policies.

\subsection{Sarsa}
\label{sec:sarsa}
% The SARSA algorithm is an on-policy Temporal Difference control method where
% the policy is typically represented by a $M \times N$ table, where $M$ and $N$
% are arbitrary positive numbers equal to the total number of feasible states and
% actions respectively.  The action-value update for agent $j$ is defined by
% \begin{equation}
% Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma Q_j(s_{jt+1},a_{jt+1}) -
% Q_j(s_{jt},a_{jt})]
% \end{equation}
% While the Q-learning method (See Section \ref{sec:qlearning}, below) updates
% action-values using a greedy policy, which is a different policy to that being
% followed, SARSA uses the discounted future reward of the next state-action
% observation following the original policy.
Sarsa (or modified Q-learning) is an on-policy TD control method that
approximates the state-action value function in (\ref{eq:actionvalue}).
Recall that the state-action value function for an agent returns the total
expected reward for following a particular policy for selecting actions as a
function of future states.  The function is updated according to the rule
\begin{equation}
\label{eq:sarsa}
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \bigl[r_{t+1} + \gamma
Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\bigr]
\end{equation}
This update also uses the action from the next time step $a_{t+1}$ and the
requirement to transition through state-action-reward-state-action for each
time step derives the algorithm's name.  Sarsa is referred to as an on-policy
method since it learns the same policy that it follows.

\subsection{Q-Learning}
\label{sec:qlearning}
% This section describes the original off-policy Temporal Difference Q-learning
% method developed by Watkins \cite{watkins:1989}.  The action-value function,
% $Q(s,a)$, returns values from a $M \times N$ matrix where $M$ and $N$ are
% arbitrary positive numbers equal to the total number of feasible states and
% actions, respectively.  Each value represents the \textit{quality} of taking a
% particular action, $a$, in state $s$.  Actions are selected using either the
% $\epsilon$-greedy or softmax (See Section \ref{sec:rotherev}, above) methods.
% The $\epsilon$-greedy method either selects the action (or one of the actions)
% with the highest estimated value or it selects an action at random, uniformly,
% independently of the estimated values with (typically small) probability
% $\epsilon$.
%
% Agent $j$ will observe a reward, $r_{jt}$, and a new state, $s_{jt+1}$,
% after taking action $a_{jt}$ at step $t$ when in state $s_{jt}$.  The
% state-action value, $Q_j(s_{jt},a_{jt})$, is updated according to the
% maximum value of available actions in state $s_{t+1}$ and becomes
% \begin{equation}
% \label{eq:qlearning}
% Q_j(s_{jt},a_{jt}) + \alpha [r_{jt+1} + \gamma\max_{a} Q_j(s_{jt+1},a_{jt}) -
% Q_j(s_{jt},a_{jt})]
% \end{equation}
% where $\alpha$ and $\gamma$ are the learning rate, $0\leq\alpha\leq1$, and
% discount factor, $0\leq\gamma\leq1$, respectively.  The learning rate
% determines the extent to which new rewards will override the effect of older
% rewards.  The discount factor allows the balance between maximising immediate
% rewards and future rewards to be set.
Q-learning is an off-policy TD method that does not estimate the finite
MDP directly, but iteratively approximates a state-action value
function which returns the value of taking action $a$ in state $s$ and
following an \textit{optimal} policy thereafter. The same theorems used in defining the TD error also apply for
state-action values.
\begin{equation}
\label{eq:qlearning}
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \bigl[r_{t+1} + \gamma\max_a
Q(s_{t+1},a)-Q(s_t,a_t) \bigr]
\end{equation}
The method is off-policy since the update function is independant of the policy
being followed and only requires that all state-action pairs be continually
updated.

\subsection{Eligibility Traces}
With the TD methods described above, only the value for the immediately
preceeding state or state-action pair is updated at each time step.  However,
the prediction $V(s_{t+1})$ also provides information concerning earlier
predictions and TD methods can be extended to update a set of values at each step.  An eligibility trace $e(s)$ represents how eligible the state $s$ is to receive
credit or blame for the TD error
\begin{equation}
\delta = r_{t+1} + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)
\end{equation}
When extended with eligibility traces TD methods update values for all states
\begin{equation}
%V(s) \leftarrow V(s) + \gamma \delta e(s)
\Delta V_t(s) = \alpha \delta_t e_t(s)
\end{equation}
For the current state $e(s) \leftarrow e(s) + 1$ and for all states the
$e(s) \leftarrow \gamma \lambda e(s)$ where $\lambda$ is the eligibility trace
attenuated factor from which the extended TD methods TD($\lambda$),
Q($\lambda$) and Sarsa($\lambda$) derive their names. For $\lambda = 0$ only
the preceeding value is updated is updated, as in the unextended definitions,
and for $\lambda = 1$ all preceeding state-values or state-action values are
updated equally.

\subsection{Action Selection}
% TODO: Epsilon-greedy.
Action selection may be acomplished using a form of the \textit{softmax} method
\cite{suttonbarto:1998} using the Gibbs, or Boltzmann, distribution to select
action $k$ for the $(t+1)^{th}$ interaction with probability
\begin{equation}
p_{jk}(t+1) = \frac{e^{q_{jk}(t+1)/\tau}}{\sum_{l=0}^K e^{q_{jl}(t+1)/\tau}}
\end{equation}
where $\tau$ is the \textit{temperature} parameter.  This parameter may be
lowered in value over the course of an experiment since high values give all
actions similar probability and encourage exploration of the action space,
while low values promote exploitation of past experience.

% \subsubsection{Q($\lambda$)}
% \label{sec:qlambda}
% In the Q-learning formulation, described in equation \ref{eq:qlearning}, only
% the quality associated with the previous state, $s_{jt}$, is updated.  However,
% the preceding states can also, in general, be said to be associated with the
% reward $r_{jt+1}$.  Eligibility traces are a mechanism for representing this
% effect and in algorithms such as Q($\lambda$), they are what the $\lambda$
% refers to.  The eligibility trace for a state $e(s)$ represents how eligible
% the state $s$ is to receive credit or blame for the error.  The term ``trace''
% refers to fact that only recently visited states become eligible.  The
% eligibility value for the current state is increased while for all other
% states it is attenuated by a factor $\lambda$.
%
% The off-policy nature of Q-learning requires special care to be taken when
% implementing eligibility traces.  While the algorithm may learn a greedy
% policy, in which the action with the maximum value would always be taken,
% typically a policy with some degree of exploration will be followed when
% choosing actions.  If an exploratory (pseudo-random) step is taken the
% preceding states can no longer be considered eligible for credit or blame.
% Setting $\lambda$ to $0$ for non-greedy actions removes much of the benefit of
% using eligibility traces if exploratory actions are frequent.  A solution to
% this has been developed, but requires a very complex implementation
% \cite{peng:1996}.  A na\"ive approach can be taken, where the effect of
% exploratory actions is ignored, but the results of this are unexplored.

\section{Policy Gradient Methods}
\label{sec:policygradient}
% The value-function methods defined in Section \ref{sec:valuebased} typically
% rely upon discretisation of the sensor and action spaces so the associated
% values may be stored in tables.  The memory requirements for this restrict the
% application of these methods to small environments.  Many environments,
% particularly from real applications, exhibit continuous sensor and/or action
% spaces and require generalisation techniques to be employed to provide a more
% compact policy representation.
%
% Policy-gradient methods use only the reward signal and search directly in the
% space of the policy parameters.  The agent's policy function approximator is
% updated according to the gradient of expected reward with respect to its
% parameters.
Value function based methods have been successfully applied with discrete lookup
table parameterisation to many problems [ref].  However, the number of discrete
states required increases exponentially as the dimensions of the state space
increase and if all possibly relevant situations are to be covered then these
methods become subject to Bellman's Curse of Dimensionality
\cite{bellman:1961}.  Value function based methods can be used in conjunction
with function approximators, artificial neural networks are popular, to work
with continuous state and action space.  However, when used with value
function approximation they have been shown to offer poor convergence and even
divergence characteristics, even in simple systems \cite{peters:enac}.

These convergence problems have motivated research into policy gradient methods
which make small incremental changes to the parameters $\theta$ of a policy
function approximator.  With artificial neural networks the parameters are
the weights of the network connections.  Policy gradient methods update
$\theta$ in the direction of the gradient of some policy performance measure
$Y$ with respect to the parameters
\begin{equation}
\theta_{i+1} = \theta_i + \alpha \frac{\partial Y}{\partial \theta_i}
\end{equation}
where $\alpha$ is a positive definite step size learning rate.

Aswell as working with continuous state and actions space, policy gradient
methods offer strong convergence guarantees, do not require all states to be
continually updated and although uncertainty in state data can degrade policy
performance, the techniques need not be altered.

Policy gradient methods are differentiated largely by the techniques used to
obtain an estimate of the policy gradient $\partial Y / \partial \theta$.
The most successful real-world robotics results have been yielded using
Williams' \textsc{Reinforce} likelihood ratio methods \cite{williams:reinforce}
and natural policy gradient methods such as Natual Actor-Critic
\cite{peters:enac}.

% \subsubsection{REINFORCE}
% \label{sec:reinforce}
% REINFORCE is an associative reinforcement learning method that determines
% a policy by modifying the parameters of a policy function approximator, rather
% than approximating a value function \cite{williams:reinforce}.  Commonly,
% feedforward artificial neural networks are used to represent the policy, where
% the input is a representation of the state and the output is action selection
% probabilities.  In learning, a policy gradient approach is taken where the
% weights of the network are adjusted in the direction of the gradient of
% expected reinforcement.
%
% Defining the network, let $\mathbf{x}^i$ denote the vector of inputs to the
% $i$th unit and $y_i$ denote output of the unit.  In the input layer of the
% network the elements $x_j$ of $\mathbf{x}^i$ are normalised sensor values from
% the environment and in the output layer, or in any hidden layers, they are
% outputs from the $j$ unit in the preceding layer.  Let $\mathbf{w}^i$ denote
% the vector of the weights, $w_{ij}$, on the connections to the $i$th unit.  The
% output of the $i$th unit is dependant on the vector of inputs, $\mathbf{x}^i$,
% and the associated weights, $\mathbf{w}^i$.
%
% For each interaction of the agent with the environment, each parameter $w_{ij}$
% is incremented by
% \begin{equation}
% \label{eq:reinforce}
% \Delta w_{ij} = \alpha_{ij}(r - b_{ij})\frac{\partial\ln\rho_i}{\partial
% w_{ij}}
% \end{equation}
% where $\alpha_{ij}$ is the \textit{learning factor}, $b_{ij}$ is the
% \textit{reinforcement baseline} and $\rho_i$ is the performance of the policy
% (e.g., the average reward per interaction).
%
% \subsubsection{ENAC}
% \label{sec:enac}
% ToDo: Episodic Natural Actor Critic\cite{peters:enac}.


\section{Roth-Erev Method}
\label{sec:rotherev}
The reinforcement learning method formulated by Alvin~E.~Roth and Ido~Erev is
based on empirical results obtained from observing how humans learn decision
making strategies in games against multiple strategic players
\cite{roth:games,roth:aer}.  It learns a stateless policy in which each action
$a$ is associated with a value $q$ for the propensity of its selection.  In
time period $t$, if agent $j$ performs action $a^\prime$ and receives a reward
$r_{ja^\prime}(t)$ then the propensity value for action $a$ at time $t+1$ is
\begin{equation}
q_{ja}(t+1) =
\begin{cases}
(1-\phi)q_{ia}(t) + r_{ja^\prime}(t)(1-\epsilon), & \text{$a = a^\prime$} \\
(1-\phi)q_{ia}(t) + r_{ja^\prime}(t)(\frac{\epsilon}{A-1}), & \text{$a \ne
a^\prime$}
\end{cases}
\end{equation}
where $A$ is the total number of feasible actions, $\phi$ is the
\textit{recency} parameter and $\epsilon$ is the \textit{experimentation} parameter.  The recency (forgetting) parameter
degrades the propensities for all actions and prevents propensity values from
going unbounded.  It is intended to represent the tendency for players to forget
older action choices and to prioritise more recent experience.  The
experimentation parameter prevents the probability of choosing an action from
going to zero and encourages exploration of the action space.

Erev and Roth proposed action selection according to a discrete probability
distribution function, where action $k$ is selected for interaction $t+1$ with
probability
\begin{equation}
\label{eq:re_prob}
p_{jk}(t+1) = \frac{q_{jk}(t+1)}{\sum_{l=0}^K q_{jl}(t+1)}
\end{equation}
Since $\sum_{l=0}^K q_{jl}(t+1)$ increases with $t$, a reward $r_{jk}(t)$ for
performing action $k$ will have a greater effect on the probability
$p_{jk}(t+1)$ during early interactions while $t$ is small.  This is intended
to represent Psychology's Power Law of Practice in which it is qualitatively
stated that, with practice, learning occurs at a decaying exponential rate and
that a learning curve will eventually flatten out.

\subsection{Modified Roth-Erev Method}
\label{sec:variant}
Two shortcomings of the basic Roth-Erev algorithm have been identified and a
modified formulation proposed \cite{nicolaisen:2001}.  The two issues are that
\begin{itemize}
  \item the values by which propensities are updated can be zero or very small
  for certain combinations of the experimentation parameter $\epsilon$ and
  the total number of feasible actions $A$ and
  \item all propensity values are decreased by the same amount when the reward,
  $r_{jk^\prime}(t)$ is zero.
\end{itemize}
Under the variant algorithm, the propensity for agent $j$ to select action $a$
for interaction $t+1$ is:
\begin{equation}
q_{ja}(t+1) =
\begin{cases}
(1-\phi)q_{ia}(t) + r_{ja^\prime}(t)(1-\epsilon), & \text{$a = a^\prime$} \\
(1-\phi)q_{ia}(t) + q_{ja}(t)(\frac{\epsilon}{A-1}), & \text{$a \ne
a^\prime$}
\end{cases}
\end{equation}
As with the original Roth-Erev algorithm, the propensity for selection of the
action that the reward is associated with is adjusted by the experimentation
parameter.  All other action propensities are adjusted by a small proportion of
their current value.


\chapter{MATPOWER OPF Formulation}
\label{sec:power_system_model}
Power systems are modelled as three-phase AC circuits operating in the
steady-state, under balanced conditions that can be represented by an
equivalent single phase nodal graph of busbars connected by branches
\cite{grainger:psa}.

\section{Branches}
Following \cite[p.11]{pserc:mp_manual}, each branch is modelled as a medium
length transmission line in series with a regulating transformer at the ``from'' end.  A nominal-$\pi$ model with total
series admittance $y_s = 1/(r_s+jx_s)$ and total shunt capacitance $b_c$
represents the transmission line.  The transformer is assumed to be ideal,
phase-shifting and tap-changing, with the ratio between primary winding
voltage $v_{f}$ and secondary winding voltage $N = \tau e^{j\theta_{ph}}$
where $\tau$ is the tap ratio and $\theta_{ph}$ is the phase shift angle.
Figure X diagrams the branch model.  From Kirchhoff's Current Law the current
in the series impedance is
\begin{equation}
\label{eq:iseries}
i_s = \frac{b_c}{2}v_t - i_t
\end{equation}
and from Kirchhoff's Voltage Law the voltage across the secondary winding of
the transformer is
\begin{equation}
\frac{v_{f}}{N} = v_t + \frac{i_s}{y_s}
\end{equation}
Substituting $i_s$ from equation (\ref{eq:iseries}), gives
\begin{equation}
\label{eq:vfrom}
\frac{v_{f}}{N} = v_t - \frac{i_t}{y_s} + v_t\frac{b_c}{2y_s}
\end{equation}
and rearranging in terms if $i_t$, gives
\begin{equation}
\label{eq:ito}
i_t = v_s \left( \frac{-y_s}{\tau e^{\theta_{ph}}} \right) +
v_r \left( y_s + \frac{b_c}{2} \right)
\end{equation}
The current through the secondary winding of the transformer is
\begin{equation}
N^*i_f = i_s + \frac{b_c}{2}\frac{v_{f}}{N}
\end{equation}
Substituting $i_s$ from equation(\ref{eq:iseries}) again, gives
\begin{equation}
N^*i_f = \frac{b_c}{2}v_t - i_t + \frac{b_c}{2}\frac{v_{f}}{N}
\end{equation}
and substituting $\frac{v_{f}}{N}$ from equation (\ref{eq:vfrom}) and
rearranging, gives
\begin{equation}
\label{eq:ifrom}
i_s = v_s \left( \frac{1}{\tau^2} \left(y_s + \frac{b_c}{2}\right) \right) +
v_r \left(\frac{y_s}{\tau e^{-j\theta}}\right)
\end{equation}
Equations (\ref{eq:ito}) and (\ref{eq:ifrom}) are used in section
\ref{sec:acpf} below to define the system admittance matrices that describe
the electrical network.

\section{Generators}
\label{sec:generators}
Each generator $k$ is modelled as an apparent power injection $s^k_g = p^k_g +
jq^k_g$ at a bus $i$, where $p^k_g$ is the active power injection,
$q^k_g$ is the reactive power injection and each are expressed in per-unit to
the system base MVA.  Upper and lower limits on $p^k_g$ are specified by
$p^k_{max}$ and $p^k_{min}$, respectively, where $p^k_{max} > p^k_{min} \geq
0$.  Similarly, upper and lower limits on $q^k_g$ are specified by $q_{max}^k$
and $q_{min}^k$, respectively, where $q^k_{max} > q^k_{min}$.

\section{Buses and Loads}
At each bus $i$, constant active power demand is specified by $p^i_d$ and
reactive power demand by $q^i_d$.  Upper and lower limits on the voltage
magnitude at the bus are defined by $v_m^{i,max}$ and $v_m^{i,min}$,
respectively.  One generator bus $i \in \mathcal{I}_{ref}$ in the circuit is
designated the \textit{reference} bus and has voltage angle $\theta^{ref}_k$.
Dispatchable loads are modelled as generators with negative $p^i_g$ and
$p^i_{min} < p^i_{max} = 0$. %TODO: Constant power factor.

\section{AC Power Flow Equations}
\label{sec:acpf}
Following \cite[p.13]{pserc:mp_manual}, for a network of $n_b$ buses, $n_l$
branches and $n_g$ generators, let $Cg$ be the $n_b \times n_g$ bus-generator connection matrix such that the $(i,j)^{th}$
element of $C_{g}$ is $1$ if generator $j$ is connected to bus $i$.  The
$n_b \times 1$ vector of complex power injections from generators at all buses
is
\begin{equation}
S_{g,bus} = C_g \cdot S_g
\end{equation}
where $S_g = P_g + jQ_g$ is the $n_g \times 1$ vector with the $i^{th}$ element
is equal to $s^i_g$.

Combining equations (\ref{eq:ito}) and (\ref{eq:ifrom}), the \textit{from}
and \textit{to} end complex current injections for branch $l$ are
\begin{equation}
\label{eq:ybranch}
\begin{bmatrix}
i_f^l\\
i_t^l
\end{bmatrix}
=
\begin{bmatrix}
y_{ff}^l& y_{ft}^l\\
y_{tf}^l& y_{tt}^l
\end{bmatrix}
\begin{bmatrix}
v_f^l\\
v_t^l
\end{bmatrix}
\end{equation}
where
\begin{eqnarray}
\label{eq:yff}
y_{ff}^l& =& \frac{1}{\tau^2} \left(y_s + \frac{b_c}{2}\right)\\
\label{eq:yft}
y_{ft}^l& =& \frac{y_s}{\tau e^{-j\theta_{ph}}}\\
\label{eq:ytf}
y_{tf}^l& =& \frac{-y_s}{\tau e^{j\theta_{ph}}}\\
\label{eq:ytt}
y_{tt}^l& =& y_s + \frac{b_c}{2}
\end{eqnarray}
Let $Y_{ff}$, $Y_{ft}$, $Y_{tf}$ and $Y_{tt}$ be $n_l \times 1$ vectors where
the $l^{th}$ element of each corresponds to $y_{ff}^l$, $y_{ft}^l$,
$y_{tf}^l$ and $y_{tt}^l$, respectively.  Furthermore, let $C_f$ and $C_t$ be the
$n_l \times n_b$ branch-bus connection matrices, where $C_{f_{i,j}} = 1$ and
$C_{t_{i,k}} = 1$ if branch $i$ connects from bus $j$ to bus $k$.  The $n_l
\times n_b$ branch admittance matrices are
\begin{eqnarray}
Y_f& =& \diag(Y_{ff})C_f + \diag(Y_{ft})C_t\\
Y_t& =& \diag(Y_{tf})C_f + \diag(Y_{tt})C_t
\end{eqnarray}
and relate the complex bus voltages $V$ to the branch ``from'' and
``to'' end current vectors
\begin{eqnarray}
I_{f}& =& Y_{f}V\\
I_{t}& =& Y_{t}V
\end{eqnarray}
The $n_b \times n_b$ bus admittance matrix
\begin{eqnarray}
Y_{bus}& =& C_f^\mathsf{T} Y_f + C_t^\mathsf{T} Y_t
\end{eqnarray}
relates the complex bus voltages to the nodal current injections
\begin{eqnarray}
I_{bus}& =& Y_{bus}V
\end{eqnarray}
The complex bus power injections are expressed as a non-linear function of $V$
\begin{eqnarray}
S_{bus}(V)& =& \diag(V)I_{bus}^* \nonumber \\
\label{eq:sbus}
&= & \diag(V)Y_{bus}^*V^*
\end{eqnarray}
As are the complex power injections at the ``from'' and ``to'' ends of all
branches
\begin{eqnarray}
S_{f}(V)& =& \diag(C_fV)I_f^* \nonumber \\
\label{eq:sf_loss}
& =& \diag(C_fV)Y_f^*V^*\\
S_{t}(V)& =& \diag(C_tV)I_t^* \nonumber \\
\label{eq:st_loss}
& =& \diag(C_tV)Y_t^*V^*
\end{eqnarray}
The net complex power injection (generation - load) at each bus must equal the
sum of complex power flows on each branch connected to the bus.  Hence the AC
power balance equations are
\begin{equation}
\label{eq:mismatch}
S_{bus}(V) + S_d - S_g = 0
\end{equation}

\section{DC Power Flow Equations}
Following \cite[p.14]{pserc:mp_manual}, the same power system model is used in
the formulation of the linearised DC power flow equations, but the following additional assumptions are made:
\begin{itemize}
  \item The resistance $r_s$ and shunt capacitance $b_c$ of all branches can be
  considered negligible.
  \begin{equation}
  \label{eq:lossless}
  y_s \approx \frac{1}{jx_s}, \quad b_c \approx 0
  \end{equation}
  \item Bus voltage magnitudes $v_{m,i}$ are all approximately 1 per-unit.
  \begin{equation}
  \label{eq:oneperunit}
  v_i \approx 1e^{j\theta_i}
  \end{equation}
  \item The voltage angle difference between bus $i$ and bus $j$ is small enough
  that
  \begin{equation}
  \label{eq:busangdiff}
  \sin\theta_{ij} \approx \theta_{ij}
  \end{equation}
\end{itemize}
Applying the assumption that branches are lossless from equation
(\ref{eq:lossless}), the quadrants of the branch admittance matrix in equations
(\ref{eq:yff}), (\ref{eq:yft}), (\ref{eq:ytf}) and (\ref{eq:ytt}), approximate
to
\begin{eqnarray}
y_{ff}^l& =& \frac{1}{jx_s \tau^2}\\
y_{ft}^l& =& \frac{-1}{jx_s \tau e^{-j\theta_{ph}}}\\
y_{tf}^l& =& \frac{-1}{jx_s \tau e^{j\theta_{ph}}}\\
y_{tt}^l& =& \frac{1}{jx_s}
\end{eqnarray}
respectively.  Applying the uniform bus voltage magnitude assumption from
equation (\ref{eq:oneperunit}) to equation (\ref{eq:ybranch}), the branch
``from'' end current approximates to
\begin{eqnarray}
i_f& \approx& \frac{e^{j\theta_f}}{jx_s\tau^2} -
\frac{e^{j\theta_t}}{jx_s \tau e^{-j\theta_{ph}}}\\
& =& \frac{1}{jx_s\tau} ( \frac{1}{\tau}e^{j\theta_f} -
e^{j(\theta_t + \theta_{ph})} )
\end{eqnarray}
% ToDo: Branch to end current derivation.
and the branch ``from'' end complex power flow $s_f = v_f \cdot i_f^*$
approximates to
\begin{eqnarray}
s_f& \approx& e^{j\theta_f} \cdot \frac{j}{x_s\tau}
(\frac{1}{\tau}e^{-j\theta_f} - e^{j(\theta_t + \theta_{ph})})\\
& =& \frac{1}{x_s\tau} \left[ \sin(\theta_f-\theta_t-\theta_{ph}) +
j\left( \frac{1}{\tau} - \cos(\theta_f-\theta_t-\theta_{ph}) \right) \right]
\end{eqnarray}
Applying the voltage angle difference assumption from equation
(\ref{eq:busangdiff}) yields the approximation
\begin{equation}
p_f \approx \frac{1}{x_s\tau}(\theta_f-\theta_t-\theta_{ph})
\end{equation}
Let $B_{ff}$ and $P_{f,ph}$ be the $n_l \times 1$ vectors where
$B_{ff_i} = 1 / (x_s^i\tau^i)$ and $P_{f,ph_i} =
-\theta_{ph}^i / (x_s^i\tau^i)$.  Then if the system $B$ matrices are
\begin{eqnarray}
B_f& =& \diag(B_{ff})(C_f-C_t)\\
B_{bus}& = &(C_f-C_t)^\mathsf{T}B_f
\end{eqnarray}
then the real power bus injections are
\begin{equation}
\label{eq:bbus}
P_{bus}(\Theta) = B_{bus}\Theta + P_{bus,ph}
\end{equation}
where $\Theta$ is the $n_b \times 1$ vector of bus voltage angles and
\begin{equation}
P_{bus,ph} = (C_f-C_t)^\mathsf{T} + P_{f,ph}
\end{equation}
The active power flows at the branch ``from'' ends are
\begin{equation}
\label{eq:pf_loss}
P_f(\Theta) = B_f\Theta + P_{f,ph}
\end{equation}
and $P_t = -P_f$ since all branches are assumed lossless.

\section{AC OPF Formulation}
Following \cite[p.26]{pserc:mp_manual}, generator active and, optionally,
reactive power output costs are defined by a convex $n$-segment piecewise
linear cost function
\begin{equation}
c^{(i)}(x) = m_ip + c_i
\end{equation}
for $p_i \leq p \leq p_{i+1}$ with $i = 1,2,\dotsc n$ where $m_{i+1} \geq m_i$
and $p_{i+1} > p_i$ as diagramed in Figure X.  Since these costs are
non-differentiable the constrained cost variable approach from
\cite{zimmerman:ccv} is used to make the optimisation problem smooth.  For
each generator $i$ a helper cost variable $y_i$ added to the objective
function.  The inequality constraints
\begin{equation}
y_i \geq m_{i,j}(p-p_j) + c_j, \quad j = 1\dotsc n
\end{equation}
require that $y_i$ lies on the epigraph\footnote{Informally, the epigraph of a
function is a set of points lying on or above its graph.} of $c^{(i)}(x)$. The objective of
the optimal power flow problem is to minimise the sum of the cost variables
for all generators.
\begin{equation}
\min_{\theta, V_m, P_g, Q_g, y} \sum_{i=1}^{n_g}y_i
\end{equation}
Equation (\ref{eq:mismatch}) forms an equality constraint which enforce the
balance between the net complex power injection and injections into the
network.  Branch complex power flow limits $S_{max}$ are enforced by the
inequality constraints
\begin{eqnarray}
\abs{S_f(V)} - S_{max}& \leq &0\\
\abs{S_f(V)} - S_{max}& \leq &0
\end{eqnarray}
and the reference bus voltage angle $\theta_i$ is fixed with the equality
constraint
\begin{equation}
\label{eq:refbusang}
\theta_i^{ref} \leq \theta_i \leq \theta_i^{ref}, \quad i \in \mathcal{I}_{ref}
\end{equation}
Upper and lower limits on the optimisation variables $V_m$, $P_g$ and $Q_g$ are
enforced by the inequality constraints
\begin{eqnarray}
v_m^{i,min} \leq v_m^i \leq v_m^{i,max},& \quad i= 1 \dotsc n_b&\\
\label{eq:pglim}
p_g^{i,min} \leq p_g^i \leq p_q^{i,max},& \quad i= 1 \dotsc n_g&\\
q_g^{i,min} \leq q_g^i \leq q_q^{i,max},& \quad i= 1 \dotsc n_g&
\end{eqnarray}

\section{DC OPF Formulation}
Piecewise linear cost functions are also used to define generator active power
costs in the DC optimal power flow formulation.  Since the power flow equations
are linearised, following the assumptions in equations (\ref{eq:lossless}),
(\ref{eq:oneperunit}) and (\ref{eq:busangdiff}), the optimal power flow
problem simplifies to a linear program.  The voltage magnitude variables $V_m$
and generator reactive power set-point variable $Q_g$ are eliminated following
the assumption in equation (\ref{eq:busangdiff}) since branch reactive power
flows depend on bus voltage angle differences.  The objective function reduces to
\begin{equation}
\min_{\theta, P_g, y} \sum_{i=1}^{n_g}y_i
\end{equation}
Combining the nodal real power injections, expressed as a function of $\Theta$,
from equation (\ref{eq:bbus}), with active power generation $P_g$ and active
demand $P_d$, the power balance constraint is
\begin{equation}
B_{bus}\Theta + P_{bus,ph} + P_d - C_gP_g = 0
\end{equation}
Limits on branch active power flows $B_f\Theta$ and $B_t\Theta$ are enforced by
the inequality constraints
\begin{eqnarray}
B_f\Theta + P_{f,ph} - F_{max}& \leq& 0\\
-B_f\Theta + P_{f,ph} - F_{max}& \leq& 0
\end{eqnarray}
The reference bus voltage angle equality constraint from
equation (\ref{eq:refbusang}) and the $p_g$ limit constraint from
(\ref{eq:pglim}) are also applied.

\section{Optimal Power Flow Solution}
\label{sec:opfsol}
% Generator dispatch points are used with the associated cost functions to
% compute the objective function value -- the total system cost.  The power
% balance Lagrangian multipliers are the shadow prices or system nodal prices and
% equal the cost to the system of supplying one more unit of load at that bus.

\section{Unit De-commitment}
The optimal power flow formulation defined in Section \ref{sec:opf} above
requires generators are dispatched within their upper and lower power limits.
Expensive generators can not be completely shutdown, even if doing so would
result in a lower total system cost.  Algorithm \ref{alg:ud} defines the unit
de-commitment algorithm from \cite[p.20]{pserc:mp_manual} which allows a least
cost commitment and dispatch to be determined using the optimal power flow
formulation. The algorithm finds the least cost dispatch by solving repeated
optimal power flow problems with different combinations of generating units
that are at their minimum active power limit deactivated.  The lowest cost
solution is returned when no further improvement can be made and no candidate
generators remain.
\begin{algorithm}[H]
\caption{Unit de-commitment}
\label{alg:ud}
\begin{algorithmic}[1]
\STATE $\text{initialise}~N \leftarrow 0$
\STATE $\text{solve initial OPF}$
\STATE $L_{tot} \leftarrow \text{total load capacity}$
\WHILE{$\text{total min gen.\ capacity} > L_{tot}$}
	\STATE $N \leftarrow N + 1$
\ENDWHILE

\REPEAT
	\FOR{c in candidates}
		\STATE $\text{solve OPF}$
	\ENDFOR
\UNTIL{$\text{done} = \text{True}$}
\end{algorithmic}
\end{algorithm}
