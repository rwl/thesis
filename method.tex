\chapter{Modelling Power Trade}
\label{ch:method}
This chapter defines the model used in chapters \ref{ch:nashanalysis} and
\ref{ch:exploitation} to simulate electric power trade and to compare learning
algorithms. The first section describes how optimal power flow solutions are
used to clear offers submitted to a simulated power exchange auction.
The second section defines how market participants are modelled as agents that
use the reinforcement learning algorithms to adjust their bidding behaviour. It
explains the modular structure of a multi-agent system that coordinates
interactions between the auction model and market participants.

% Societies reliance on secure energy supplies and the high volumes of
% electricity typically consumed render it impractical to experiment with
% radically new approaches to energy trade on real systems.  This section
% explains the approach taken modelling real systems in software such that they
% may be simulated computationally.  The method by which the physical power
% systems, that deliver electricity to consumers, were modeled is given, as well
% as for the mechanisms that facilitate trade and participants that utilise
% these mechanisms.
%
% \subsection{Energy market model}
% Mechanisms for facilitating competitive trade between electricity producers and
% consumers differ greatly in the specifics of their implementations in coutries
% throughout the world.  However, fundamentally they either provide a
% centralised pool through which all electricity is bought and sold or they
% permit producers and suppliers to trade directly.
%
% The UK transmission network is frequently congested[].  The thermal limits of
% transmission lines between particular areas are often reached.  The balancing
% mechanism is the financial instrument used by the system operator to resolve
% constraint issues and energy imbalances.  Should the market not be suitably
% effective in this function the system operator may choose to contract outwith
% the balancing mechanism.  By way of incentive to match demand and avoid
% congestion, imbalance charges are imposed on responsible participants.  There
% is some evidence to suggest that centralised resolution by a system operator
% and socialisation of the incurred costs leads to inefficient despatch of
% generators[Neuhoff].
%
% There are a number of alternative approaches to congestion
% resolution.
% %\cite{neuhoff:power}
%
% \subsection{Transmission capacity rights}
% One approach is to issue contracts for transmission capacity rights or
% equivalent financial rights.  The maximum available transmission capacity
% being auctioned for certain periods of time and firm contracts made entitling
% owners to full compensation upon curtailment or
% withdrawl.
% %\cite{efet:principles}.
%
% The states of Pensylvania, New Jersey and Maryland (PJM) operate a
% non-compulsory power pool with nodal market-clearing prices based on
% competitive bids.  This is complemented by daily and monthly capacity markets
% plus the monthly auction of Financial Transmission Rights to provide a hedging
% mechanism against future congestion charges.
%
% \subsection{Transmission charging}
% Impose delivery charges which increase as network constraints are approached.
%
%
% \subsection{Extended bids/offers}
% Request extended bids and offers which include costs associated with the
% adjustment of participant's desired position.
%
% \subsection{Software agents}
% Participants are modeled in software also.  The nature of a highly distributed
% power system dictates that a very large number of entities may be interacting
% in the marketplace.  Economic studies regularly integrate participant logic
% into the same optimisation problem as the market.  However, this does not
% scale to large numbers of individual participants.  Separating participant
% logic into individual software agents allows their action selection procedures
% to be processed in simultaneously.  The definition of an agent in this context
% emerges from the machine learning technique employed to implement the
% competitive decision making process.

\section{Electricity Market Model}
A power exchange auction market, based on SmartMarket by
\citeA[p.92]{pserc:mp_manual}, is used in this thesis to provide a trading
environment for comparing reinforcement learning algorithms.  In each trading
period the auction accepts offers to sell blocks of power from participating
agents\footnote{A double-sided auction, in which bids to buy blocks of power may
be submitted by agents associated with dispatchable loads, has also been
implemented, but this feature is not used.}.
% \subsection{Clearing Process} To simulate electric power trade a model is used
% in which agents representing market participants do not provide cost functions
% for the generators in their portfolio, but submit offers to sell and/or bids
% to buy blocks of active or reactive power.  The offers/bids are submitted to a
% power exchange auction market model based on SmartMarket from
% \citeA[p.92]{pserc:mp_manual}.
A clearing process begins by withholding offers above the price cap, along with
those specifying non-positive quantities.  Valid offers for each generator are
sorted into non-decreasing order with respect to price and converted into
corresponding generator capacities and piecewise linear cost functions (See
Section \ref{sec:pw_linear} below).  The newly configured units form an optimal
power flow problem, the solution to which provides generator set-points and
nodal marginal prices that are used to determine the proportion of each offer
block that is cleared and the associated clearing price.  The cleared offers
determine each agent's revenue and hence the profit that is used as a reward
signal.

% Pricing may be uniform,
% where each offer/bid is cleared at the price of the marginal unit, or
% discriminatory, where the offer/bid is cleared at the price at which it
% offered/bid (pay-as-bid).

A nodal marginal pricing scheme is used in which the price of each offer is
cleared at the value of the Lagrangian multiplier on the power balance
constraint for the bus at which the offer's generator is connected. An
alternative a discriminatory pricing scheme may be used in which offers are
cleared at the price at which they were submitted (pay-as-bid).  The
advanced auction types from \textsc{Matpower} that scale nodal marginal prices
are not used.

\subsection{Optimal Power Flow}
\label{sec:pw_linear}
Bespoke implementations of the optimal power flow formulations from \matpower
are used in the auction clearing process.  Both the DC and AC formulations are
used in this thesis.

The trade-offs between DC and AC formulations have been examined by
\citeA{overbye:acdc}.  DC models were found suitable for most nodal marginal
price calculations and are considerably less computationally expensive.  The
AC optimal power flow formulation is used in this thesis to examine the
exploitation of voltage constraints, which are not part of the DC formulation.

% A class diagram in the Unified Modelling Language (UML) for the
% object-orientated power system model that is used to compute optimal power
% flow solutions is shown in Figure \ref{fig:cls_pylon}.  Each branch is
% associated with two buses and forms an edge in the nodal power system
% representation.
%
% \ifthenelse{\boolean{includefigures}}{\input{tikz/cls_pylon}}{}

As in \textsc{Matpower}, generator active
power, and optionally reactive power, output costs may be defined by convex
$n$-segment piecewise linear cost functions
\begin{equation}
c^{(i)}(p) = m_ip + b_i
\end{equation}
where $p$ is the generator set-point for $p_i \leq p \leq p_{i+1}$ with
$i = 1,2,\dotsc n$, $m_i$ is the variable cost for segment $i$ in
\$/MWh where $m_{i+1} \geq m_i$ and $p_{i+1} > p_i$, and $b_i$ is the
$y$-intercept in \$ for segment $i$.  Offers submitted to the market are
converted into a piecewise linear cost function for the associated generator.
Since these cost functions are non-differentiable, the constrained cost
variable approach from \citeA{zimmerman:ccv} is used to make the optimisation
problem smooth.  For each generator $j$ a helper cost variable $y_j$ is added
to the vector of optimisation variables.  Figure~\ref{fig:ccv_function}
illustrates how the additional inequality constraints
\begin{equation}
y_j \geq m_{j,i}(p-p_i) + c_i, \quad i = 1\dotsc n
\end{equation}
ensure that $y_j$ lies on or above $c^{(i)}(p)$
\cite[Figure5-3]{pserc:mp_manual}.  The objective function for the optimal
power flow formulation used in the auction clearing process is the minimisation
of the sum of cost variables for all generators:
\begin{equation}
\min_{\theta, V_m, P_g, Q_g, y} \sum_{j=1}^{n_g}y_j
\end{equation}

The extended optimal power flow formulations from \textsc{Matpower} with
user-defined cost functions and generator P-Q capability curves are not used.

% \section{DC OPF Formulation}
% Piecewise linear cost functions are also used to define generator active power
% costs in the DC optimal power flow formulation.  Since the power flow equations
% are linearised, following the assumptions in equations (\ref{eq:lossless}),
% (\ref{eq:oneperunit}) and (\ref{eq:busangdiff}), the optimal power flow
% problem simplifies to a linear program.  The voltage magnitude variables $V_m$
% and generator reactive power set-point variable $Q_g$ are eliminated following
% the assumption in equation (\ref{eq:busangdiff}) since branch reactive power
% flows depend on bus voltage angle differences.  The objective function reduces to
% \begin{equation}
% \min_{\theta, P_g, y} \sum_{i=1}^{n_g}y_i
% \end{equation}
% Combining the nodal real power injections, expressed as a function of $\Theta$,
% from equation (\ref{eq:bbus}), with active power generation $P_g$ and active
% demand $P_d$, the power balance constraint is
% \begin{equation}
% B_{bus}\Theta + P_{bus,ph} + P_d - C_gP_g = 0
% \end{equation}
% Limits on branch active power flows $B_f\Theta$ and $B_t\Theta$ are enforced by
% the inequality constraints
% \begin{eqnarray}
% B_f\Theta + P_{f,ph} - F_{max}& \leq& 0\\
% -B_f\Theta + P_{f,ph} - F_{max}& \leq& 0
% \end{eqnarray}
% The reference bus voltage angle equality constraint from
% equation (\ref{eq:refbusang}) and the $p_g$ limit constraint from
% (\ref{eq:pglim}) are also applied.

\subsection{Unit De-commitment}
\label{sec:decommit}
The optimal power flow formulations constrain generator set-points between
upper and lower power limits.  The output of expensive generators can be
reduced to the lower limit, but they can not be completely shutdown.  The
online status of generators could be added to the vector of
optimisation variables, but being Boolean the problems would become
mixed-integer non-linear programs which are typically very difficult to
solve.

To compute a least cost commitment and dispatch the unit de-commitment
algorithm from \citeA[p.57]{pserc:mp_manual} is used.  The algorithm
involves shutting down the most expensive units until the
minimum generation capacity is less than the total load capacity and then
solving repeated optimal power flow problems with candidate generating units,
that are at their minimum active power limit, deactivated.  The lowest cost
solution is returned when no further improvement can be made and no candidate
generators remain.

% \begin{algorithm}%[H]
% \caption{Unit de-commitment}
% \label{alg:ud}
% \begin{algorithmic}[1]
% %\STATE $\text{initialise}~N \leftarrow 0$
% %\STATE $P_{d} \leftarrow \text{total load capacity}$
% %\STATE $P_{g}^{min} \leftarrow \text{total min.\ gen.\ capacity}$
% \WHILE{$\sum P_{g}^{min} > \sum P_{d}$}
% %	\STATE $N \leftarrow N + 1$
% 	\STATE shutdown most expensive unit
% \ENDWHILE
%
% %\STATE $\text{solve initial OPF}$
% \STATE $f \leftarrow \text{initial total system cost}$
%
% \REPEAT
% 	\STATE $c \leftarrow \text{generators at } P_{min}$
% 	\FOR{$g$ in $c$}
% 		\STATE $d \leftarrow \text{true}$
% 		\STATE shutdown $g$
% 		\STATE $f^\prime \leftarrow \text{new total system cost}$
% 		\IF{$f^\prime < f$}
% 			\STATE $f \leftarrow f^\prime$
% 			\STATE $g_{c} \leftarrow g$
% 			\STATE $d \leftarrow \text{false}$
% 		\ENDIF
% 		\STATE startup $g$
% 	\ENDFOR
% 	\STATE shutdown $g_c$
% \UNTIL{$d = \text{true}$}
% \end{algorithmic}
% \end{algorithm}

%\newpage
\section{Multi-Agent System}
\label{sec:mas}
Market participants are modelled with software agents from PyBrain that use
reinforcement learning algorithms to adjust their behaviour \cite{schaul:2010}.
Their interaction with the market is coordinated in multi-agent simulations,
the structure of which is derived from PyBrain's single player design.

% In PyBrain, agents do not interact directly with their environment, but are
% associated with a particular \textit{task}.
This section describes discrete and continuous market environments,
agent tasks and modules that are used for policy function approximation and
storing state-action values or action propensities.  The process by which each
agent's policy is updated by a learning algorithm is explained and the sequence of interactions
between multiple agents and the market is described and illustrated.

\subsection{Market Environment}
Each agent has a portfolio of $n_g$ generators associated their environment.
Figure \ref{fig:cls_pyreto} illustrates the association and how the environment
references an instance of the auction market for offer submission.  Each
environment is responsible for \begin{inparaenum}[(i)] \item returning a
vector representation of its current state and \item accepting an action
vector which transforms the environment into a new state. \end{inparaenum}  To
facilitate testing of value function based and policy gradient learning
methods, both discrete and continuous representations of an electric power
trading environment are defined.

\ifthenelse{\boolean{includefigures}}{\input{tikz/cls_pyreto}}{}

\subsubsection{Discrete Market Environment}
For agents operating learning methods that make use of look-up tables an
environment with $n_s$ discrete states and $n_a$ discrete action possibilities
is defined. The environment produces a state $s$, where $s \in \mathbb{Z}^+$
and $0\leq s < n_s$, at each simulation step and accepts an action $a$,
where $a \in \mathbb{Z}^+$ and $0\leq a < n_a$.

To keep the size of the state space reasonable, the state is derived only
from the total system demand $d=\sum P_d$.  Each simulation episode of $n_t$
steps has a demand profile vector $u$ of length $n_t$, where $0 \leq u_i \leq 1$.
The load at each bus $P_{dt} = u_tP_{d0}$ in simulation period $t$, where
$P_{d0}$ is the initial demand vector.  The state size $d_s =
d(\max u - \min u)/n_s$ and the state space vector is $\mathscr{S}=d_si$ for
$i=1\dotsc n_s$.  At simulation step $t$, the state returned by the
environment $s_t = i$ if $\mathscr{S}_i \leq P_{dt} \leq \mathscr{S}_{i+1}$ for
$i = 0\dotsc n_s$.  Informally, the state space is $n_s$ states
between the minimum and maximum demand and the current state for the
environment is the index of the state to which the current demand relates.

The action space for a discrete environment is defined by a vector $m$, where
$0 \leq m_i \leq 100$, of percentage markups on marginal cost with length
$n_m$, a vector $w$, where $0 \leq w_i \leq 100$, of percentage capacity
withholds with length $n_w$ and the number of offers $n_o$, where $n_o \in
\mathbb{Z}^+$, to be submitted for each generator associated with the
environment.

A $n_a \times 2n_gn_o$ matrix that contains all permutations of markup and
withhold for each offer that is to be submitted for each generator is computed.
For example, Table \ref{tbl:example_actions} shows all possible actions when
markups are restricted to 0, 10\% or 20\% and 0\% of capacity may be withheld
from two generators with one offer submitted for each.  Each row corresponds to
an action and the column values specify the percentage price markup and the
percentage of capacity to be withheld for each of the $n_gn_o$ offers.  The
size of the permutation matrix grows rapidly as $n_o$, $n_g$, $n_m$ and $n_w$
increase.

% For operation with learning methods that use look-up tables to store
% state-action values, an environment with $n_s$ discrete states and $n_a$
% discrete actions is defined.  An agent can not observe offers/bids submitted
% by competitor agents, but may sense other aspects of the power system
% model.  To ensure that the size of the environment state space is
% kept reasonable, the agent is limited to observing a demand forecast.  The
% initial demand at each bus $P_{d0}$, as defined in the original power system
% model, is assumed to be peak and the demand can follow a profile at each
% step $t$ of the simulation (See Chapter \ref{ch:exploitation}).  The state
% space is divided into discrete steps of size $P_{step} = (P_{d0} - P_d^{min}) /
% n_s$, where $P_d^{min}$ is the total demand at the lowest point of the profile.  The
% environment computes the total system demand $P_{dt}$ at step $t$ and returns
% an integer representation of the state
% \begin{equation}
% s_t = \frac{(P_{dt} - P_d^{min})}{P_{step}} + 1.
% \end{equation}
%
% To define the action space, a vector of percentage markups on marginal cost
% $m_e$ is defined for each environment $e$ along with a variable $n_o \in
% \mathbb{Z}^+$ which denotes the number of offers/bids to be submitted by the
% agent.  (A similar vector of percentage markdowns on total capacity has also
% been implemented, but is not used in this thesis.)  A set of all unique
% permutations of markup for $n_o$ offers/bids of length $n_a$ is formed, from
% which the agent must select select.  The action vector that the discrete
% environment is passed consists of a single integer value, corresponding to the
% column index in the agent's action value table.  The quantity and price for
% each offer/bid submitted to the market is taken from the vector of
% permutations using the $a_t$ as the index.  An example of the possible
% permutations of 0, 10 and 20\% markups for a portfolio of two generators is
% given in Table \ref{tbl:example_actions}.  It should be clear how quickly the
% number of possible actions can grow as the number of possible markups and the
% size of the portfolio increases.

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
$a$ &$m_1$ &$m_2$ &$w_1$ &$w_2$ \\
\hline
 0 &0 &0 &0 &0 \\
 1 &0 &10 &0 &0 \\
 2 &0 &20 &0 &0 \\
 3 &10 &0 &0 &0 \\
 4 &10 &10 &0 &0 \\
 5 &10 &20 &0 &0 \\
 6 &20 &0 &0 &0 \\
 7 &20 &10 &0 &0 \\
 8 &20 &20 &0 &0 \\
\hline
\end{tabular}
\caption{Example discrete action domain.}
\label{tbl:example_actions}
\end{center}
\end{table}

\subsubsection{Continuous Market Environment}
% For a power system with $n_b$ buses and $n_l$ branches, the visible state of
% the environment is a vector $s_e$ of length $n_s = 2n_b + 2n_l$.
%$s^i_g$ represents the  for the agent associated with generator $i$.
% $s^i_e$ is composed of sensor values for all buses, branches and generators.
% \begin{equation}
% s^i_{e,l} =
% \begin{bmatrix}
% P_f\\
% Q_f\\
% P_t\\
% Q_t\\
% \mu_{S_f}\\
% \mu_{S_t}
% \end{bmatrix}, \quad
% s^i_{e,b} =
% \begin{bmatrix}
% V_m\\
% V_a\\
% \lambda_P\\
% \lambda_Q\\
% \mu_{v_{min}}\\
% \mu_{v_{max}}
% \end{bmatrix}, \quad
% s^i_{e,g} =
% \begin{bmatrix}
% P_g\\
% \mu_{p_{min}}\\
% \mu_{p_{max}}\\
% \mu_{q_{min}}\\
% \mu_{q_{max}}
% \end{bmatrix}\ \quad
% s^i_e =
% \begin{bmatrix}
% s^i_{e,b}\\
% s^i_{e,b}\\
% s^i_{e,g}
% \end{bmatrix}
% \end{equation}
% Not all of the values are used by each agent and they are filtered according
% to the agent's task.

% For agents operating policy gradient methods, continuous environments that
% output $n_s$ sensors and accept $n_a$ actions are defined.  Each environment
% may be configured for actions that specify just price or price and quantity.  If
% $q_e^i = 0$ where $q_e^i \in (0,1)$ then the agent's action is price selection
% and the offer/bid quantity is determined by the maximum rated capacity of the
% generator in question divided by the number of offers being submitted for it.
% The environment accepts an action vector $a_e$ of length $n_a$ if $q_e^i = 0$,
% otherwise of length $2n_a$.  If $q_e^i = 0$, the $i$-th element of $a_e$ is
% the offered/bid price in \$/MWh, where $i = 1,2,\dotsc n_{in}$.  If $q_e^i =
% 1$, the $i$-th element of $a_e$ is the offered/bid price in \$/MWh, where $i =
% 1,3,5,\dotsc n_{in}-1$ and the $j$-th element of $a_e$ is the offered/bid
% quantity in MW where $j = 2,4,6,\dotsc n_{in}$.  The action vector passed to
% the environment is converted into sets of offers/bids that are submitted to
% the market model.
% %If $q_e^i = 0$, then $qty = p_{max}/n_{in}$.

A continuous market environment that outputs a state vector $s$, where $s_i
\in \mathbb{R}$, and accepts an action vector $a$, where $a_i \in \mathbb{R}$,
is defined for agents operating policy gradient methods.  Scalar variables
$m_{max}$ and $w_{max}$ define the maximum allowable percentage markup on
marginal cost and the maximum allowable percentage of capacity that can be
withheld, respectively.  Again, $n_o$ defines the number of offers to be
submitted for each generator associated with the environment.

The state vector may consist of any data from the power system or market
model.  For example: bus voltages, branch power flows, generator limit
Lagrangian multipliers etc.  Each element of the vector provides one input to the neural network used for policy function approximation.

The action vector $a$ has length $2n_gn_o$.  Element $a_i$, where $0\leq a_i
\leq m_{max}$, corresponds to the price markup and $a_{i+1}$, where $0\leq
a_{i+1} \leq w_{max}$, to the withhold of capacity for the $(i/2)^{th}$ offer,
where $i=0,2,4,\dotsc,2n_gn_o$.

Not having to discretize the state space and compute a matrix of action
permutations greatly simplifies the implementation of a continuous environment
and increases in $n_g$ and $n_o$ only impact the number of output nodes
in the policy function approximator.

\subsection{Agent Task}
To allow alternative goals, such a profit maximisation or the meeting some
target level for plant utilisation, to be associated with a single type of
environment, an agent does not interact directly with its environment, but is
paired with a particular \textit{task}. A task defines the reward returned to
the agent and thus defines the agent's purpose.

For all simulations in this thesis the goal of each agent is to maximise
financial profit.  Rewards are defined as the sum of earnings from
the previous period $t$ as determined by the difference between revenue from
cleared offers and marginal cost at the total cleared quantity.  As explained in
Section \ref{sec:moody}, utilising some measure of risk adjusted return might be
of interest in the context of simulated electricity trade and this would simply
involve the definition of a new task and would not require any modification of
the environment.

% Sensor data from the environment is filtered according to the task
% being performed.  Agents with value-function learning methods use a table to
% store state-action values, with one row per environment state.  Thus, observations
% consist of a single value $s_v$, where $s_v \leq n_s$ and $s_v \in
% \mathbb{Z}^+$.

Agents with policy-gradient learning methods approximate their policy functions
using artificial neural networks that are presented with input vector $v$ of
length $n_s$ where $v_i \in \mathbb{R}$.  To condition the environment state
before input to the connectionist system, where possible, a vector $s_{min}$
of minimum sensor values and a vector $s_{max}$ of maxmimum sensor values is
defined.  These are used to calculated a normalised state vector
\begin{equation}
v = 2\left(\frac{s - s_{min}}{s_{max} - s_{min}}\right) - 1
\end{equation}
where $-1 \leq v_i \leq 1$.

The output from the policy function approximator $y$ is denormalized using
vectors of minimum and maximum action limits, $a_{min}$ and $a_{max}$
respectively, to give an action vector
\begin{equation}
a = \left(\frac{y + 1}{2}\right)(a_{max} - a_{min}) + a_{min}
\end{equation}
with valid values for price markups and capacity withholding.

\subsection{Market Participant Agent}
Each agent is defined as an entity capable of producing an action $a$
based on previous observations of its environment $s$.
% , where $a_i$ and $s_i$
% are vectors of length $n_a$ and $n_s$ respectively, where
% $n_s$ is the total number of states and $n_a$ is the total number of actions.
%Figure X shows in UML that
The UML class diagram in Figure \ref{fig:cls_agent} illustrates how each agent
in PyBrain is associated with a \textit{module}, a \textit{learner} (variant
Roth-Erev in the case of the diagram), a \textit{dataset} and an
\textit{explorer}.

\ifthenelse{\boolean{includefigures}}{\input{tikz/cls_agent}}{}

The module is used to determine the agent's policy for action selection and
returns an action vector $a_m$ when activated with observation~$s$.  When
using value function based methods the module is a $n_s \times n_a$ table:
\begin{equation}
\bordermatrix[{[]}]{%
 & a_0 & a_1 & & a_{n_a} \cr
s_0 & v_{0,0}& v_{0,1}& \dotsb & v_{0,m} \cr
s_1 & v_{1,0}& \ddots& & \vdots \cr
    & \vdots & & \ddots & \vdots \cr
s_{n_s} & v_{n,0} & \dotsb & \dotsb & v_{n_s,n_a}
}
\end{equation}
where each element $v_{i,j}$ is the value associated with selecting action
$j$ in state $i$.  When using a policy gradient method, the module is a
multi-layer feed-forward artificial neural network that outputs a vector $a$
when presented with observation~$s$.

The learner can be any reinforcement learning algorithm that modifies the
values/parameters of the module to increase expected future reward.  The
dataset stores state-action-reward triples for each interaction between the
agent and its environment.  The stored history is used by value-function
learners when computing updates to the table values.  Policy gradient learners
search directly in the space of the policy network parameters.

Each learner has an association with an explorer that returns an explorative
action $a_e$ when activated with the current state $s$ and action $a_m$ from
the module.  Softmax and $\epsilon$-greedy explorers are implemented for
discrete action spaces.  Policy gradient methods use a module
that adds Gaussian noise to the output of the policy function approximation
module.  The explorer has a parameter $\sigma$ that relates to the standard
deviation of the normal distribution.  The actual standard deviation
\begin{equation}
\sigma_e = \begin{cases}
\ln(\sigma + 1) + 1 & \text{if $\sigma \geq 0$}\\
\exp(\sigma) & \text{if $\sigma < 0$}
\end{cases}
\end{equation}
to allow for negative $\sigma$ values.

%TODO: PG explorer module.

% For example, the $\epsilon$-greedy explorer
% has a randomness parameter $\epsilon$ and a decay parameter $d$.  When the
% $\epsilon$-greedy explorer is activated, a random number $x_r$ is drawn where
% $0 \leq x_r \leq 1$.  If $x_r < \epsilon$ then a random vector of the same
% length as $a_e$ is returned, otherwise $a_e = a_m$.

\subsection{Simulation Event Sequence}
\ifthenelse{\boolean{includefigures}}{\input{tikz/cls_experiment}}{}
Each simulation consists of one or more task-agent pairs.  Figure
\ref{fig:cls_experiment} shows the class associations for a simulation
experiment.  At the beginning of each simulation step (trading period) $t$ the
market is initialised and all existing offers are removed.  Figure
\ref{fig:seq_action} is a UML sequence diagram that illustrates the process of
choosing and performing an action.  For each task-agent tuple an observation
$s_t$ is retrieved from the task and integrated into the agent.  When an action
is requested from the agent its module is activated with $s_t$ and the action
$a_e$ is returned.  Action $a_e$ is performed on the environment associated with
the agent's task.

\ifthenelse{\boolean{includefigures}}{\input{tikz/seq_action}}{}
\ifthenelse{\boolean{includefigures}}{\input{tikz/seq_reward}}{}

When all actions have been performed the offers are cleared by the market using
the solution to a newly formed optimal power flow problem.  Figure
\ref{fig:seq_action} illustrates the reward precess that follows.  The cleared offers associated with
the generators in the task's environment are retrieved from the market and the
reward $r_t$ in \$ is computed from the difference between revenue and marginal
cost at the total cleared quantity.  For each generator in the agent's portfolio
that was previously online and is not dispatched, a shutdown cost $C_{down}$ is
subtracted from the reward.
% \begin{equation} r_t = \mbox{revenue} - (c_{fixed} + c_{variable})
% \end{equation}
The reward $r_t$ is given to the associated agent and the value is stored, along
with the previous state $s_t$ and selected action $a_e$, under a new sample is
the dataset.

\ifthenelse{\boolean{includefigures}}{\input{tikz/seq_learn}}{}

The learning process is illustrated by the UML sequence diagram in Figure
\ref{fig:seq_learn}.  Each agent learns from its actions using $r_t$, at which
point the values or parameters of the module associated with the agent are
updated according to the output of the learner's algorithm.  Each agent is then
reset and the history of states, actions and rewards is cleared.

The combination of action, reward and learning processes for each agent
constitutes one step of the simulation and they are repeated until a specified
number of steps are complete.

% \subsection{Auction Example}
% To demonstrate the simulation process this example will walk through a single
% step.  The six bus power system model used in this example is adapted from
% \citeA[pp.~104, 112, 119, 123-124, 549]{wood:pgoc} and a one-line diagram for
% the case is given in Figure \ref{fig:case6ww}.  The model has 3 generators
% with a total capacity of 530MW and the total system load is 210MW.  The
% initial generator costs are defined by quadratic functions of the form $a + bx
% + cx^2$, where $x$ is the generator set-point, with the parameters are given in
% Table \ref{tbl:ex_coeffs}.
% \begin{table}
% \begin{center}
% \begin{tabular}{c|c|c|c}
% \hline
% Generator bus & $a$ & $b$ & $c$ \\
% \hline
% 1 & 215.0 & 10.0 & 0.005 \\
% 2 & 200.0 & 12.0 & 0.008 \\
% 3 & 240.0 & 15.0 & 0.010 \\
% \hline
% \end{tabular}
% \end{center}
% \caption{Generator cost function coefficients.}
% \label{tbl:ex_coeffs}
% \end{table}
%
% Suppose each offers half of its capacity with a markup of 10\% and the
% remainder marked up by 20\%.  This correlates to a set of offers with
% quantities and prices given in Table \ref{tbl:ex_offers}.
% \begin{table}
% \begin{center}
% \begin{tabular}{c|cc|cc}
% \hline
% Generator bus & \multicolumn{2}{c}{Offer 1} & \multicolumn{2}{|c}{Offer 2}\\
%  & MW & \$/MWh & MW & \$/MWh \\
% \hline
% 1 & 100 & 20 & 100 & 30 \\
% 2 & 75  & 25 & 75  & 40 \\
% 3 & 90  & 30 & \sout{90}  & \sout{50} \\
% \hline
% \end{tabular}
% \end{center}
% \caption{Offered quantities and prices.}
% \label{tbl:ex_offers}
% \end{table}
% Setting a price
% cap of \$45 causes the second offer from the generator at bus 3 to be withheld
% and ignored in the conversion to piecewise linear cost functions.
%
% Table
% \ref{tbl:ex_pwl} lists the points of the resulting piecewise linear cost
% functions and Figure X plots the original marginal cost function and the cost
% function corresponding to the submitted offers for the generator at bus 1.
% \begin{table}
% \begin{center}
% \begin{tabular}{c|c|c|c}
% \hline
% Generator bus & $(P_g, C)$ & $(P_g, C)$ & $(P_g, C)$ \\
% \hline
% 1 & (0, 215) & (100, 10.0) & (200, 0.005) \\
% 2 & (0, 200) & (75, 12.0) & (150, 0.008) \\
% 3 & (0, 240) & (90, 15.0) & (180, 0.010) \\
% \hline
% \end{tabular}
% \end{center}
% \caption{Piecewise linear cost function points.}
% \label{tbl:ex_pwl}
% \end{table}
% Also plotted is the generator set-point from the optimal power flow
% solution and the elevation of the nodal marginal price caused by network
% congestion and branch losses.  The diagram indicates the difference between the
% original marginal cost function and the cleared price that is the earnings from
% that generator and would be used as part of the reward for the responsible
% agent.

\section{Summary}
The power exchange auction market model defined in this chapter provides a layer
of abstraction over the underlying optimal power flow problem and presents
agents with a simple interface for selling power.  The modular nature of the
simulation framework described allows the type of learning algorithm, policy
function approximator, exploration technique or task to be easily changed.  The
framework can simulate competitive electric power trade using any conventional
bus-branch power system model with little configuration, but provides the
ability to adjust all of the main aspects of a simulation. The modular framework
and its support for easy configuration is intended to allow transparent
comparison of learning methods in the domain of electricity trade under a number
of different scenarios.
