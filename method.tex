\chapter{Modelling Power Trade}
\label{ch:method}
The present chapter defines the model used in this thesis to simulate electric
power trade. The first section describes how optimal power flow solutions
are used to clear offers and bids submitted to a power exchange auction model.
The second section defines how market participants are modelled as agents that
use reinforcement learning algorithms to adjust their bidding behaviour.
The interaction of market participants with the auction is coordinated by a
multi-agent system that can be configured in many ways.

% Societies reliance on secure energy supplies and the high volumes of
% electricity typically consumed render it impractical to experiment with
% radically new approaches to energy trade on real systems.  This section
% explains the approach taken modelling real systems in software such that they
% may be simulated computationally.  The method by which the physical power
% systems, that deliver electricity to consumers, were modeled is given, as well
% as for the mechanisms that facilitate trade and participants that utilise
% these mechanisms.
%
% \subsection{Energy market model}
% Mechanisms for facilitating competitive trade between electricity producers and
% consumers differ greatly in the specifics of their implementations in coutries
% throughout the world.  However, fundamentally they either provide a
% centralised pool through which all electricity is bought and sold or they
% permit producers and suppliers to trade directly.
%
% The UK transmission network is frequently congested[].  The thermal limits of
% transmission lines between particular areas are often reached.  The balancing
% mechanism is the financial instrument used by the system operator to resolve
% constraint issues and energy imbalances.  Should the market not be suitably
% effective in this function the system operator may choose to contract outwith
% the balancing mechanism.  By way of incentive to match demand and avoid
% congestion, imbalance charges are imposed on responsible participants.  There
% is some evidence to suggest that centralised resolution by a system operator
% and socialisation of the incurred costs leads to inefficient despatch of
% generators[Neuhoff].
%
% There are a number of alternative approaches to congestion
% resolution.
% %\cite{neuhoff:power}
%
% \subsection{Transmission capacity rights}
% One approach is to issue contracts for transmission capacity rights or
% equivalent financial rights.  The maximum available transmission capacity
% being auctioned for certain periods of time and firm contracts made entitling
% owners to full compensation upon curtailment or
% withdrawl.
% %\cite{efet:principles}.
%
% The states of Pensylvania, New Jersey and Maryland (PJM) operate a
% non-compulsory power pool with nodal market-clearing prices based on
% competitive bids.  This is complemented by daily and monthly capacity markets
% plus the monthly auction of Financial Transmission Rights to provide a hedging
% mechanism against future congestion charges.
%
% \subsection{Transmission charging}
% Impose delivery charges which increase as network constraints are approached.
%
%
% \subsection{Extended bids/offers}
% Request extended bids and offers which include costs associated with the
% adjustment of participant's desired position.
%
% \subsection{Software agents}
% Participants are modeled in software also.  The nature of a highly distributed
% power system dictates that a very large number of entities may be interacting
% in the marketplace.  Economic studies regularly integrate participant logic
% into the same optimisation problem as the market.  However, this does not
% scale to large numbers of individual participants.  Separating participant
% logic into individual software agents allows their action selection procedures
% to be processed in simultaneously.  The definition of an agent in this context
% emerges from the machine learning technique employed to implement the
% competitive decision making process.

\section{Electricity Market Model}
A double-sided power exchange auction market is used to compare the
algorithms trading abilities.  To determine the dispatch of generators bespoke
implementations of the optimal power flow formulations from \matpower
\cite[\S5]{pserc:mp_manual} are used.  A class diagram in the Unified Modelling
Language (UML) for the underlying power system model is shown in Figure
\ref{fig:cls_pylon}.  Both the DC and AC formulations are utilised. The
trade-offs between DC and AC models have been examined in \citeA{overbye:acdc}
and DC models have been found suitable for most nodal marginal price
calculations.  They are considerably less computationally expensive and are
used in simple initial experiments.  The more accurate AC optimal power flow
formulation is required for the more challenging experiments that test agents
in their ability to exploit constraints in the power system models. As
in \matpower, piecewise linear generator cost functions are modelled using the
constrained cost variable method \cite{zimmerman:ccv}.  Unlike \matpower, the
formulations are not extended to facilitate user-defined cost functions or
observe generator P-Q capability curves.

%\input{tikz/cls_pylon}

\subsection{Unit De-commitment}
The optimal power flow formulation used constrains generator set-points
between upper and lower power limits.  Expensive generators can not be
completely shutdown, even if doing so would result in a lower total system
cost.  The online status of generators could be made an optimisation variable,
but being boolean the resulting programs would become mixed-integer and
considerably more difficult to solve, particularly in the case of the
non-linear AC formulation.

To compute a least cost commitment and dispatch the unit de-commitment algorithm from
\citeA[p.57]{pserc:mp_manual} is used.  Algorithm~\ref{alg:ud} shows the
process of shutting down the most expensive units until the minimum generation
capacity is less than the total load capacity and then solving repeated
optimal power flow problems with candidate generating units that are at their
minimum active power limit deactivated.  The lowest cost solution is returned
when no further improvement can be made and no candidate generators remain.
\begin{algorithm}%[H]
\caption{Unit de-commitment}
\label{alg:ud}
\begin{algorithmic}[1]
%\STATE $\text{initialise}~N \leftarrow 0$
%\STATE $P_{d} \leftarrow \text{total load capacity}$
%\STATE $P_{g}^{min} \leftarrow \text{total min.\ gen.\ capacity}$
\WHILE{$\sum P_{g}^{min} > \sum P_{d}$}
%	\STATE $N \leftarrow N + 1$
	\STATE shutdown most expensive unit
\ENDWHILE

%\STATE $\text{solve initial OPF}$
\STATE $f \leftarrow \text{initial total system cost}$

\REPEAT
	\STATE $c \leftarrow \text{generators at } P_{min}$
	\FOR{$g$ in $c$}
		\STATE $d \leftarrow \text{true}$
		\STATE shutdown $g$
		\STATE $f^\prime \leftarrow \text{new total system cost}$
		\IF{$f^\prime < f$}
			\STATE $f \leftarrow f^\prime$
			\STATE $g_{c} \leftarrow g$
			\STATE $d \leftarrow \text{false}$
		\ENDIF
		\STATE startup $g$
	\ENDFOR
	\STATE shutdown $g_c$
\UNTIL{$d = \text{true}$}
\end{algorithmic}
\end{algorithm}

\subsection{Power Exchange}
Agents do not directly adjust the cost functions of the
generators in their portfolio, but submit offers to sell and/or bids to buy
blocks of active or reactive power to a power exchange auction market
\cite[p.92]{pserc:mp_manual}, with prices specified in \$/MWh or  \$/MVArh.
The offers/bids are converted into corresponding generator/dispatchable load
capacities and piecewise linear cost functions.  The newly configured
units form a unit de-commitment optimal power flow problem, the solution
of which holds the dispatch points and nodal prices which are used to
determine the proportion of each offer/bid block that should be cleared and
the associated price for each.  Pricing may be uniform, where each is cleared
at the market clearing price, or discriminatory, where the cleared price is
that of the original offer/bid.  The cleared offers/bids are returned to the
agents and used to determine the revenue from which each agent's earnings or
losses are derived.

Maximum offer price limits and minimum bids price limits can be set.  The
clearing process the market begins by withholding offers/bids outwith these
limits, along with those specifying non-positive quantities. Valid offers/bids
for each generator are then sorted into non-decreasing/non-increasing order
and used to form the new piecewise-linear cost functions and generator power
limits.
% TODO: Provide an example.

\section{Multi-Agent System}
\label{sec:mas}
Market participants are modelled using software agents from PyBrain
\cite{schaul:2010} which use reinforcement learning algorithms to adjust their
behaviour.  Their interaction with the market is coordinated in multi-agent
experiments, the structure of which is derived from PyBrain's single player
designs.  The agent's do not interact directly with their environment, but are
associated with a particular task that defines the agent's reward and hence
gives it a purpose.  This section defines the agent's environment, their
task and the design of the artificial neural networks and the tables used in
determining their action selection policies.  The process by which the policies
are modified by the agent's learning algorithm is explained and the sequence
of interactions within the multi-agent system is illustrated.

\subsection{Environment}
In each experiment, agents are endowed with a portfolio of one or more
generators from the electric power system model (See Figure
\ref{fig:cls_pylon}). Figure \ref{fig:cls_pyreto} illustrates the association
between an agent's environment and the generator model along with the
association with an instance of the market for the submission of offers/bids by
the agent. The two operations that an environment is responsible for
are: returning a vector representation of the current state and accepting an
action vector which transforms the environment into a new state.

%\input{tikz/cls_pyreto}

For a power system with $n_b$ buses and $n_l$ branches, the visible state of
the environment is a length $n_s$ vector where $n_s = 2n_b + 2n_l$.  $s^i_g$
represents the  for the agent associated with generator $i$.
% $s^i_e$ is composed of sensor values for all buses, branches and generators.
% \begin{equation}
% s^i_{e,l} =
% \begin{bmatrix}
% P_f\\
% Q_f\\
% P_t\\
% Q_t\\
% \mu_{S_f}\\
% \mu_{S_t}
% \end{bmatrix}, \quad
% s^i_{e,b} =
% \begin{bmatrix}
% V_m\\
% V_a\\
% \lambda_P\\
% \lambda_Q\\
% \mu_{v_{min}}\\
% \mu_{v_{max}}
% \end{bmatrix}, \quad
% s^i_{e,g} =
% \begin{bmatrix}
% P_g\\
% \mu_{p_{min}}\\
% \mu_{p_{max}}\\
% \mu_{q_{min}}\\
% \mu_{q_{max}}
% \end{bmatrix}\ \quad
% s^i_e =
% \begin{bmatrix}
% s^i_{e,b}\\
% s^i_{e,b}\\
% s^i_{e,g}
% \end{bmatrix}
% \end{equation}
% Not all of the values are used by each agent and they are filtered according
% to the agent's task.

The environment accepts a $n_a$ length vector of action values
$a_e$ if $s_{bid} = 0$, otherwise a $2n_a \times 1$ vector.  If $s_{bid} = 0$,
the $i$-th element of $a_e$ is the offered/bid price in \$/MWh, where
$i = 1,2,\dotsc n_{in}$.  If $s_{bid} = 1$, the $j$-th element of $a_e$ is the
offered/bid price in \$/MWh, where $j = 1,3,5,\dotsc n_{in}-1$ and the $k$-th
element of $a_e$ is the offered/bid quantity in MW where $j = 2,4,6,\dotsc
n_{in}$.  The action vector is separated into offers/bids and submitted to the
market.  If $s_{bid} = 0$, then $qty = p_{max}/n_{in}$.

\subsection{Task}
An agent does not interact directly with it's environment, but is associated
with a particular task.  This allows multiple tasks, such a profit
maximisation or meeting a target utilisation level, to be associated with a
single environment.  A task defines the reward returned to the agent and
thus associates a purpose with an environment.  For all experiments in this
thesis the goal of each participant agent is to maximise financial profit and
the rewards are thus defined as the sum of earnings from the previous period
$t$ as determined by the revenue from the market and any incurred costs.

Sensor data from the environment is filtered according to the task being
performed.  Agents using the value-function methods use a table to store
state-action values, with one row per environment state.  Thus, observations
consist of a single value $s_v$, where $s_v \leq n_s$ and $s_v \in
\mathbb{Z}^+$.

Agents using the policy-gradient methods have their policy functions
approximated by connectionist systems that use an input vector $w_i$ of
arbitrary length where the $i$-th element $\in \mathbb{R}$.  To condition the
environment state before input to the connectionist system, where possible,
each sensor $i$ in the state vector $s$ is associated with a minimum value
$s_{i,min}$ and a maximum value $s_{i,max}$.   The state vector is normalised
to between $-1$ and $+1$ giving a vector:
\begin{equation}
s_c = 2\left(\frac{s - s_{min}}{s_{max} - s_{min}}\right) - 1
\end{equation}

The output from the policy $a_c$ is denormalised using minimum action limits
$a_{min}$ and maximum action limits $a_{max}$ giving an action vector
\begin{equation}
a = \left(\frac{a + 1}{2}\right)(a_{max} - a_{min}) + a_{min}
\end{equation}
to be performed on the environment.

\subsection{Agent}
Each agent $i$ is defined as an entity capable of producing an action $a_i$
based on previous observations of its environment $s_i$, where $a_i$ and $s_i$
are vectors of arbitrary length.  Figure X illustrates in UML how each agent is
associated with a \textit{module}, a \textit{learner}, a \textit{dataset} and
an \textit{explorer}. The module is used to determine the agent's policy for
action selection and returns an action vector $a_m$ when activated with observation $s_t$.  The value-function
methods under test use modules which represent a $N \times M$ table, where $N$
is the total number of states and $M$ is the total number of actions.
\begin{equation}
\bordermatrix[{[]}]{%
 & a_0 & a_1 & & a_n \cr
s_0 & v_{1,1}& v_{1,2}& \dotsb & v_{1,m} \cr
s_1 & v_{2,1}& \ddots& & \vdots \cr
    & \vdots & & \ddots & \vdots \cr
s_n & v_{n,1} & \dotsb & \dotsb & v_{n,m}
}
\end{equation}
Whereas for the policy gradient methods, the module is a connectionist network
of other modules.  The learner can use any
reinforcement learning algorithm and modifies the values/parameters of the
policy module to increase expected future reward.  The dataset stores
state-action-reward tuples for each interaction between the agent and its
environment.  The stored history is used by value-function learners when
computing updates to the policy values.  Policy gradient learners search
directly in the space of the policy network parameters.

% TODO: Agent, module and learner class diagram.

Value-function learners have an association with an explorer module which
returns an explorative action $a_e$ when activated with the current state $s_t$
and action $a_m$ from the policy module.  For example, the $\epsilon$-greedy
explorer has a randomness parameter $\epsilon$ and a decay parameter $d$.  When
the $\epsilon$-greedy explorer is activated, a random number $x_r$ is drawn
where $0 \leq x_r \leq 1$.  If $x_r < \epsilon$ then a random vector of the same
length as $a_e$ is returned, otherwise $a_e = a_m$.

\subsection{Simulation Event Sequence}

%\input{tikz/cls_experiment}

In each simulation of a system consisting of one or more task-agent pairs a
sequence of interactions is coordinated, as illustrated in Figure
\ref{fig:cls_experiment}.

%\input{tikz/seq_action}

At the beginning of each step/period the market is initialised and all
offers/bids removed.  From each task-agent tuple $(T,A)$ an observation $s_t$
is retrieved from $T$ and integrated into agent $A$.  When an action is
requested from $A$ its module is activated with $s_t$ and the action $a_e$ is
returned.  $a_e$ is performed on the environment of $A$ via its associated task
$T$.  Recall, this process involves the submission of offer/bids to the market.
Figure \ref{fig:seq_action} is a UML sequence diagram that illustrates the
process of performing actions.

%\input{tikz/seq_reward}

Once all actions have been performed the offer/bids are cleared using the
auction mechanism.  Each task $T$ is requested to return a reinforcement reward
$r_t$. All cleared offers/bids associated with the generator in the environment
of $T$ are retrieved from the market and $r_t$ is computed from the difference
between revenue and cost values.
\begin{equation}
r_t = \mbox{revenue} - (c_{fixed} + c_{variable})
\end{equation}
The reward $r_t$ is given to agent $A$ and the value is stored under a new
sample is the dataset, along with the last observation $s_t$ and the last action
performed $a_e$.  Figure \ref{fig:seq_action} is a UML sequence diagram that
illustrates the reward process.

%\input{tikz/seq_learn}

As illustrated in Figure \ref{fig:seq_learn}, each agent learns from its
actions using $r_t$, at which point the values/parameters of the module of $A$
are updated according to the algorithm of the learner. This constitutes one
step of the simulation and the process is repeated until the specified number
of steps are complete.  Unless agents are reset, the complete history of
states, actions and received rewards is stored in the dataset of each agent.

\section{Summary}
