\chapter{Modelling Power Trade}
\label{ch:method}
The present chapter defines the model used in this thesis to simulate electric
power trade. The first section describes how optimal power flow solutions are
used to clear offers and bids submitted to a simulated power exchange auction.
The second section defines how market participants are modelled as agents that
use reinforcement learning algorithms to adjust their bidding behaviour. It
explains the modular structure of a multi-agent system that coordinates the
interactions of the auction model and the market participants.

% Societies reliance on secure energy supplies and the high volumes of
% electricity typically consumed render it impractical to experiment with
% radically new approaches to energy trade on real systems.  This section
% explains the approach taken modelling real systems in software such that they
% may be simulated computationally.  The method by which the physical power
% systems, that deliver electricity to consumers, were modeled is given, as well
% as for the mechanisms that facilitate trade and participants that utilise
% these mechanisms.
%
% \subsection{Energy market model}
% Mechanisms for facilitating competitive trade between electricity producers and
% consumers differ greatly in the specifics of their implementations in coutries
% throughout the world.  However, fundamentally they either provide a
% centralised pool through which all electricity is bought and sold or they
% permit producers and suppliers to trade directly.
%
% The UK transmission network is frequently congested[].  The thermal limits of
% transmission lines between particular areas are often reached.  The balancing
% mechanism is the financial instrument used by the system operator to resolve
% constraint issues and energy imbalances.  Should the market not be suitably
% effective in this function the system operator may choose to contract outwith
% the balancing mechanism.  By way of incentive to match demand and avoid
% congestion, imbalance charges are imposed on responsible participants.  There
% is some evidence to suggest that centralised resolution by a system operator
% and socialisation of the incurred costs leads to inefficient despatch of
% generators[Neuhoff].
%
% There are a number of alternative approaches to congestion
% resolution.
% %\cite{neuhoff:power}
%
% \subsection{Transmission capacity rights}
% One approach is to issue contracts for transmission capacity rights or
% equivalent financial rights.  The maximum available transmission capacity
% being auctioned for certain periods of time and firm contracts made entitling
% owners to full compensation upon curtailment or
% withdrawl.
% %\cite{efet:principles}.
%
% The states of Pensylvania, New Jersey and Maryland (PJM) operate a
% non-compulsory power pool with nodal market-clearing prices based on
% competitive bids.  This is complemented by daily and monthly capacity markets
% plus the monthly auction of Financial Transmission Rights to provide a hedging
% mechanism against future congestion charges.
%
% \subsection{Transmission charging}
% Impose delivery charges which increase as network constraints are approached.
%
%
% \subsection{Extended bids/offers}
% Request extended bids and offers which include costs associated with the
% adjustment of participant's desired position.
%
% \subsection{Software agents}
% Participants are modeled in software also.  The nature of a highly distributed
% power system dictates that a very large number of entities may be interacting
% in the marketplace.  Economic studies regularly integrate participant logic
% into the same optimisation problem as the market.  However, this does not
% scale to large numbers of individual participants.  Separating participant
% logic into individual software agents allows their action selection procedures
% to be processed in simultaneously.  The definition of an agent in this context
% emerges from the machine learning technique employed to implement the
% competitive decision making process.

\section{Electricity Market Model}
It is a double-sided power exchange auction market model is used in this
thesis to compare the learning algorithm's trading abilities.  To determine the
dispatch of generators, bespoke implementations of the optimal power flow
formulations from \matpower \cite[\S5]{pserc:mp_manual} are used.  Both the DC
and AC formulations are utilised. The trade-offs between DC and AC models have
been examined in \citeA{overbye:acdc} and DC models have been found suitable
for most nodal marginal price calculations.  They are considerably less
computationally expensive and are used in the more simple initial experiments.
The accurate AC optimal power flow formulation is required for the more
challenging experiments which test learning algorithms in their ability to
exploit constraints in the power system models.  A class diagram in the
Unified Modelling Language (UML) for the object-orientated power system model
used is shown in Figure \ref{fig:cls_pylon}.  As in \textsc{Matpower},
piecewise linear generator cost functions are modelled using the constrained cost variable
method \cite{zimmerman:ccv}.  Unlike \textsc{Matpower}, the optimal power flow
formulations are not extended to facilitate user-defined cost functions or
observe generator P-Q capability curves.

%\input{tikz/cls_pylon}

\subsection{Unit De-commitment}
The optimal power flow formulations used constrain generator set-points
between upper and lower power limits.  Expensive generators can not be
completely shutdown, even if doing so would result in a lower total system
cost.  The formulations could incorporate the online status of generators in
the optimisation variable, but mixed-integer non-linear programs (MINLP) are
typically very challenging to solve.

To compute a least cost commitment and dispatch the unit de-commitment algorithm from
\citeA[p.57]{pserc:mp_manual} is used.  Algorithm~\ref{alg:ud} shows how the
process involves shutting down the most expensive units until
the minimum generation capacity is less than the total load capacity and then solving repeated
optimal power flow problems with candidate generating units that are at their
minimum active power limit deactivated.  The lowest cost solution is returned
when no further improvement can be made and no candidate generators remain.
\begin{algorithm}%[H]
\caption{Unit de-commitment}
\label{alg:ud}
\begin{algorithmic}[1]
%\STATE $\text{initialise}~N \leftarrow 0$
%\STATE $P_{d} \leftarrow \text{total load capacity}$
%\STATE $P_{g}^{min} \leftarrow \text{total min.\ gen.\ capacity}$
\WHILE{$\sum P_{g}^{min} > \sum P_{d}$}
%	\STATE $N \leftarrow N + 1$
	\STATE shutdown most expensive unit
\ENDWHILE

%\STATE $\text{solve initial OPF}$
\STATE $f \leftarrow \text{initial total system cost}$

\REPEAT
	\STATE $c \leftarrow \text{generators at } P_{min}$
	\FOR{$g$ in $c$}
		\STATE $d \leftarrow \text{true}$
		\STATE shutdown $g$
		\STATE $f^\prime \leftarrow \text{new total system cost}$
		\IF{$f^\prime < f$}
			\STATE $f \leftarrow f^\prime$
			\STATE $g_{c} \leftarrow g$
			\STATE $d \leftarrow \text{false}$
		\ENDIF
		\STATE startup $g$
	\ENDFOR
	\STATE shutdown $g_c$
\UNTIL{$d = \text{true}$}
\end{algorithmic}
\end{algorithm}

\subsection{Power Exchange}
In the simulated power trade model, agents representing market participants do
not provide cost functions for the generators in their portfolio, but submit
offers to sell and/or bids to buy blocks of active or reactive power.  The
offers/bids are submitted to a power exchange auction market based on that from
\citeA[p.92]{pserc:mp_manual}.  The clearing process the begins by withholding
offers/bids outwith maximum offer and minimum bid price limits, along with
those specifying non-positive quantities. Valid offers/bids for each generator
are then sorted into non-decreasing/non-increasing order and are converted
into corresponding generator/dispatchable load capacities and piecewise linear
cost functions. The newly configured units form a unit de-commitment optimal
power flow problem, the solution of which holds generator set-points and nodal
prices which are used to determine the proportion of each offer/bid block that
should be cleared and the associated price for each.  Pricing may be uniform,
where each offer/bid is cleared at the price of the marginal unit, or
discriminatory, where the offer/bid is cleared at the price at which it
offered/bid (pay-as-bid). The cleared offers/bids are returned to the
agents and used to determine the revenue from which each agent's earnings or
losses are derived.

\subsection{Auction Example}
This example demonstrates how a set of offers and a set of bids are cleared
using an auction mechanism based on the Smart Market from
\citeA{pserc:mp_manual}.  The six bus power system model used in this example
is adapted from \citeA[pp.~104, 112, 119, 123-124, 549]{wood:pgoc} and a
one-line diagram for the case is given in Figure \ref{fig:case6ww}.  The model
has 3 generators with a total capacity of 530MW and the total system load is
210MW.  The initial generator costs are defined by quadratic functions of the
form $a + bx + cx^2$, where $x$ is the generator set-point, and the parameters
are given in Table \ref{tbl:ex_coeffs}.
\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c}
\hline
Generator bus & $a$ & $b$ & $c$ \\
\hline
1 & 215.0 & 10.0 & 0.005 \\
2 & 200.0 & 12.0 & 0.008 \\
3 & 240.0 & 15.0 & 0.010 \\
\hline
\end{tabular}
\end{center}
\caption{Generator cost function coefficients.}
\label{tbl:ex_coeffs}
\end{table}

Suppose each offers half of its capacity with a markup of 10\% and the
remainder marked up by 20\%.  This correlated to a set of offers with
quantities and prices as given in Table \ref{tbl:ex_offers}.
\begin{table}
\begin{center}
\begin{tabular}{|c|cc|cc|}
\hline
Generator bus & \multicolumn{2}{c}{Offer 1} & \multicolumn{2}{|c|}{Offer 2}\\
 & MW & \$/MWh & MW & \$/MWh \\
\hline
1 & 100 & 20 & 100 & 30 \\
2 & 75  & 25 & 75  & 40 \\
3 & 90  & 30 & 90  & 50 \\
\hline
\end{tabular}
\end{center}
\caption{Offered quantities and prices.}
\label{tbl:ex_offers}
\end{table}
Setting a price
cap of \$45 causes the seconf offer from the generator at bus 3 to be withheld
and ignored in the conversion to piecewise linear cost functions.  Table
\ref{tbl:ex_pwl} lists the points of the resulting piecewise linear cost
functions and Figure X plots the original marginal cost function and the cost
function corresponding to the submitted offers for the generator at bus 1.
\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c}
\hline
Generator bus & $(P_g, C)$ & $(P_g, C)$ & $(P_g, C)$ \\
\hline
1 & (0, 215) & (100, 10.0) & (200, 0.005) \\
2 & (0, 200) & (75, 12.0) & (150, 0.008) \\
3 & (0, 240) & (90, 15.0) & (180, 0.010) \\
\hline
\end{tabular}
\end{center}
\caption{Piecewise linear cost function points.}
\label{tbl:ex_pwl}
\end{table}
Also plotted is the generator set-point from the \textit{optimal power flow}
solution and the elevation of the nodal marginal price caused by network
congestion and branch losses.  The diagram indicates the difference between the
original marginal cost function and the cleared price that is the earnings from
that generator and is used as part of the reward for the responsible agent.

\newpage
\section{Multi-Agent System}
\label{sec:mas}
Market participants are modelled using software agents from PyBrain which use
reinforcement learning algorithms to adjust their behaviour \cite{schaul:2010}.
Their interaction with the market is coordinated in multi-agent experiments,
the structure of which is derived from PyBrain's single player designs.  In
PyBrain, agents do not interact directly with their environment, but are
associated with a particular \textit{task}.  This section describes the
agent's environment, its task and the design of the policy function
approximators and the action-value tables. The process by which each agent's
policy is updated by a learning algorithm is explained, along with the
sequence of interactions between multiple agents and the market.

\subsection{Environment}
In each experiment, agents are endowed with a portfolio of one or more
generators from the electric power system model shown in Figure
\ref{fig:cls_pylon}. Figure \ref{fig:cls_pyreto} illustrates the association
between an agent's environment and the generator model along with the
association with an instance of the market that is used the submission of
offers/bids. Each environment is responsible for \begin{inparaenum}[(i)]
\item returning a vector representation of the current state and \item
accepting an action vector which transforms the environment into a new state.
\end{inparaenum}
%\input{tikz/cls_pyreto}

\subsubsection{Discrete Environment}

\subsubsection{Continuous Environment}
For a power system with $n_b$ buses and $n_l$ branches, the visible state of
the environment is a vector $s_e$ of length $n_s = 2n_b + 2n_l$.
%$s^i_g$ represents the  for the agent associated with generator $i$.
% $s^i_e$ is composed of sensor values for all buses, branches and generators.
% \begin{equation}
% s^i_{e,l} =
% \begin{bmatrix}
% P_f\\
% Q_f\\
% P_t\\
% Q_t\\
% \mu_{S_f}\\
% \mu_{S_t}
% \end{bmatrix}, \quad
% s^i_{e,b} =
% \begin{bmatrix}
% V_m\\
% V_a\\
% \lambda_P\\
% \lambda_Q\\
% \mu_{v_{min}}\\
% \mu_{v_{max}}
% \end{bmatrix}, \quad
% s^i_{e,g} =
% \begin{bmatrix}
% P_g\\
% \mu_{p_{min}}\\
% \mu_{p_{max}}\\
% \mu_{q_{min}}\\
% \mu_{q_{max}}
% \end{bmatrix}\ \quad
% s^i_e =
% \begin{bmatrix}
% s^i_{e,b}\\
% s^i_{e,b}\\
% s^i_{e,g}
% \end{bmatrix}
% \end{equation}
% Not all of the values are used by each agent and they are filtered according
% to the agent's task.

For agent $i$ the environment $e_i$ may be configured for the submission of
offers/bids in terms of both price and quantity according to $q_e^i \in (0,1)$.
If $q_e^i = 0$ then the agent's action involves only price selection and the
offer/bid quantity determined by the maximum rated capacity of the generator in
question.  The environment accepts a vector $a_e$ of action values of length
$n_a$ if $q_e^i = 0$, otherwise $a_e$ is of length $2n_a$.  If $q_e^i = 0$, the
$i$-th element of $a_e$ is the offered/bid price in \$/MWh, where $i = 1,2,\dotsc
n_{in}$.  If $q_e^i = 1$, the $j$-th element of $a_e$ is the offered/bid
price in \$/MWh, where $j = 1,3,5,\dotsc n_{in}-1$ and the $k$-th element of
$a_e$ is the offered/bid quantity in MW where $j = 2,4,6,\dotsc n_{in}$.  The
action vector is separated into offers/bids and submitted to the market.  If
$q_e^i = 0$, then $qty = p_{max}/n_{in}$.

\subsection{Task}
To allow different goals for an agent (such a profit maximisation or the
meeting some target level for plant utilisation) to be associated with a single
type of environment, an agent does not interact directly with it,
but is instead paired with a particular \textit{task}. A task defines the
reward returned to the agent and thus defines the agent's purpose.  For all
experiments in this thesis the goal of each participant agent is to maximise
financial profit and the rewards are thus defined as the sum of earnings from
the previous period $t$ as determined by the revenue from the market and any
incurred costs.  As elaborated upon in Section X, utilising some measure of
risk adjusted return would simply involve the definition of a new task without
any need for modification of the environment.

Sensor data from the environment is filtered according to the task
being performed.  Agents using the value-function methods use a table to store
state-action values, with one row per environment state.  Thus, observations
consist of a single value $s_v$, where $s_v \leq n_s$ and $s_v \in
\mathbb{Z}^+$.

Agents using the policy-gradient learning methods approximate their policy
functions using artificial neural networks that are presented with input vector
$w$ of length $n_i$ where $w_i < n_i$ and $w_i \in \mathbb{R}$.  To condition
the environment state before input to the connectionist system, where possible,
each sensor $i$ in the state vector $s$ is associated with a minimum value
$s_{i,min}$ and a maximum value $s_{i,max}$.   The state vector is normalised
to a vector:
\begin{equation}
s_c = 2\left(\frac{s - s_{min}}{s_{max} - s_{min}}\right) - 1
\end{equation}
such that $-1 \leq s_c^i \leq 1$.

The output from the policy function approximator, $a_c$, is denormalised using
minimum action limits $a_{min}$ and maximum action limits $a_{max}$ giving an action vector
\begin{equation}
a = \left(\frac{a + 1}{2}\right)(a_{max} - a_{min}) + a_{min}
\end{equation}
with values for price (and optionally quantity) in suitable ranges for
application to the environment.

\subsection{Agent}
Each agent $i$ is defined as an entity capable of producing an action $a_i$
based on previous observations of its environment $s_i$, where $a_i$ and $s_i$
are vectors of arbitrary length.  Figure X shows in UML that in PyBrain
each agent is associated with a \textit{module}, a \textit{learner}, a \textit{dataset} and
an \textit{explorer}. The module is used to determine the agent's policy for
action selection and returns an action vector $a_m$ when activated with
observation $s_t$.  When using a value-function method the module is a $n_s
\times n_a$ table, where $n_s$ is the total number of states and $n_a$ is the
total number of actions.
\begin{equation}
\bordermatrix[{[]}]{%
 & a_0 & a_1 & & a_n \cr
s_0 & v_{1,1}& v_{1,2}& \dotsb & v_{1,m} \cr
s_1 & v_{2,1}& \ddots& & \vdots \cr
    & \vdots & & \ddots & \vdots \cr
s_n & v_{n,1} & \dotsb & \dotsb & v_{n,m}
}
\end{equation}
When using a policy gradient method, the module is a multi-layer feedforward
neural network.

The learner can be any reinforcement learning algorithm that modifies the
values/parameters of the module to increase expected future reward.  The dataset stores state-action-reward triples for each interaction between the
agent and its environment.  The stored history is used by value-function
learners when computing updates to the table values.  Policy gradient learners
search directly in the space of the policy network parameters.

% TODO: Agent, module and learner class diagram.

Each learner may have an association with an explorer which returns an
explorative action $a_e$ when activated with the current state $s_t$ and
action $a_m$ from the module.  For example, the $\epsilon$-greedy explorer
has a randomness parameter $\epsilon$ and a decay parameter $d$.  When the
$\epsilon$-greedy explorer is activated, a random number $x_r$ is drawn where
$0 \leq x_r \leq 1$.  If $x_r < \epsilon$ then a random vector of the same
length as $a_e$ is returned, otherwise $a_e = a_m$.

\subsection{Simulation Event Sequence}
%\input{tikz/cls_experiment}
Each experiment consists one or more agent-task pairs and Figure
\ref{fig:cls_experiment} illustrates the class relationship.
%\input{tikz/seq_action}
At the beginning of each simulation step (trading period) the market is
initialised and all existing offers/bids are removed.  From each task-agent tuple $(T,A)$ an
observation $s_t$ is retrieved from $T$ and integrated into agent $A$.  When
an action is requested from $A$ its module is activated with $s_t$ and the
action $a_e$ is returned.  Action $a_e$ is performed on the environment
associated with task $T$.  This is the process that involves the submission of
offer/bids to the market.  Figure \ref{fig:seq_action} is a UML sequence
diagram that illustrates the process of performing an action.
%\input{tikz/seq_reward}
When all actions have been performed the offer/bids are cleared using an
optimal power flow solution.  Each task $T$ is requested to return a reward
$r_t$. The cleared offers/bids associated with the generators in the environment of $T$ are retrieved from the market and $r_t$ is computed from
the difference between revenue and cost values.
\begin{equation}
r_t = \mbox{revenue} - (c_{fixed} + c_{variable})
\end{equation}
The reward $r_t$ is given to agent $A$ and the value is stored, along with the
previous state $s_t$ and selected action $a_e$, under a new sample is the
dataset.  The reward process is illustrated in a UML sequence diagram in Figure
\ref{fig:seq_action}.
%\input{tikz/seq_learn}
Each agent learns from its actions using $r_t$, at which point the
values/parameters of the module associated with $A$ are updated according to
the learner's algorithm.  Each agent is then reset and the history of states,
actions and rewards is cleared.  The learning process is illustrated by the UML
sequence diagram in  Figure \ref{fig:seq_learn}.

All of this constitutes one step of the simulation and the process is repeated
until the specified number of steps are complete.

\section{Summary}
The power exchange auction market model defined in this chapter provides a
layer of abstraction over the underlying optimal power flow problem and
presents agents with a simple virtual interface for selling
and buying power.  The highly modular simulation framework described allows
different types of learning algorithm, policy function approximator,
exploration technique and task to be used interchangeably. The framework can
simulate competitive electric power trade using any conventional bus-branch
power system model, requiring very little configuration, but providing the
options to adjust a large number of aspects of the simulation with reasonable
ease. The modular nature of the model and its support for easy
configuration is intended to allow learning methods to be compared in this
domain under a number of different metrics.
