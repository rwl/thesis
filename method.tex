\chapter{Modelling Power Trade}
\label{ch:method}
The present chapter defines the model used to simulate electric power trade.
The first section explains how an optimal power flow formulation is used to
represent a generic power exchange auction market and the interface that
participants use.  The second section defines how market
participants are modelled as agents that use reinforcement learning algorithms
to adjust their bidding behaviour and how their interactions with the auction
interface are coordinated in a multi-agent system.

% Societies reliance on secure energy supplies and the high volumes of
% electricity typically consumed render it impractical to experiment with
% radically new approaches to energy trade on real systems.  This section
% explains the approach taken modelling real systems in software such that they
% may be simulated computationally.  The method by which the physical power
% systems, that deliver electricity to consumers, were modeled is given, as well
% as for the mechanisms that facilitate trade and participants that utilise
% these mechanisms.
%
% \subsection{Energy market model}
% Mechanisms for facilitating competitive trade between electricity producers and
% consumers differ greatly in the specifics of their implementations in coutries
% throughout the world.  However, fundamentally they either provide a
% centralised pool through which all electricity is bought and sold or they
% permit producers and suppliers to trade directly.
%
% The UK transmission network is frequently congested[].  The thermal limits of
% transmission lines between particular areas are often reached.  The balancing
% mechanism is the financial instrument used by the system operator to resolve
% constraint issues and energy imbalances.  Should the market not be suitably
% effective in this function the system operator may choose to contract outwith
% the balancing mechanism.  By way of incentive to match demand and avoid
% congestion, imbalance charges are imposed on responsible participants.  There
% is some evidence to suggest that centralised resolution by a system operator
% and socialisation of the incurred costs leads to inefficient despatch of
% generators[Neuhoff].
%
% There are a number of alternative approaches to congestion
% resolution.
% %\cite{neuhoff:power}
%
% \subsection{Transmission capacity rights}
% One approach is to issue contracts for transmission capacity rights or
% equivalent financial rights.  The maximum available transmission capacity
% being auctioned for certain periods of time and firm contracts made entitling
% owners to full compensation upon curtailment or
% withdrawl.
% %\cite{efet:principles}.
%
% The states of Pensylvania, New Jersey and Maryland (PJM) operate a
% non-compulsory power pool with nodal market-clearing prices based on
% competitive bids.  This is complemented by daily and monthly capacity markets
% plus the monthly auction of Financial Transmission Rights to provide a hedging
% mechanism against future congestion charges.
%
% \subsection{Transmission charging}
% Impose delivery charges which increase as network constraints are approached.
%
%
% \subsection{Extended bids/offers}
% Request extended bids and offers which include costs associated with the
% adjustment of participant's desired position.
%
% \subsection{Software agents}
% Participants are modeled in software also.  The nature of a highly distributed
% power system dictates that a very large number of entities may be interacting
% in the marketplace.  Economic studies regularly integrate participant logic
% into the same optimisation problem as the market.  However, this does not
% scale to large numbers of individual participants.  Separating participant
% logic into individual software agents allows their action selection procedures
% to be processed in simultaneously.  The definition of an agent in this context
% emerges from the machine learning technique employed to implement the
% competitive decision making process.

\section{Electricity Market Model}
Computation of the generator dispatch points is achieved using a bespoke
implementation of the optimal power flow formulation from
\matpower \cite[\S5]{pserc:mp_manual}.  Figure \ref{fig:cls_pylon} shows a
class diagram in the Unified Modelling Language (UML) for the power system
model used.  The DC optimal power flow formulation is used in the initial experiments
as it is less computationally expensive.  While the more accurate AC optimal
power flow formulation is required for the more complex experiments that test
agents in their ability to exploit constraints in the power system models.
The trade-offs between using DC models over AC have been examined in
\citeA{overbye:acdc} and DC models have been found reasonable for nodal
marginal price calculation.  Piecewise linear generator cost functions are
modelled using the constrained cost variable method \cite{zimmerman:ccv}.  The
standard extensions to the \matpower optimal power flow formulation that are
not used are:
\begin{itemize}
  \item User-defined cost functions, and
  \item Generator P-Q capability curves.
\end{itemize}

%\input{tikz/cls_pylon}

The standard optimal power flow formulation constrains generator set-points
between upper and lower power limits.  Expensive generators can not be
completely shutdown, even if doing so would result in a lower total system
cost.  Algorithm~\ref{alg:ud} defines the unit de-commitment algorithm from
\citeA[p.57]{pserc:mp_manual} and it is used to compute a least cost commitment
and dispatch by solving repeated optimal power flow problems with different
combinations of generating units that are at their minimum active power limit
deactivated.  The lowest cost solution is returned when no further improvement
can be made and no candidate generators remain. \begin{algorithm}[H]
\caption{Unit de-commitment}
\label{alg:ud}
\begin{algorithmic}[1]
\STATE $\text{initialise}~N \leftarrow 0$
\STATE $\text{solve initial OPF}$
\STATE $L_{tot} \leftarrow \text{total load capacity}$
\WHILE{$\text{total min gen.\ capacity} > L_{tot}$}
	\STATE $N \leftarrow N + 1$
\ENDWHILE

\REPEAT
	\FOR{c in candidates}
		\STATE $\text{solve OPF}$
	\ENDFOR
\UNTIL{$\text{done} = \text{True}$}
\end{algorithmic}
\end{algorithm}

To present agents participating in the market with an interface more
representative of a real power exchange market, an auction clearing
mechanism is implemented \cite[p.92]{pserc:mp_manual}.  The interface
converts offers to sell and bids to buy blocks of power in MW, with prices
specified in \$/MWh, into corresponding generator capacities and piecewise
linear cost functions.  The solution of the unit de-commitment optimal power
flow algorithm is used to clear the offer/bid quantities and prices and the
cleared offers/bids are used to determine revenue values from which each
agent's earnings/losses are derived.

The interface allows maximum offer price limits and minimum bids price limits
to be set.  The clearing process the market begins by withholding offers/bids
outwith these limits, along with those specifying non-positive quantities.
Valid offers/bids for each generator are then sorted into
non-decreasing/non-increasing order and used to form new piecewise-linear cost
functions and adjust the generator's active power limits.
% TODO: Provide an example.

The dispatch points and nodal prices from solving the unit de-commitment
optimal power flow with the newly configured generators as input are used in
to determine the proportion of each offer/bid block that should be
cleared and the associated price for each.  Pricing may be uniform or
discriminatory (pay-as-bid).

\section{Multi-Agent System}
\label{sec:mas}
Participants are modelled as agents that use reinforcement learning algorithms
to adjust their bidding behaviour and their interaction with the auction
interface is coordinated by a multi-agent system.  Agent's do not interact
directly with their environment, but are associated with a particular task
that defines the agent's reward and hence the agent's purpose.  This section
defines the agent's environment, their tasks and the design of connectionist
systems and tables used to represent their action selection policies.  The
process by which the policies are modified by the agent's learning algorithm is
explained and the sequence of interactions within the multi-agent system is
illustrated.

\subsection{Environment}
Each generator/dispatchable load in the power system model (See Figure
\ref{fig:cls_pylon}, above) is associated with an agent\footnote{Management of
a portfolio of generators is also supported by the architecture used, but this
feature is not exploited.} via the agent's environment.  Figure
\ref{fig:cls_pyreto} diagrams the class relationship between an agent's
environment and the generator model along with the association with an instance
of the market with allows submission of offers/bids by the agent. Two main
operations are supported by an agent's environment.

%\input{tikz/cls_pyreto}

For a power system with $n_b$ buses, $n_l$ branches and $n_g$ generators, the
\texttt{getSensors} method returns a $n_s \times 1$ vector of sensor values
$s^i_e$ for generator $i$ where $n_s = 2n_b + 2n_l + 3n_g$.  $s^i_g$ represents
the visible state of the environment for the agent associated with generator
$i$.  $s^i_e$ is composed of sensor values for all buses, branches and
generators.
\begin{equation}
s^i_{e,l} =
\begin{bmatrix}
P_f\\
Q_f\\
P_t\\
Q_t\\
\mu_{S_f}\\
\mu_{S_t}
\end{bmatrix}, \quad
s^i_{e,b} =
\begin{bmatrix}
V_m\\
V_a\\
\lambda_P\\
\lambda_Q\\
\mu_{v_{min}}\\
\mu_{v_{max}}
\end{bmatrix}, \quad
s^i_{e,g} =
\begin{bmatrix}
P_g\\
\mu_{p_{min}}\\
\mu_{p_{max}}\\
\mu_{q_{min}}\\
\mu_{q_{max}}
\end{bmatrix}\ \quad
s^i_e =
\begin{bmatrix}
s^i_{e,b}\\
s^i_{e,b}\\
s^i_{e,g}
\end{bmatrix}
\end{equation}
Not all of the values are used by each agent and they are filtered according
to the agent's task.

The \texttt{performAction} method takes $n_a \times 1$ vector of action values
$a_e$ if $s_{bid} = 0$, otherwise a $2n_a \times 1$ vector.  If $s_{bid} = 0$,
the $i$-th element of $a_e$ is the offered/bid price in \$/MWh, where
$i = 1,2,\dotsc n_{in}$.  If $s_{bid} = 1$, the $j$-th element of $a_e$ is the
offered/bid price in \$/MWh, where $j = 1,3,5,\dotsc n_{in}-1$ and the $k$-th
element of $a_e$ is the offered/bid quantity in MW where $j = 2,4,6,\dotsc
n_{in}$.  The action vector is separated into offers/bids and submitted to the
market.  If $s_{bid} = 0$, then $qty = p_{max}/n_{in}$.

\subsection{Task}
An agent does not interact directly with it's environment, but is associated
with a particular task.  A task associates a purpose with an environment and
defines what constitutes a reward.  Regardless of the learning method employed,
the goal of each participant agent is to make a financial profit and the
rewards are thus defined as the sum of earnings from the previous period $t$
as calculated by the market.

Sensor data from the environment is filtered according to the task being
performed.  Agents using the value-function methods under test have a tabular
representation of their policy with one row per environment state.  Thus,
observations consist of a single integer value $s_v$, where $s_v \leq n_s$ and
$s_v \in \mathbb{Z}^+$.  Agents using the policy-gradient methods under test
have policy functions represented by connectionist systems that use an input
vector $w_i$ of arbitrary length where the $i$-th element $\in \mathbb{R}$.
Before input to the connectionist policy function approximator, sensor values
are scaled to be between $-1$ and $1$. Outputs from the policy are
denormalised using action limits before the action is performed on the
environment.

\subsection{Agent}
Agent $i$ is defined as an entity capable of producing an action $a_i$
based on previous observations of its environment $s_i$, where $a_i$ and $s_i$
are vectors of arbitrary length.  Each agent is associated with a
\textit{module}, a \textit{learner} and a \textit{dataset}. The module
represents the agent's policy for action selection and returns an action
vector $a_m$ when activated with observation $s_t$.  The value-function
methods under test use modules which represent a $N \times M$ table, where $N$
is the total number of states and $M$ is the total number of actions.
\begin{equation}
\begin{bmatrix}
v_{1,1}& v_{1,2}& \dotsb& v_{1,m}\\
v_{2,1}& \ddots& & \vdots\\
\vdots& &\ddots& \vdots\\
v_{n,1}& \dotsb& \dotsb& v_{n,m}
\end{bmatrix}
\end{equation}
Whereas for the policy gradient methods, the module is a connectionist network
of other modules.  The learner can use any
reinforcement learning algorithm and modifies the values/parameters of the
policy module to increase expected future reward.  The dataset stores
state-action-reward tuples for each interaction between the agent and its
environment.  The stored history is used by value-function learners when
computing updates to the policy values.  Policy gradient learners search
directly in the space of the policy network parameters.

% TODO: Agent, module and learner class diagram.

Value-function learners have an association with an explorer module which
returns an explorative action $a_e$ when activated with the current state $s_t$
and action $a_m$ from the policy module.  For example, the $\epsilon$-greedy
explorer has a randomness parameter $\epsilon$ and a decay parameter $d$.  When
the $\epsilon$-greedy explorer is activated, a random number $x_r$ is drawn
where $0 \leq x_r \leq 1$.  If $x_r < \epsilon$ then a random vector of the same
length as $a_e$ is returned, otherwise $a_e = a_m$.

\subsection{Simulation Event Sequence}

%\input{tikz/cls_experiment}

In each simulation of a system consisting of one or more task-agent pairs a
sequence of interactions is coordinated, as illustrated in Figure
\ref{fig:cls_experiment}.

At the beginning of each step/period the market is initialised and all
offers/bids removed.  From each task-agent tuple $(T,A)$ an observation $s_t$
is retrieved from $T$ and integrated into agent $A$.  When an action is
requested from $A$ its module is activated with $s_t$ and the action $a_e$ is
returned.  $a_e$ is performed on the environment of $A$ via its associated task
$T$.  Recall, this process involves the submission of offer/bids to the market.
Figure \ref{fig:seq_action} is a UML sequence diagram that illustrates the
process of performing actions.

Once all actions have been performed the offer/bids are cleared using the
auction mechanism.  Each task $T$ is requested to return a reinforcement reward
$r_t$. All cleared offers/bids associated with the generator in the environment
of $T$ are retrieved from the market and $r_t$ is computed from the difference
between revenue and cost values.
\begin{equation}
r_t = \mbox{revenue} - (c_{fixed} + c_{variable})
\end{equation}
The reward $r_t$ is given to agent $A$ and the value is stored under a new
sample is the dataset, along with the last observation $s_t$ and the last action
performed $a_e$.  Figure \ref{fig:seq_action} is a UML sequence diagram that
illustrates the reward process.

%\input{tikz/sequence}

As illustrated in Figure \ref{fig:seq_learn}, each agent learns from its
actions using $r_t$, at which point the values/parameters of the module of $A$
are updated according to the algorithm of the learner. This constitutes one
step of the simulation and the process is repeated until the specified number
of steps are complete.  Unless agents are reset, the complete history of
states, actions and received rewards is stored in the dataset of each agent.

\section{Summary}
