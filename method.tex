\chapter{Modelling Power Trade}
\label{ch:method}
The present chapter defines the models used to simulate electric power trade.
An electricity market model is defined using an optimal power flow
formulation, unit decommitment algorithm and an auction interface derived from
\cite{zimmerman:mp_pes}.  Market participants are modelled as agents, with
associated reinforcement learning methods, whose interactions with the
auction interface are coordinated using a multi-agent system.

% Societies reliance on secure energy supplies and the high volumes of
% electricity typically consumed render it impractical to experiment with
% radically new approaches to energy trade on real systems.  This section
% explains the approach taken modelling real systems in software such that they
% may be simulated computationally.  The method by which the physical power
% systems, that deliver electricity to consumers, were modeled is given, as well
% as for the mechanisms that facilitate trade and participants that utilise
% these mechanisms.
%
% \subsection{Energy market model}
% Mechanisms for facilitating competitive trade between electricity producers and
% consumers differ greatly in the specifics of their implementations in coutries
% throughout the world.  However, fundamentally they either provide a
% centralised pool through which all electricity is bought and sold or they
% permit producers and suppliers to trade directly.
%
% The UK transmission network is frequently congested[].  The thermal limits of
% transmission lines between particular areas are often reached.  The balancing
% mechanism is the financial instrument used by the system operator to resolve
% constraint issues and energy imbalances.  Should the market not be suitably
% effective in this function the system operator may choose to contract outwith
% the balancing mechanism.  By way of incentive to match demand and avoid
% congestion, imbalance charges are imposed on responsible participants.  There
% is some evidence to suggest that centralised resolution by a system operator
% and socialisation of the incurred costs leads to inefficient despatch of
% generators[Neuhoff].
%
% There are a number of alternative approaches to congestion
% resolution.
% %\cite{neuhoff:power}
%
% \subsection{Transmission capacity rights}
% One approach is to issue contracts for transmission capacity rights or
% equivalent financial rights.  The maximum available transmission capacity
% being auctioned for certain periods of time and firm contracts made entitling
% owners to full compensation upon curtailment or
% withdrawl.
% %\cite{efet:principles}.
%
% The states of Pensylvania, New Jersey and Maryland (PJM) operate a
% non-compulsory power pool with nodal market-clearing prices based on
% competitive bids.  This is complemented by daily and monthly capacity markets
% plus the monthly auction of Financial Transmission Rights to provide a hedging
% mechanism against future congestion charges.
%
% \subsection{Transmission charging}
% Impose delivery charges which increase as network constraints are approached.
%
%
% \subsection{Extended bids/offers}
% Request extended bids and offers which include costs associated with the
% adjustment of participant's desired position.
%
% \subsection{Software agents}
% Participants are modeled in software also.  The nature of a highly distributed
% power system dictates that a very large number of entities may be interacting
% in the marketplace.  Economic studies regularly integrate participant logic
% into the same optimisation problem as the market.  However, this does not
% scale to large numbers of individual participants.  Separating participant
% logic into individual software agents allows their action selection procedures
% to be processed in simultaneously.  The definition of an agent in this context
% emerges from the machine learning technique employed to implement the
% competitive decision making process.

\section{Electricity Market Model}
Computation of the generator dispatch points is executed using parts of the
of the optimal power flow formulation from \textsc{Matpower}.
This section describes parts of the optimal power flow formulation,
unit-decommitment algorithm and auction interface from \textsc{Matpower} that
were used to represent a centralised electricity market.  Notable components
of the full optimal power flow forumlation that have been ignored are
generator P-Q capability curves and dispatchable loads. The power flow
equations associated with a network of these components are subsequently
defined. The constrained cost variable approach to modelling generator cost
functions from \cite{zimmerman:ccv} is introduced, from which the optimal
power flow formulation follows.

% The experiments described in Section \ref{ch:method} require an optimal power
% flow problem to be solved at each step.  To accelerate the simulation process
% for certain experiments the option to use a linearised DC formulation is used,
% the formulation of which is provided also.  The tradeoffs between using DC
% models over AC have been examined in \cite{overbye:acdc} and found reasonable
% for locational marginal price calculations.

Since the optimal power flow formulations do not facilitate shutting down
expensive generators, the unit-decommitment algorithm from MATPOWER is defined.
Finally, to provide an interface to agent participants that resembles that of
real electricity market, MATPOWER's auction wrapper for the optimal power flow
routine is described.


\subsection{Auction Interface}
Solving the optimisation problem defined in section \ref{sec:opf} is
intended to represent the function of a pool market operator.  To present
agents participating in this market with an interface more representative of a
real pool market, an auction clearing mechanism is implemented
\cite[p.31]{pserc:mp_manual}.  The interface formulates optimal power flow
problems from lists of offers to sell and bids to buy blocks of power.

An offer/bid specifies a quantity of power in MW and a price for that power in
\$/MWh, to be traded over a particular period of time.  The market accepts sets
of offers and bids and uses the solution of the unit de-commitment algorithm to
clear the offers and bids as appropriate.  The cleared offers/bids are then be
used to compute values of revenue from which earnings/losses may be determined.

The interface allows maximum offer price limits and minimum bids price limits
to be set.  The clearing process the market begins by withholding offers/bids
outwith these limits, along with those specifying non-positive quantities.
Valid offers/bids for each generator are then sorted into
non-decreasing/non-increasing order and used to form new piecewise-linear cost
functions and adjust the generator's active power limits.
% TODO: Provide an example.

The dispatch points and nodal prices from solving the unit de-commitment
optimal power flow with the newly configured generators as input are used in
to determine the proportion of each offer/bid block that should be
cleared and the associated price for each.  Pricing may be uniform or
discriminatory (pay-as-bid).

% Different pricing options arise from the fact that a gap in which any price is
% acceptable to all participants may exist between the last accepted offer price
% and the last accepted bid price (See Figure X).  This allows, for example,
% the prevention of bids setting the price, even when they are marginal, by
% selecting the \textit{last accepted offer} auction type.

% \begin{figure}
% \label{fig:aution_types}
% \centering
% \begin{tikzpicture}[scale=0.75]
% %    \draw[step=1.0cm,color=gray] (0,0) grid (8,5);
%     \coordinate (y) at (0,5);
%     \coordinate (x) at (8,0);
%     \draw[axis] (y) -- node[left] {Price \$/MWh} (0,0) -- node[below] {Quantity (MW)} (x);
%     \coordinate (offprca) at ($0.8*(y)$);
%     \coordinate (offprcb) at ($0.533*(y)$);
%     \coordinate (offqtya) at ($.6*(x)$);
%     \coordinate (offqtyb) at ($.9*(x)$);
%     \draw[] let \p1=(offprca), \p2=(offqtya) in
%     (\p1) node[left] {$\alpha$} -- (\p2);
% \end{tikzpicture}
% \caption{Acceptable price range}
% \end{figure}

\newpage
\section{Multi-Agent System}
\label{sec:mas}
This section describes the implementation of agents and the coordination of
their interactions in multi-agent systems.  A generic market environment, with
which agents interact regardless of the learning method employed, is defined
along with tasks that associate a purpose with an environment.  The design of
connectionist systems and tables, used to represent agent policies, are given
and the process by which they are modified by the agent's learning algorithm is
explained.  Finally, the collection of agents and tasks into a multi-agent
system and the sequence of interactions is illustrated.

\subsection{Agent, Task \& Environment}

\subsubsection{Environment}
Each generator/dispatchable load in the power system model (See Section
\ref{sec:generators}, above) is associated with an agent\footnote{Management of
a portfolio of generators is also supported by the architecture used, but this
feature has not been exploited.} via the agent's environment.  Each environment
maintains an association with a singular market instance for submission of
offers/bids.  Two main operations are supported by an agent's environment.

For a power system with $n_b$ buses, $n_l$ and $n_g$ generators, the
\texttt{getSensors} method returns a $n_s \times 1$ vector of sensor values
$s^i_e$ for generator $i$ where $n_s = 2n_b + 2n_l + 3n_g$.  $s^i_g$ represents
the visible state of the environment for the agent associated with generator
$i$.  $s^i_e$ is composed of sensor values for all buses, branches and
generators.
\begin{equation}
s^i_{e,l} =
\begin{bmatrix}
P_f\\
Q_f\\
P_t\\
Q_t\\
\mu_{S_f}\\
\mu_{S_t}
\end{bmatrix}, \quad
s^i_{e,b} =
\begin{bmatrix}
V_m\\
V_a\\
\lambda_P\\
\lambda_Q\\
\mu_{v_{min}}\\
\mu_{v_{max}}
\end{bmatrix}, \quad
s^i_{e,g} =
\begin{bmatrix}
P_g\\
\mu_{p_{min}}\\
\mu_{p_{max}}\\
\mu_{q_{min}}\\
\mu_{q_{max}}
\end{bmatrix}\ \quad
s^i_e =
\begin{bmatrix}
s^i_{e,b}\\
s^i_{e,b}\\
s^i_{e,g}
\end{bmatrix}
\end{equation}
Not all values are used by the agent and the filtration is done according to
the agent's task.

The \texttt{performAction} method takes $n_a \times 1$ vector of action values
$a_e$ if $s_{bid} = 0$, otherwise a $2n_a \times 1$ vector.  If $s_{bid} = 0$,
the $i$-th element of $a_e$ is the offered/bid price in \$/MWh, where
$i = 1,2,\dotsc n_{in}$.  If $s_{bid} = 1$, the $j$-th element of $a_e$ is the
offered/bid price in \$/MWh, where $j = 1,3,5,\dotsc n_{in}-1$ and the $k$-th
element of $a_e$ is the offered/bid quantity in MW where $j = 2,4,6,\dotsc
n_{in}$.  The action vector is separated into offers/bids and submitted to the
market.  If $s_{bid} = 0$, then $qty = p_{max}/n_{in}$.

\subsubsection{Task}
An agent does not interact directly with it's environment, but is associated
with a particular task.  A task associates a purpose with an environment and
defines what constitutes a reward.  Regardless of the learning method employed,
the goal of an agent participant is to make a financial profit and the rewards
are thus defined as the sum of earnings from the previous period $t$ as
calcualted by the market.  Sensor data from the environment is filtered
according to the task being performed.  Agents using the value-function methods
under test have a tabular representation of their policy with one row per
environment state.  Thus, observations consist of a single integer value $s_v$,
where $s_v \leq n_s$ and $s_v \in \mathbb{Z}^+$.  Agents using the
policy-gradient methods under test have policy functions represented by
connectionist systems that use an input vector $w_i$ of arbitrary length where
the $i$-th element $\in \mathbb{R}$.  Before input to the connectionist policy
function approximator, sensor values are scaled to be between $-1$ and $1$.
Outputs from the policy are denormalised using action limits before the
action is performed on the environment.

\subsubsection{Agent}
Agent $i$ is defined as an entity capable of producing an action $a_i$
based on previous observations of its environment $s_i$, where $a_i$ and $s_i$
are vectors of arbitrary length.  As illustrated in Figure X, each agent is
associated with a \textit{module}, a \textit{learner} and a \textit{dataset}.
The module represents the agent's policy for action selection and returns an
action vector $a_m$ when activated with observation $s_t$.  The value-function
methods under test use modules which represent a $N \times M$ table, where $N$
is the total number of states and $M$ is the total number of actions.
\begin{equation}
\begin{bmatrix}
v_{1,1}& v_{1,2}& \dotsb& v_{1,m}\\
v_{2,1}& \ddots& & \vdots\\
\vdots& &\ddots& \vdots\\
v_{n,1}& \dotsb& \dotsb& v_{n,m}
\end{bmatrix}
\end{equation}
Whereas for the policy gradient methods, the module is a connectionist network
of other modules as illustrated in Figure X.  The learner can use any
reinforcement learning algorithm and modifies the values/parameters of the
policy module to increase expected future reward.  The dataset stores
state-action-reward tuples for each interaction between the agent and its
environment.  The stored history is used by value-function learners when
computing updates to the policy values.  Policy gradient learners search
directly in the space of the policy network parameters.

Value-function learners have an association with an explorer module which
returns an explorative action $a_e$ when activated with the current state $s_t$
and action $a_m$ from the policy module.  For example, the $\epsilon$-greedy
explorer has a randomness parameter $\epsilon$ and a decay parameter $d$.  When
the $\epsilon$-greedy explorer is activated, a random number $x_r$ is drawn
where $0 \leq x_r \leq 1$.  If $x_r < \epsilon$ then a random vector of the same
length as $a_e$ is returned, otherwise $a_e = a_m$.

\subsection{Simulation Event Sequence}
In each simulation of a system consisting of one or more task-agent pairs a
sequence of interactions is coordinated, as illustrated in Figure X.

At the beginning of each step/period the market is initialised and all
offers/bids removed.  From each task-agent tuple $(T,A)$ an observation $s_t$
is retrieved from $T$ and integrated into agent $A$.  When an action is
requested from $A$ its module is activated with $s_t$ and the action $a_e$ is
returned.  $a_e$ is performed on the environment of $A$ via its associated task
$T$.  Recall, this process involves the submission of offer/bids to the market.
Once all actions have been performed the offer/bids are cleared using the
auction mechanism.  Each task $T$ is requested to return a reinforcement reward
$r_t$. All cleared offers/bids associated with the generator in the environment
of $T$ are retrieved from the market and $r_t$ is computed from the difference
between revenue and cost values.
\begin{equation}
r_t = \mbox{revenue} - (c_{fixed} + c_{variable})
\end{equation}
The reward $r_t$ is given to agent $A$ and the value is stored under a new
sample is the dataset, along with the last observation $s_t$ and the last action
performed $a_e$.  Each agent is instructed to learn from its actions using
$r_t$, at which point the values/parameters of the module of $A$ are updated
according to the algorithm of the learner.

This constitutes one step of the simulation and the process is repeated until
the specified number of steps are complete.  Unless agents are reset, the
complete history of states, actions and received rewards is stored in the
dataset of each agent.
