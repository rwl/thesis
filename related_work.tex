\chapter{Related Work}
\label{ch:related_work}
This chapter describes the research in this thesis in the context of similar
work.  It reviews previously published research with particular focus made on
the learning methods and simulation models used.  For a similar review with
greater emphasis on criticism of simulation results and the conclusions drawn
from them, the interested reader is referred to \citeA{weidlich:08}.
% In the interests of repeatability, the software developed for this thesis has
% been released as open source \cite{lincoln:pyreto}.  The second section in
% this chapter describes the software project in the context of other open source
% Electric Power Engineering programs.

\section{Custom Learning Methods}
The earliest agent-based electricity market simulations in the literature do
not utilise traditional learning methods from Artificial Intelligence, but rely
upon custom heuristic methods.  They are typically formulated using the
author's intuition and represent basic trading rules, but do not encapsulate
many of the key concepts from reinforcement learning theory.

\subsection{Market Power}
Under Professor Derek Bunn, researchers from the London Business School
performed some the first and most reputable agent-based electricity
market simulations.  Their research was initially motivated
by proposals in 1999 to transform the structure of The England and Wales
Electricity Pool, with the aim of combating the perceived generator market
power that was widely believed to be resulting in elevated market prices.

In \citeA{bower:2001} a
detailed model of electricity trading in England and Wales is used to compare
day-ahead and bilateral contract markets under uniform price and
discriminatory settlement.  Twenty generating companies operating in the Pool
during 1998 are modelled as agents endowed with portfolios of generating
plant.  Plant capacities, costs and expected availabilities are synthesised
from public and private data sources and the author's own estimates.  In
simulations of the day-ahead market, each agent submits a single price for the
following simulated trading day, for each item of plant in its portfolio.
Whereas, under the bilateral contract model, 24 bids are submitted for each
generator, corresponding to each hour of the following simulated day.  Revenues
are calculated at the end of each trading day and are determined either by the
bid price of the marginal unit or the generator's own bid price.  Each
generating plant is characterised in part by an estimated target utilisation
rate that represents its desire for forward contract cover.  The agents learn
to achieve this utilisation rate and then improve profitability.

% TODO: Algorithm X defines the rules followed by each agent.
If the utilisation rate is not achieved, a random percentage from a uniform
distribution with a range of $\pm10\%$ and $0\%$ mean is subtracted from the
bid price of all generators in the agent's portfolio.  Agents with more than one generator
transfer successful bidding strategies between plant by setting the
bid price for a generator to the level of the next highest submitted bid price if
the generator sold at a price lower than that of other generators in the same
portfolio.  If an agent's total profit does not increase, a random percentage
from the same distribution as above is added or subtracted from the bid price
from the previous day for each of its generators.  A cap on bid prices is
imposed at \pounds1000 in each period.  Demand follows a 24-hour profile based
on the 1997-1998 peak winter load pattern.  The response of the load schedule
to high prices is modelled as a reduction of 25MW for every \pounds1/MWh that
the system marginal price rises above \pounds75/MWh.

In total, 750 trading days are simulated for each of the four combinations of a
day-ahead market and the bilateral trading model under uniform pricing and discriminatory
settlement.  Prices were found to generally be higher under pay-as-bid pricing
for both market models.  Agents with larger portfolios are shown to have a
significant advantage over smaller generators due to their greater ability to
gather scarce market price information and distribute it among generators.

The research question addressed is a common one in agent-based electricity
market simulation and the paper demonstrates the use of a relatively simple
learning method.  It is an example of how such simulations need not be
restricted to simple models, but can be scaled to study systems at a national
level.

In \citeA{bower:2001b} a more sophisticated custom learning method, resembling
the Roth-Erev method, is applied to a
more detailed model of the New Electricity Trading Arrangements.  The balancing
mechanism is modelled as a one-shot market, that follows the contracts market,
to which increment and decrement bids are submitted.  Active demand side
participation is modelled and generator dynamic constraints are represented by
limiting the number of off/on cycles per day.  Again, transmission constraints
and regional price variations are ignored.

Supplier and generator agents are assigned an optimal value for
exposure to the balancing mechanism that is typically low due to high price and
volume uncertainty.  The agents learn to maximise profit, but profits are
penalised if the objective for balancing mechanism exposure is not
achieved.  They learn policies for pricing markups on the bids submitted
to the power exchange and the increments and decrements submitted to the
balancing mechanism.  Markups in the power exchange are relative to prices
from the previous day and markups on balancing mechanism bids are relative to
power exchange bid prices on the same day.  Different markup
ranges are specified for generators and suppliers in the power exchange and
balancing mechanism and each is partitioned into ten discrete intervals.

As with the Roth-Erev method, a probability for the selection of each markup is
calculated by the learning method.  Daily profits and acceptance rates for
bids/offers from previous trading days are extrapolated out to determine
expected values and thus the expected reward for each markup.  The markups are then
sorted according to expected reward in descending order.  The perceived utility
of each markup $j$ is
\begin{equation}
U_j = \mu \biggl(\frac{\phi - n}{\phi}\biggr)^{i_j-1}
\end{equation}
where $i$ is the index of $j$ in the ordered vector of markups and $\phi$ is a
search parameter.  High values of $\phi$ cause the agent to adopt a more
exploratory markup selection policy.  For all of the experiments $\mu = 1000$,
$\phi = 4$, $n = 3$ and the probability of selecting markup $j$ is
\begin{equation}
Pr_j = \frac{U_j}{\sum_{k=1}^K U_k}
\end{equation}
for $K$ possible markups.

A representative model of the England and Wales system with 24 generator
agents, associated with a total of 80 generating units, and 13 supplier agents
is analysed over 200 simulated trading days.  The authors draw conclusions on
the importance of accurate forecasts, greater risk for suppliers than
generators, the value of flexible plant and the influence of capacity margin on opportunities
for collusive behaviour.
The same learning method is applied in \citeA{bunn:03} as part of an
inquiry by the Competition Commission into whether two specific companies in the
England and Wales electricity market had enough market power to operate against
the public interest.

These papers show that previous research by a respected author was perceived
to be of sufficient value for the approach the be pursued further, but the need
to improve participant and market models was recognised.  Despite neglecting
transmission system constraints, the work is an ambitious attempt to
extrapolate results out to consquences for a national market.

\citeA{visud:99} is another early publication on agent-based simulation of
electricity markets in which a custom learning method is used.  The simulations
comprise only three generators, market power is assumed, and the authors
analyse the mechanisms by which the market power is exercised.  Two bid formats
are modelled.  The single-step supply function (SSF) model requires each
generator agent to submit a price and a quantity, where the quantity is
determined by the generator's marginal cost function.  The linear supply
function (LSF) model requires each generator agent to submit a value
corresponding to the slope of its supply function.  The bid price or slope value for generator
$i$ after simulation period $t$ is
\begin{equation}
x_i(t+1) = x_i(t) + b_i (p_m(t)) u_i(t)
\end{equation}
where $b_i \in \lbrace-1,0,1\rbrace$ is the reward as a function of the market
clearing price $p_m$ from stage $t$ and $u_i$ is a reward gain or attenuation
parameter.  The calculation of $b_i$ is defined according to strategies for
estimated profit maximisation and competition to be the base load generator.
Both elastic and inelastic load models are considered.  Using the SSF
model, the two strategies are compared in a day-ahead market setting,
using a case where there is sufficient capacity to meet demand and a case where
there is excessive capacity to the point where demand can be met by just two of
the generators.  The LSF model is analysed using both day-ahead and
hour-ahead markets with inelastic load.  The hour-ahead simulation is repeated
with elastic demand response.

The number of if-then rules required to define participant strategies in this
paper is demonstrates a drawback of implementing custom learning methods that
is exacerbated when defining multiple strategies.

A similar custom learning method is compared with two other algorithms in
\citeA{visud:thesis}.  The custom method is designed specifically for the power pool model used and employs separate policies for
selecting bid quantities and prices according to several if-then rules that
attempt to capture capacity withholding behaviour.  The method is compared
with algorithms developed in \citeA{auer:03} for application to the $n$-armed
bandit problem \cite[\S2.1]{robbins:53,suttonbarto:1998} and a method based on
evaluative feedback with softmax action selection.

In the algorithms from \citeA{auer:03} each action $i = 1,2,\dotsc K$, for $K$
possible actions, is associated with a weight $w_t(i)$ in simulation period $t
\in T$, for $T$ simulation periods, that is used in
determining the action's probability of selection
\begin{equation}
p_i(t) = (1 - \gamma) \frac{w_i(t)}{\sum_{j=1}^K w_j(t)} + \frac{\gamma}{K}
\end{equation}
where $\gamma$ is a tuning parameter, with $0 < \gamma \leq 1$, that is
initialised such that
\begin{equation}
\gamma = \min \Biggl\lbrace \frac{3}{5}, 2\sqrt{\frac{3}{5} \frac{K\ln K}{T}}
\Biggr\rbrace .
\end{equation}
Using the received reward $x_t(i_t)$, the weight for action $j$ in period
$t+1$ is
\begin{equation}
w_{t+1}(j) = w_t(i) \exp \biggl(\frac{\gamma}{3K} \biggl(\hat{x}_t(i) +
\frac{\alpha}{p_t(i)\sqrt{KT}}\biggr)\biggr)
\end{equation}
where
\begin{equation}
\hat{x}_t(i) =
\begin{cases}
x_t(j)/p_t(i)& \text{if $j=i_t$}\\
0& \text{otherwise}
\end{cases}\end{equation}
and
\begin{equation}
\alpha = 2\sqrt{\ln(KT/\gamma)}.
\end{equation}

In the evaluative feedback method from \citeA[\S2]{suttonbarto:1998} each action
$i$ has a value $Q_t(i)$ in simulation period $t$ equal to the expected average
reward if that action is selected.
% The softmax method uses a Boltzman
% distribution to select actions with probability
% \begin{equation}
% p_t(i) = \frac{e^{Q_t(i)/\tau}}{\sum_{j=1}^K e^{Q_t(j)/\tau}}
% \end{equation}
% where $\tau$ is a \textit{temperature} parameter with $\tau > 0$.
The value of
action $i$ in the $(t+1)^{th}$ period is
\begin{equation}
Q_{t+1}(i) = \begin{cases}
(1-\alpha)Q_t(i) + \alpha r_t(i) & \text{if $i_{t+1}=i$}\\
Q_t(i) & \text{otherwise}
\end{cases}
\end{equation}
where $\alpha$ is a constant \textit{step-size} parameter with $0 < \alpha
\leq 1$.

Extensive simulation results are presented and the choice of learning method is
found to have a significant impact on agent performance, but no quantitative
comparison measure is provided and no conclusions are drawn as to which method
is the superior.

\subsection{Financial Transmission Rights}
In \citeA{ernst:04} a custom learning method is defined and used to study
generator and supplier profits where financial transmission rights are included
in the electricity market.  A two node transmission system is defined with one
lossless transmission line of limited capacity that is endowed to a transmission
operator agent.  Generator agents submit bids for their respective generating
units and the transmission owner submits a bid representing the cost per MW of
transmitting power between the nodes.  The market operator clears the bids,
minimising costs while balancing supply and demand and not breaching the line
capacity.  Prices at each node are calculated to provide a signal to the
agents that captures both energy and transmission costs.

% TODO: Two bus test system.

Each agent selects its bid according to a calculation of the reward that it
would expect to receive if all other agents were to bid as they did in the previous stage.  If multiple bids are found to have
the same value then the least expensive is selected.  In the first period,
previous bids are assumed to be at marginal cost.

Several case studies are
examined with different numbers of generators and line capacities, but few
explicit conclusions are drawn.  Financial transmission rights are an
important issue in electricity markets, but the learning algorithm and
network model are perhaps overly simple for practical conclusions to be drawn.
Agent-based simulation has the potential to provide further insight into
financial transmission rights and the issue is one that perhaps ought to be
revisited as advances in the field are made.

% \section{Agent-Based Simulation}
% Relative to the traditional closed-form equilibrium approaches, agent-based
% simulation of (electricity) markets is a new field of research.  For
% comprehensive reviews and surveys of the many different techniques that have
% been applied in recent years the interested reader is directed to
% \cite{weidlich:08,tesfatsi:handbook,visud:thesis}.  This section will focus on
% reviewing literature from the field in which reinforcement learning techniques
% were applied in combination with explicit power system models.  A short review
% is also provided of some more general applications of reinforcement learning
% with connectionist systems and policy-gradient methods.

% Game theoretic models are commonly associated with economics and attempt to
% capture behaviour in strategic situations mathematically.  They have been
% applied to electric energy problems of many forms, including but not limited
% to analysis of market structure, market liquidity, pricing methodologies,
% regulatory structure, plant positioning and network congestion.  More
% recently, agent-based simulation has received a certain degree of attention
% from researchers and has been applied in some of these fields also.
%
% While popular and seemingly promising, agent-based simulation is still centred
% around abstracted models.  The assumptions made is this abstraction must be
% subjected to the same verification and validation as with equation-based
% models.  Verification of assumptions and model validation are often overlooked
% in agent-based simulations of energy markets, yet they are possibly the most
% important steps in the model building process.  Techniques used to develop,
% debug and maintain large computer programs can often be used to verify that a
% model does what it is intended to do.
%
% Validation of an energy market model is more difficult.  It can be accomplished
% using the intuition of experts or through comparison of simulation results
% with either historical market data or theoretical results from more abstract
% representations of the model.  Finding verifyable trends in existing markets
% is a very large challenge.  To then prove that a computational model
% replicates these characteristics with suitable fidelity is yet more
% challenging still.  Only when a model is suitably verified and validated can
% any conclusions be drawn from results obtained through implementation and
% simulation of suitable scenarios.

\section{Simulations Applying Q-learning}
% Krause et al.\ have published agent-based energy market research in which
% Q-learning methods were applied while considering physical system properties.
% In a comparison between Nash equilibrium analysis and agent-based simulation,
% the suitability of bottom-up modelling for the assessment of market evolution
% was assessed\cite{krause:nash}.  This is built upon in subsequent publications
% which evaluate the influence on market power and social welfare of three
% congestion management schemes and which analyse strategic behavior in combined
% gas and electricity markets\cite{krause:cong,krause:gas}.  Power Transmission
% Distribution Factors (PTDF) are used in place of explicit power flow equations
% in determining line flows.  The action domain of generating agents is limited
% to 0\%, 5\% and 10\% markups on true marginal costs.  The implementation of the
% Q-learning method used does not differentiate between environment states when
% selecting actions. This is a modification to the traditional formulation that
% still results in convincing conclusions, as with the popular Roth-Erev method.
%
% There are similar applications of Q-learning in which states \textit{are}
% defined, but none model the AC transmission system.  A common approach is to
% use categorised market price from the previous period as state
% information\cite{bakirtzis:psce,xiong:discrim}.
Recent agent-based simulations of electricity markets has been carried out
with participant's behavioral aspects modelled using the Q-learning methods
described in Section \ref{sec:qlearning}.

\subsection{Nash Equilibrium Convergence}
The most prominent work in which Q-learning is applied was conducted
at the Swiss Federal Institutes of Technology in Zurich and Lausanne. The
foundations for this work were laid in \citeA{krause:nash04} with a comparison
of agent-based modelling using reinforcement learning and Nash equilibrium
analysis when assessing network constrained power pool market dynamics.
Parameter sensitivity of comparison results were later analysed in
\citeA{krause:nash06}.

The authors model a mandatory spot market which is cleared using a DC
optimal power flow formulation.  A five bus power system model is defined with
three generators and four inelastic and constant loads.  Linear marginal cost
functions
\begin{equation}
C_{g,i}(P_{g,i}) = b_{g,i} + s_{g,i}P_{g,i}
\end{equation}
are defined for each generator $i$ where $P_{g,i}$ is the active power output,
$s_{g,i}$ is the slope of the cost function and $b_{g,i}$ is the intercept.
Suppliers are given the option to markup their bids to the market not by increasing $s_{g,i}$, but increasing $b_{g,i}$ by either 0,
10, 20 or 30\%.
%A price cap of \$60/MW is set, but may not be exceeded by any of the
%available markups.

Nash equilibrium is computed by clearing the market for all possible markup
combinations and determining the actions for which no player is motivated to
deviate from, as it would result in a decrease in expected reward. Experiments
are conducted in which there is a single Nash equilibrium and where there
are two Nash equilibria.

An $\epsilon$-greedy strategy \cite{suttonbarto:1998} is applied for action
selection and a \textit{stateless} action value function is updated at each time step $t$
according to
\begin{equation}
Q(a_t) = Q(a_t) + \alpha(r_{t+1} - Q(a_t))
\end{equation}
where $\alpha$ is the learning rate.  Further to \citeA{krause:nash04},
simulations with discrete sets of values for the parameters $\alpha$ and
$\epsilon$ were carried out in \citeA{krause:nash06}.  While parameter
variations effected the frequency of equilibrium oscillations, Nash equilibrium
was still approached and the oscillatory behaviour observed for almost all
of the combinations.

The significance of this research is that is verifies
that the agent-based approach settles at the same theoretical optimum as with closed-form
equilibrium approaches and that exploratory policies result in the exploitation
of multiple equlibria if they exist.

Convergence to a Nash equilibrium is also shown in \citeA{sistani:06}.
Boltzman (soft-max) exploration is used for action selection with the temperature
parameter adjusted during the simulations.  A modified version of the IEEE 30
bus test system is used with the number of generators reduced from nine to
six.  No optimal power flow formulation or details of the reward signal used
are provided.  Generators are given a three step action space where the slope of a
linear supply function may be less than, equal to or above marginal cost.  The
experimental results show that with temperature parameter adjustment Nash
equilibrium is achieved and the oscillations associated with $\epsilon$-greedy
action selection are avoided.

Dynamic modification of the softmax temperature parameter is a technique that
is employed in several other such publications, but as noted in
\citeA[pp.~1746]{weidlich:08}, the approach taken in this paper conflicts with
the need to balance exploration and exploitation.

\subsection{Congestion Management Techniques}
\label{sec:related_cong}
Having validated the suitability of an agent-based, bottom-up, approach to
assessing the evolution of market characteristics, the authors applied the same
technique to compare congestion management schemes in \citeA{krause:cong}.
The first scheme considered is locational marginal pricing (or nodal
pricing) where congestion is managed by optimising the output of generators
with respect to maximum social welfare.
% Loading branches to their flow
% limits results in non-uniform nodal marginal prices where a nodal marginal
% price is the increase in the total system cost (the value of the objective
% function) when generation at that node is increased by 1MW.  These prices are
% commonly used in electricity market analysis as they may be determined from
% the Lagrangian multipliers on the active power balance constraints in the
% optimal power flow formulation.
The ``market splitting'' scheme they considered is similar to locational
marginal pricing, but the system is subdivided into zones, within which the
nodal prices are uniform.  The final ``flow based market coupling'' scheme
also features uniform zonal pricing, but uses a simplified representation
of the network.  Power flows within the zones are not represented and all lines
between zones are aggregated into one equivalent interconnector.

As an alternative to the conventional DC optimal power flow formulation, line
power flows computation is done using a power transfer distribution
factor (PTDF) matrix.  The $(i,j)^{th}$ element of the PTDF matrix corresponds
to the change in active power flow on line $j$ given an additional injection of
1MW at the slack bus and corresponding withdrawal of 1MW at node $i$
\cite{grainger:psa}.

The congestion management schemes get evaluated under perfect competition,
where suppliers bid at marginal cost, and under oligopolistic competition, in
which markups of 5\% and 10\% can be added to marginal cost.  The benefits
obtained between reward at marginal cost and a maximum markup are used to
assess market power.  The experimental results show that market power
allocations are different under each of the three constraint management
schemes.

This is a compelling example of how optimal power flow can be used with
traditional reinforcement learning methods to address an important research
question.  The decision not to define environment states is unusual for a
Q-learning application and the impact of this deserves investigation.

\subsection{Gas-Electricity Market Integration}
The Q-learning method from \citeA{krause:nash04,krause:nash06} is used to
analyse strategic behaviour in integrated electricity and gas markets in
\citeA{krause:gas}.  Again, power flows are computed using a PTDF matrix.
Pipeline losses in the gas network are approximated using using a cubic
function of flow and three combined gas and electricity models are
compared.

In the first model, operators of gas-fired power plant submit separate bid
functions for gas and electricity.  Bids are then cleared as a single
optimisation problem.  In model two, operators submit one offer for their
capacity to convert gas to electricity.  In the third model, bids are
submitted only to the electricity market, after which gas is purchased
regardless of price.  Gas supply offers are modelled as a linear function with
no strategic involvement.  The models are compared in terms of social welfare,
using a three bus power system model with three non-gas-fired power plants and
one gas-fired plant.

The experimental results show little difference between electricity prices and
social welfare prices between the models.  However, this research illustrates
the interest in and complexity associated with modelling relationships between
multiple markets.  The authors recognise the need for further and more detailed
simulation in order to improve evaluation of market coupling models.

While this work is of a preliminary nature, it is an important step towards
achieving greater understanding the interrelationships between gas and
electricity markets using agent-based simulation.  Further neglect of state
information in the Q-learning method possibly alludes to the difficulty of
creating discrete representations of largely continuous environments.

\subsection{Electricity-Emissions Market Interactions}
Researchers at the Argonne National Laboratory have published results from a
preliminary study of interactions between \textit{emissions} and electricity
markets \cite{wang:09}.  A cap-and-trade system for emissions is modelled where
generator companies are allocated with $\mbox{CO}_2$ allowances that may
subsequently be traded.  Generator companies are assumed to have negligible
influence on market clearing prices in the emissions market and allowance
prices from the European Energy Exchange were used.  In the electricity market,
an oligopoly structure is assumed and bids are cleared using a DC optimal power
flow formulation.

To improve selection of the $\epsilon$ parameter for exploratory
action selection, a simulated annealing (SA) Q-learning method based on the
Metropolis criterion \cite{guo:sa} is used.  Under this method $\epsilon$ is
changed at each simulation step to allow solutions to escape from local
optima.  A two bus system is used to study cases in which allowance trading
is not used, allowances can be exchanged in the emissions market and with
variations in the allowance allocations.  A one year, hourly load profile with
a summer peak is used to model changes in demand.  The electricity market is
cleared for each simulated hour and the emissions market gets cleared at the
end of each simulated week.

The agents learn, when they have a deficit of allowances, to borrow future
allowances in the summer when load and allowance prices are high.  Conversely,
when having a surplus, they learn to sell at this time.  In the third case, the
authors show the sensitivity of profits to initial allocations and conclude
that the experimental results can not be generalised.  The authors cite
further model validation and agent learning method improvements as necessary
future work.

The complexity of the combined electricity and emissions market model
illustrates how the search spaces for learning methods grows dramatically as
models are expanded.  A problem that policy gradient learning methods
seek to address.

\subsection{Tacit Collusion}
The SA-Q-learning method was previously used in \citeA{tellidou:tacit} by
researchers from the University of Thessaloniki to study capacity withholding
and tacit collusion among electricity market participants.  A mandatory spot
market is implemented, where bid quantities may be less than net capacity and
bid prices may be marked up upon marginal cost by increasing the slope of a
linear cost function.  Again the market is cleared using a DC
optimal power flow formulation and locational marginal prices are used to
calculate profits that are used as the reinforcement signal in the learning
process.  Demand is assumed to be inelastic and transmission system parameters
constant between simulation periods.

A simple two node power system model containing two generators is applied in
three test cases. In a reference case, each generator bids full capacity at
marginal cost.  In the second case, generators bid quantities in steps of 10MW
and price markups in steps of \EUR{2}/MWh.  In the third case, the same
generation capacity is split among eight identical generators to increase the
level of competition. The experimental results show that generators learn to
withhold capacity and develop tacit collusion strategies to capture
congestion profits.

This work is similar to earlier research from other institutions and makes only
a minor contribution.  It suggests that there is potential to accelerate
advancement in this field through increased collaboration and sharing of
software source code.

\section{Simulations Applying Roth-Erev}
Roth and Erev's reinforcement learning method (defined in Section
\ref{sec:rotherev}) has received considerable attention from the
agent-based electricity market simulation community.

\subsection{Market Power}
In \citeA{nicolaisen:2001} an agent-based model of a wholesale electricity market
with both supply and demand side participation is constructed.  It is used to
study market power and short-run market efficiency under discriminatory pricing
through systematic variation of concentration and capacity conditions.

To model the power system, each trader is assigned values of available
transmission capability (ATC) with respect to each of the other traders.
Offers from buyers and sellers are matched on a merit order basis, with quantities restricted by
ATC values.  Two issues with the original Roth-Erev method are observed and
the modified version defined in Section \ref{sec:variant} is proposed.

A maximum markup (markdown) of \$40/MWh is specified for each seller (buyer).
Traders are not permitted to make negative profits and the feasible price range
is divided into 30 offer prices for 1000 auction rounds cases and 100 offer
prices for 10000 auction round cases.  The parameters of the Roth-Erev method
are calibrated using direct search within reasonable ranges.  Nine combinations of
buyer and seller numbers and total trading capacities are tested using the
calibrated parameter values and \textit{best-fit} values determined
empirically in \citeA{roth:aer}.

The experimental results show that good market efficiency is achieved under all
configurations and sensitivity to method parameter changes is low.  Levels of
market power are found to be strongly predictive and little difference is found
between cases in which opportunistic price offers are permitted and when traders
are forced to bid at marginal cost.  The results are compared with those from
\citeA{nicolaisen:2000}, in which genetic algorithms are used.  The authors
conclude that the reinforcement learning approach leads to higher market
efficiency due their adaption according to \textit{individual} profits.

Genetic algorithms were a popular alternative to reinforcement learning methods
in early agent-based electricity market research.  This paper compares the two
and illustrates some of the reasons that they have now been largely abandoned
in this field. The modified Roth-Erev method proposed in this paper is later
used in several other publications.

Further research from Iowa State University, involving the modified Roth-Erev
method, has used the AMES wholesale electricity market test bed.  A
detailed description of AMES is provided in Appendix \ref{sec:ames} below.  In
\citeA{tesfatsi:psce} it is used to investigate strategic capacity
withholding in a wholesale electricity market design proposed by the U.S.
Federal Energy Regulatory Commission in April 2003.  A five bus power system model
with five generators and three dispatchable loads is defined and capacity
withholding is represented by permitting traders to bid lower than true
operating capacity and higher than true marginal costs.

Comparing results from a benchmark case (in which true production costs are
reported, but higher than marginal cost functions may be reported) and cases in
which reported production limits may be less than the true values, the authors
find that with sufficient capacity reserve there is no evidence to suggest
potential for inducing higher net earnings through capacity withholding in the
market design.

AMES was the first agent-based electricity market simulation program to be
released as open source, but while there are serveral publications on the
project, papers involving its application are scarce.  This shows how
niche the field is and the challenge faced if such projects are to benefits
from the collaboration of communities that often leads to successful open
source software projects.

\subsection{Italian Wholesale Electricity Market}

\ifthenelse{\boolean{includefigures}}{\input{tikz/italy}}{}

\citeA{cincotti:09} from the University of Genoa used the modified
Roth-Erev method to study strategic behaviour in the Italian wholesale electricity market.  An accurate model of the actual clearing procedure is
implemented and the model of the Italian transmission system, including an
interconnector to Sicily and zonal subdivision, illustrated in Figure
\ref{fig:italy} is defined.  Within each of the 11 zones, thermal plant is
combined according to technology (coal, oil, combined cycle gas, turbo gas and
repower) and associated with one of 16 generation companies according to the
size of the companies share.  The resulting 53 agents are assumed to bid full
capacity and may markup bid prices in steps of 5\%, with a maximum markup of
300\%.

% TODO: One-line diagram of Italian grid model.

Bids are cleared using a DC optimal power flow formulation with
generation capacity constraints and zone interconnector flow limits.  Agents are
rewarded according to a uniform national price, computed as a weighted average
of zonal prices with respect to zonal load.  Using real hourly load data it
is shown that in experiments in which agents learn their optimal
strategy, historical trends can be replicated in all but certain hours of
peak load.  The authors state a desire to test different learning methods and perform
further empirical validation.

\subsection{Vertically Related Firms and Crossholding}
In \citeA{micola:08} a multi-tier model of wholesale natural gas, wholesale
electricity and retail electricity markets is studied using another variant of
the Roth-Erev method.  Coordination between strategic business units (SBU)
within the same firm, but participating in different markets, is varied
systematically and profit differences are analysed.

A two-tier model involves firms with two associated agents
whose rewards $r_1$ and $r_2$ are initially independent.  A ``reward
independence'' parameter $\alpha$ is used to control the fraction of profit
from one market that is used in rewarding the agent in the other market.
The total rewards are
\begin{equation}
R_1(t) = (1-\alpha)r_1(t) + \alpha r_2(t)
\end{equation}
and
\begin{equation}
R_2(t) = (1-\alpha)r_2(t) + \alpha r_1(t).
\end{equation}
Each action $a$ is a single price bid between zero and the clearing price from
the preceding market.  The Roth-Erev method is modified such that similar
actions, $a-1$ and $a+1$, are reinforced also.  For each agent $i$, the action
selection propensities in auction round $t$ are
\begin{equation}
p^i_a(t) = \begin{cases}
(1-\gamma)p^i_a(t-1) + R_i(t)& \text{if $s=k$}\\
(1-\gamma)p^i_a(t-1) + (1-\delta)R_i(t)& \text{if $s=k-1$ or $s=k+1$}\\
(1-\gamma)p^i_a(t-1)& \text{if $s\neq k-1$, $s\neq k$ or $s\neq k+1$}
\end{cases}
\end{equation}
where $\delta$, with $0\leq \delta \leq 1$, is the local experimentation
parameter, $\gamma$ is the discount parameter and $i\in \lbrace 1,2 \rbrace$.
Actions whose probability of selection fall below a specified value are
removed from the action space.

The initial simulation consists of two wholesalers and three retailers and
$\alpha$ is varied from $0$ to $0.5$ in $51$ discrete steps.  The experiment
is repeated using a three tier model in which two natural gas shippers supply
three electricity generators who, in turn, sell to four electricity retailers.
The results show a rise in market prices as reward interdependence is
increased and greater profits for integrated firms.

The same alternative formulation of the Roth-Erev method is also used in
\citeA{micola:08b} to analyse the effect on market prices of different degrees
of producer crossholding\footnote{Crossholdings occur when one publicly
traded firm owns stock in another such firm.} under private and public bidding
information.  Crossholding is represented with the introduction of a factor
to each agent's reward function that controls the fraction of profit from
the crossowned rival that the agent receives.  Public information availability
is modelled using a vector of probabilities for selection of each
possible action that is the average of each agent's private probability and is
available to all agents.

The degree to which the public probabilities
influence the agent's action selection probability from equation (\ref{eq:re_prob}) is varied
systematically in a series of experiments, along with crossholding levels and
buyer numbers.  The results are illustrated using three-dimensional
plots and show a direct relationship between crossholding and market price.
The conclusions drawn on market concentration by the authors are dependant upon
the ability to model both the demand and supply side participation in the
market and the authors state that this shows, to a certain extent, the value
of the agent-based simulation approach.

\subsection{Two-Settlement Markets}
In \citeA{weidlich:06} the modified Roth-Erev method is used to study
interrelationships between contracts markets and balancing markets.  Bids on the
day-ahead contracts market consist of a price and a volume, which are assumed to
be the same for each hour of the day.  Demand is assumed to be fixed and
inelastic.  Bids on the balancing market consist of a reserve price, a
\textit{work} price and an offered quantity.  The reserve price is that which
must be paid for the quantity to be kept on standby and the work price must be
paid if that quantity is called upon for transmission system stabilisation. No
optimal power flow formulation or power system model is defined.

At the day-ahead stage, contract market and balancing market (according to
reserve price) bids are cleared by stacking in order of ascending price until the
forecast demand is met.  On the following day, accepted balancing bids are
cleared according to work price such that requirements for reserve dispatch
are met.

Bid prices on the contracts market are stratified into 21 discrete
values between 0 and 100 and bid quantities into six discrete values between 0
and maximum capacity, giving 126 possible actions.  Bid quantities on the
balancing market equal the capacity remaining after contract market
participation.  21 discrete capacity prices between 0 and 500 and 5 work prices
between 0 and 100 are permitted, giving 105 possible actions in the balancing
market.  Separate instances of the modified Roth-Erev method are used to learn
bidding strategies for each agent in each of the markets.

Interrelationships between the markets are studied using four scenarios in
which the order of market execution and the balancing market pricing mechanism
(discriminatory or pay-as-bid) are changed.  Clearing prices in the market
executed first are shown to have a marked effect on prices in the
following market.  The authors find agent-based simulation to be a suitable
tool for reproducing realistic market outcomes and recognise a need for more
detailed models with larger action domains.

In the same year, the authors collaborated with Jian Yao and Shmuel Oren from
the University of California to study the dynamics between two settlement
markets using the modified Roth-Erev method \cite{viet:06}.  The markets are a
forward contracts market, in which transmission constraints are ignored, and a spot
market that is cleared using a DC optimal power flow formulation with line
flows calculated using a PTDF matrix.  The authors state that suppliers utility
functions are to include aspects of risk aversion in future work.  The use of
some measure of risk adjusted return to assess performance is commonplace in
economics research, but is currently lacking from the agent-based electicity
market simulation literature.

Zonal prices are set in the forward market as weighted averages of nodal prices
with respect to historical load shares.  Profits are determined using the
zonal prices and nodal prices from optimisation of the spot market.  Demand is assumed
inelastic to price, but different contingency states with peak and low demand
levels are examined.  A stylised 53 bus model of the Belgian electricity
system from \citeA{yao:07} and \citeA{yao:08} is used to validate the results
against those obtained using equilibrium methods.  The nineteen generators are divided among
two firms which learn strategies for bid price and quantity selection using the
modified Roth-Erev method with a set of fixed parameter values taken from
\citeA{roth:aer}.  The results show that the presence of a forward contracts
market produces lower overall electricity prices and lower price volatility.
The authors note that risk aversion is to be included in suppliers utility
functions in future work.

\section{Policy Gradient Reinforcement Learning}
Policy gradient reinforcement learning methods, defined in Section
\ref{sec:policygradient}, have been successfully applied in both laboratory and
operational settings \cite{barto:policy,shaal:robots,peshkin:routing}.  This
section reviews the \textit{market} related applications of these methods.

\subsection{Financial Decision Making}
\label{sec:moody}
Conventionally, \textit{supervised} learning techniques are used in financial
decision making problems to minimise errors in price forecasts and are trained
on sample data.  In \citeA{moody:98} a recurrent reinforcement learning
method is used to optimise investment performance without price forecasting.
The method is ``recurrent'' in that it uses information from past decisions as
input to the decision process.  The authors compare direct profit and the
Sharpe ratio \cite{sharpe:ratio66,sharpe:ratio94} as reward signals. The Sharpe ratio is a
measure of risk adjusted return defined as
\begin{equation}
S_t = \frac{\mbox{Average}(r_t)}{\mbox{Standard Deviation}(r_t)}
\end{equation}
where $r_t$ is the return for period $t$.
% The parameters of the trading system $\theta$ are adjusted in the direction of
% the gradient $\partial S_t / \partial \theta_t$.

The parameters $\theta$ of the trading system are updated in the direction of
the steepest accent of the gradient of some performance function $U_t$ with
respect to~$\theta$
\begin{equation}
\Delta\theta_t = \rho \frac{dU_t(\theta_t)}{d\theta_t}
\end{equation}
where $\rho$ is the learning rate.  Direct profit is the simplest performance
function defined, but assumes traders are insensitive to risk.  Investors
being sensitive to losses are, in general, willing to sacrifice potential gains
for reduced risk of loss. To allow on-line learning and parameter updates at
each time period, the authors define a \textit{differential} Sharpe ratio.  By maintaining an
exponential moving average of the Sharpe ratio, the need to compute return
averages and standard deviations for the entire trading history at each
simulation period is avoided.  Alternative performance ratios, including the
Information ratio, Appraisal ratio and Sterling ratio, are also mentioned.

Simulations are conducted using artificial price data, equivalent to one year
of hourly trade in a 24-hour market, and using 45 years of monthly data from
the Standard \& Poor (S\&P) 500 stock index and 3 month Treasury Bill (T-Bill)
data. In a portfolio management simulation, in which trading systems invest
portions of their wealth among three different securities, it was shown
that trading systems maximising the differential Sharpe ratio, produced more
consistent results and achieved higher risk adjusted returns than those
trained to simply maximise profit.  This result is important as the majority
of reinforcement learning applications in electricity market simulation use
direct profit for the reward signal and may benefit from using measures of risk
adjusted return.

In \citeA{moody:direct} the recurrent reinforcement learning method from
\citeA{moody:98} is contrasted with value function based methods.  In addition
to the Sharpe ratio, a Downside Deviation ratio is defined.  Results from trading
systems trained on half-hourly United States Dollar-Great British Pound foreign
exchange rate data and, again, learning switching strategies between the S\&P
500 index and T-Bills are presented.  They show that the recurrent
reinforcement learning method outperforms Q-learning in the S\&P
500/T-Bill allocation problem.  The authors observe also that the recurrent
reinforcement learning method has a much simpler functional form, that the
output, not being discrete, maps easily to real valued actions and that the
algorithm is more robust to noise in the financial data and adapts quickly to
non-stationary environments.

\subsection{Grid Computing}
In \citeA{vengerov:grid} a marketplace for computational resources in
envisioned.  The authors propose a market in which grid service suppliers offer
to execute jobs submitted by customers for a price per CPU-hour.  The problem
formulation requires customers to request a quote for computing a job $k$ for a
time $\tau_k$ on $n_k$ CPUs.  The quote returned specifies a price $P_k$ at
which $k$ would be charged and a delay time $d_k$ for the job.  The service
provider's goal is to learn a policy for pricing quotes that maximises long
term revenue when competing in a market with other providers.  Price
differentiation is implemented though provision of a standard service, priced
at \$1/CPU-hour and a premium service a \$$P$/CPU-hour, with premium jobs
prioritised over standard jobs.  The state of the market environment is
defined by the current expected delays in the standard and premium service
classes and by $n_k \tau_k$: the product of the number of CPUs requested and
the job execution time.  The reward $r(s,a)$ for action $a$ in state $s$ is the
total price paid for the job.  The policy gradient method employed is a
modified version of Williams' REINFORCE \cite{williams:reinforce} where
% TODO: Validate against Chapter 2.
\begin{equation}
Q(s_t,a_t) = \sum_{t=1}^T r(s_t,a_t) - \overline{r}_t
\end{equation}
and $\overline{r}_t$ is the current average reward.

The authors recognise that their grid market model could be generalised to
other multi-seller retail markets.  The experimental results show that if
all grid service providers simultaneously use the learning algorithm then the
process converges to a Nash equilibrium.  The results also showed that
significant increases in profit were possible by offering both standard and
premium services.

While this work applies policy gradient methods in a different domain, it shows
how these methods can be used to set prices in a market and the author
recognises the potential for the approach to be extended to other domains.

% \subsection{Learning Method Comparisons}
% \subsection{Heuristic Approaches}
% \section{Simulations Applying Genetic Algorithms}
% \section{Learning Classifier Systems}

% \section{Closed-Form Equilibrium}
% Hobbs, Neuhoff

\section{Summary}
Agent-based simulation of electricity markets has been a consistently active
field of research for more than a decade.  Researchers around the world have
sought to tackle important Electric Power Engineering problems including:
\begin{itemize}
  \item Market power,
  \item Congestion management,
  \item Tacit collusions,
  \item Discriminatory vs.~pay-as-bid pricing,
  \item Financial transmission rights, and
  \item Day ahead markets vs.~bilateral trade.
\end{itemize}
Improvements in these areas have the potential to provide large financial
benefits to societies.

There is a trend in the literature towards the use of more complex learning
methods for participant behavioural representation and increasingly accurate
electric power system models.  Some of the more ambitious studies have used
stylised models of national transmission systems for countires including the
UK, Italy, Belgium and Germany.  There have been previous attempts to compare
learning methods for simulated electricity trade, but no concensus exisits as
to which are most appropriate for particular applications.

Actions spaces are growing as researchers extend their studies to investigate
energy business structures and the relationships between electricity, fuel and
emission allowance markets.  It seems that policy gradient reinforcement
learning methods have not been previously used in electricity market
simulation, but have been shown to work well in similar problems.
