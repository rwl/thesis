\chapter{Related Work}
\label{ch:related_work}
This thesis concerns methodological advancement in the field of agent-based
electricity market simulation.  This chapter describes the research in the
context of similar work with particular emphasis on the methods employed.  The
bulk of the review focuses upon agent-based simulation with decision models
based upon reinforcment learning.  However, simulations applying alternative
methods, such as genetic algorithms and learning classifier systems, are also
surveyed.  In the interests of repeatability, the software developed for
this thesis has been released as open source and the project is described in
the context of other open source electric power engineering tools.

\section{Policy Gradient Reinforcement Learning}
Direct policy search reinforcement learning methods have been sucessfully
applied to financial decision making problems.  It is more common for
supervised learning techniques to be trained on sample data and used to
minimise errors in price forecasts.  However, in \cite{moody:98} a recurrent
reinforcement learning method is used to optimise investment performance
without forecasting prices.  The method is recurrent as it uses information
related to past decisions as input.  The authors compare using direct profit
and the Sharpe ratio \cite{sharpe:ratio66,sharpe:ratio94} as a reward signal.
The Sharpe ratio is a measure of isk adjusted return defined as
\begin{equation}
S_t = \frac{\mbox{Average}(R_t)}{\mbox{Standard Deviation}(R_t)}
\end{equation}
where $R_t$ is the return for period $t$.
% The parameters of the trading system $\theta$ are adjusted in the direction of
% the gradient $\partial S_t / \partial \theta_t$.

The trading system parameters $\theta$ are updated in the direction of the
steepest accent of the gradient of some performance function $U_t$ with repect
to $\theta$
\begin{equation}
\Delta\theta_t = \rho \frac{dU_t(\theta_t)}{d\theta_t}
\end{equation}
where $\rho$ is the learning rate.  Direct profit is the simplest performance
function defined, but assumes traders are insensitive to risk.  Investors
being, in general, sensitive to losses are willing to sacrifice potantial gains
for reduced risk of loss.
To allow on-line learning and parameter updates at each time period, the
authors define a \textit{differential} Sharpe ratio.  By maintaining an
exponential moving average of the Sharpe ratio, the need to compute return
averages and standard deviations for the entire trading history at each period
is avoided.  Alternative performance ratios including the \textit{information
ratio}, \textit{appraisal ratio} and \textit{Sterling ratio} are mentioned.

Simulations are conducted using artificial price data, equivalent to one year
of hourly trade in a 24-hour market, and 45 years of monthly data from the
Standard \& Poor (S\&P) 500 index and 3 month Treasury Bill (T-Bill) data. In a
portfolio management simulation, in which trading systems invested proportions
of their wealth among three different securities, it was shown that trading
systems maximising the differential Sharpe ratio produced more consistent
results and achieved higher risk adjusted returns than those trained to simply
maximise profit.  This result is interesting as the majority of applications of
reinforcement learning to electricity market simulation use profit as a reward
signal and may benefit from using measures of risk adjusted return.

In \cite{moody:direct} the recurrent reinforcement learning method from
\cite{moody:98} is contrasted with value function based approaches.  In
addition to the Sharpe ratio, the \textit{Downside Deviation} ratio is
described and may also be of use in electricity market simulation.  Simulation
results from trading systems trained on half-hourly USD/GBP foreign exchange
rate data and again learning switching strategies between the S\&P 500 stock
index and T-Bills are presented.  The results show that the recurrent
reinforcement learning method outperforms the Q-learning in the S\&P 500/T-Bill
allocation problem.  The authors also observe that the recurrent reinforcement
learning method has a simpler functional form, the output is not discrete and
easily maps to real valued actions and that the algorithm is more robust to
noise in financial data and adapts quickly to non-stationary environments.

In \cite{vengerov:grid} a marketplace for computational resources in
envisioned.  The authors propse a market in which grid service suppliers offer
to execute jobs submitted by customers for a price per CPU-hour.  The problem
formulation requires customers to request a quote for computing a job $k$ for a
time $\tau_k$ on $n_k$ CPUs.  The quote returned specifies a price $P_k$ at
which the $k$ would be charged and a delay time $d_k$ for the job.  The service
provider's goal is to learn a policy for pricing quotes that maximises long
term revenue when competing in a market environment with other providers.  A
differentiated pricing model is implemented where a standard service is priced
at 1~\$/CPU-hour and a premium sercice a $P$~\$/CPU-hour, with premium jobs
prioratised over standard jobs.  The state of the market environment is defined
by the current expected delays in the standard and premium service classes and
the product of the number of CPUs requested and the job execution time,
$n_k \tau_k$.  The reward $r(s,a)$ for action $a$ in state $s$ is the total
price paid for the job.  The policy gradient method employed is a modified
version of Williams' REINFORCE where
\begin{equation}
Q(s_t,a_t) = \sum_{t=1}^T r(s_t,a_t) - \overline{r}_t
\end{equation}
and $\overline{r}_t$ is the current average reward.

The authors recognise that their grid market model could be applied to any
multi-seller retail market environment.  The experimental results show that if
all grid service providers simultaneously use the learning algorithm then the
process converges to a Nash equilibrium.  The results also showed that
significant increases in profit were possible by offering both standard and
premium services.

% \section{Agent-Based Simulation}
% Relative to the traditional closed-form equilibrium approaches, agent-based
% simulation of (electricity) markets is a new field of research.  For
% comprehensive reviews and surveys of the many different techniques that have
% been applied in recent years the interested reader is directed to
% \cite{anke:2008,tesfatsi:handbook,visud:thesis}.  This section will focus on
% reviewing literature from the field in which reinforcement learning techniques
% were applied in combination with explicit power system models.  A short review
% is also provided of some more general applications of reinforcement learning
% with connectionist systems and policy-gradient methods.

% Game theoretic models are commonly associated with economics and attempt to
% capture behaviour in strategic situations mathematically.  They have been
% applied to electric energy problems of many forms, including but not limited
% to analysis of market structure, market liquidity, pricing methodologies,
% regulatory structure, plant positioning and network congestion.  More
% recently, agent-based simulation has received a certain degree of attention
% from researchers and has been applied in some of these fields also.
%
% While popular and seemingly promising, agent-based simulation is still centred
% around abstracted models.  The assumptions made is this abstraction must be
% subjected to the same verification and validation as with equation-based
% models.  Verification of assumptions and model validation are often overlooked
% in agent-based simulations of energy markets, yet they are possibly the most
% important steps in the model building process.  Techniques used to develop,
% debug and maintain large computer programs can often be used to verify that a
% model does what it is intended to do.
%
% Validation of an energy market model is more difficult.  It can be accomplished
% using the intuition of experts or through comparison of simulation results
% with either historical market data or theoretical results from more abstract
% representations of the model.  Finding verifyable trends in existing markets
% is a very large challenge.  To then prove that a computational model
% replicates these characteristics with suitable fidelity is yet more
% challenging still.  Only when a model is suitably verified and validated can
% any conclusions be drawn from results obtained through implementation and
% simulation of suitable scenarios.

\section{Simulations Applying Q-learning}
% Krause et al.\ have published agent-based energy market research in which
% Q-learning methods were applied while considering physical system properties.
% In a comparison between Nash equilibrium analysis and agent-based simulation,
% the suitability of bottom-up modelling for the assessment of market evolution
% was assessed\cite{krause:nash}.  This is built upon in subsequent publications
% which evaluate the influence on market power and social welfare of three
% congestion management schemes and which analyse strategic behavior in combined
% gas and electricity markets\cite{krause:cong,krause:gas}.  Power Transmission
% Distribution Factors (PTDF) are used in place of explicit power flow equations
% in determining line flows.  The action domain of generating agents is limited
% to 0\%, 5\% and 10\% markups on true marginal costs.  The implementation of the
% Q-learning method used does not differentiate between environment states when
% selecting actions. This is a modification to the traditional formulation that
% still results in convincing conclusions, as with the popular Roth-Erev method.
%
% There are similar applications of Q-learning in which states \textit{are}
% defined, but none model the AC transmission system.  A common approach is to
% use categorised market price from the previous period as state
% information\cite{bakirtzis:psce,xiong:discrim}.
Agent-based simulation of electricity markets has been researched with
participants behavioral aspects modelled using Q-learning methods.  The most
prominent work in which this method has been is adopted has been conducted at
the Swiss Federal Institutes of Technology in Zurich and Lausanne.  The
foundations for this were laid in \cite{krause:nash04} with a
comparison of agent-based modelling using reinforcement learning with Nash
equilibrium analysis in assessing network constrained power pool market
dynamics.  Parameter sensistivity of comparison results were later analysed in
\cite{krause:nash06}.

In the papers a mandatory spot market is modelled and cleared using a DC
optimal power flow formulation.  A five bus power system model is defined with
three generators and four inelastic and constant loads.  Linear marginal cost
functions
\begin{equation}
C_{g,i}(P_{g,i}) = b_{g,i} + s_{g,i}P_{g,i}
\end{equation}
are assumed for each generator $i$ where $P_{g,i}$ is the active power output,
$s_{g,i}$ is the slope of the cost function and $b_{g,i}$ is the cost when
$P_{g,i} = 0$.  Suppliers are given the option to markup their bids to the
market not by increasing the slope, but by increasing $b_{g,i}$ by 10\%, 20\%
or 30\%.  A price cap of \$60/MW is set, but may not be exceeded by any of the
available markups.

Nash equilibrium of the market is computed by clearing the market for all
possible markup combinations and determining the actions for which no player is
motivated to deviate from as it would result in a decrease in expected reward.
Experiments are conducted in which there is a single Nash equilibrium and two
equilibria.

An $\epsilon$-greedy strategy is applied for action selection and a stateless
action value function is updated at each time step $t$ according to
\begin{equation}
Q(a_t) \leftarrow Q(a_t) + \alpha(r_{t+1} - Q(a_t))
\end{equation}
where $\alpha$ is the learning rate.  Further to \cite{krause:nash04},
simulations with discrete sets of values for the parameters $\alpha$ and
$\epsilon$ were carried out in \cite{krause:nash06}.  While parameter
variations affected the frequency of equilibrium oscillations, Nash equilibrium
was still approached and the oscillatory behaviour observed for almost all
combinations.

The significance of this research is that is verifies that the agent-based
approach settles at the same theoretical optimum as with closed-form equilibrium
approaches and that exploratory policies result in the exploitation of multiple
equlibria if they exist.

\section{Simulations Applying Roth-Erev}
The AMES (Agent-based Modeling of Electricity Systems) power market test bed is
a software package that models core features of the Wholesale Power Market
Platform (WPMP) -- a market design proposed by the US Federal Energy Regulatory
Commission (FERC) in April 2003 for common adoption in regions of the
US\cite{tesfatsi:wpmp}. The design features:
\begin{itemize}
  \item a centralised structure managed by an independent market operator,
  \item parallel day-ahead and real-time markets and
  \item locational marginal pricing.
\end{itemize}
Learning agents may represent load serving entities or generating companies and
learn using implementations of the Roth-Erev method (See sections
\ref{sec:rotherev} and \ref{sec:variant}, above) built on the Repast agent
simulation toolkit\cite{gieseler:thesis}.  The permissive license under which
the source code for these algorithms has been released allowed direct
translation of them for use in this study.  Agents learn from the solutions of
hourly bid/offer based DC-OPF problems formulated as quadratic programs and
solved using QuadProgJ\cite{tesfatsi:dcopf}.

The ability of generator agents to learn particular supply offers has been
demonstrated along with the plotting and data handling capabilities of AMES
using a 5-bus network model\cite{tesfatsi:pes09}.  The same network has been
used to investigate strategic capacity withholding in FERC wholesale power
market design\cite{tesfasi:psce}.  Generator agents report linear marginal cost
functions to the market operator and supply functions are formed through linear
interpolation between the prices at minimum and maximum production limits.
Load serving agents submit combinations of fixed demand bids and
price-sensitive bid functions, the ratio between which is varied between 0.0 and
1.0 to test physical and economic capacity withholding potential.  Comparing
results from a benchmark case (in which true production costs are reported,
but higher than marginal cost functions may be reported) and cases in which
reported production limits may be less than the true values, the authors find,
that with sufficient capacity reserve, no evidence to suggest potential for
inducing higher net earnings through capacity withholding in the WPMP.


% \subsection{Learning Method Comparisons}
% \subsection{Heuristic Approaches}
\section{Simulations Applying Genetic Algorithms}
\section{Learning Classifier Systems}

% \section{Closed-Form Equilibrium}
% Hobbs, Neuhoff

% \section{Policy-Gradient Reinforcement Learning}
% TD-Gammon

\section{Open Source Power Engineering Software}
% MATPOWER, OpenDSS
% Feature matrix
