\chapter{Related Work}
\label{ch:related_work}
This thesis concerns methodological advancement in the field of agent-based
electricity market simulation.  This chapter describes the research in the
context of similar work with particular emphasis on the methods employed.  The
bulk of the review focuses upon agent-based simulation with decision models
based upon reinforcment learning.  However, simulations applying alternative
methods, such as genetic algorithms and learning classifier systems, are also
surveyed.  In the interests of repeatability, the software developed for
this thesis has been released as open source and the project is described in
the context of other open source electric power engineering tools.

\section{Policy Gradient Reinforcement Learning}
Direct policy search reinforcement learning methods have been sucessfully
applied to financial decision making problems.  It is more common for
supervised learning techniques to be trained on sample data and used to
minimise errors in price forecasts.  However, in \cite{moody:98} a recurrent
reinforcement learning method is used to optimise investment performance
without forecasting prices.  The method is recurrent as it uses information
related to past decisions as input.  The authors compare using direct profit
and the Sharpe ratio \cite{sharpe:ratio66,sharpe:ratio94} as a reward signal.
The Sharpe ratio is a measure of isk adjusted return defined as
\begin{equation}
S_t = \frac{\mbox{Average}(R_t)}{\mbox{Standard Deviation}(R_t)}
\end{equation}
where $R_t$ is the return for period $t$.
% The parameters of the trading system $\theta$ are adjusted in the direction of
% the gradient $\partial S_t / \partial \theta_t$.

The trading system parameters $\theta$ are updated in the direction of the
steepest accent of the gradient of some performance function $U_t$ with repect
to $\theta$
\begin{equation}
\Delta\theta_t = \rho \frac{dU_t(\theta_t)}{d\theta_t}
\end{equation}
where $\rho$ is the learning rate.  Direct profit is the simplest performance
function defined, but assumes traders are insensitive to risk.  Investors
being, in general, sensitive to losses are willing to sacrifice potantial gains
for reduced risk of loss.
To allow on-line learning and parameter updates at each time period, the
authors define a \textit{differential} Sharpe ratio.  By maintaining an
exponential moving average of the Sharpe ratio, the need to compute return
averages and standard deviations for the entire trading history at each period
is avoided.  Alternative performance ratios including the \textit{information
ratio}, \textit{appraisal ratio} and \textit{Sterling ratio} are mentioned.

Simulations are conducted using artificial price data, equivalent to one year
of hourly trade in a 24-hour market, and 45 years of monthly data from the
Standard \& Poor (S\&P) 500 index and 3 month Treasury Bill (T-Bill) data. In a
portfolio management simulation, in which trading systems invested proportions
of their wealth among three different securities, it was shown that trading
systems maximising the differential Sharpe ratio produced more consistent
results and achieved higher risk adjusted returns than those trained to simply
maximise profit.  This result is interesting as the majority of applications of
reinforcement learning to electricity market simulation use profit as a reward
signal and may benefit from using measures of risk adjusted return.

In \cite{moody:direct} the recurrent reinforcement learning method from
\cite{moody:98} is contrasted with value function based approaches.  In
addition to the Sharpe ratio, the \textit{Downside Deviation} ratio is
described and may also be of use in electricity market simulation.  Simulation
results from trading systems trained on half-hourly USD/GBP foreign exchange
rate data and again learning switching strategies between the S\&P 500 stock
index and T-Bills are presented.  The results show that the recurrent
reinforcement learning method outperforms the Q-learning in the S\&P 500/T-Bill
allocation problem.  The authors also observe that the recurrent reinforcement
learning method has a simpler functional form, the output is not discrete and
easily maps to real valued actions and that the algorithm is more robust to
noise in financial data and adapts quickly to non-stationary environments.

In \cite{vengerov:grid} a marketplace for computational resources in
envisioned.  The authors propse a market in which grid service suppliers offer
to execute jobs submitted by customers for a price per CPU-hour.  The problem
formulation requires customers to request a quote for computing a job $k$ for a
time $\tau_k$ on $n_k$ CPUs.  The quote returned specifies a price $P_k$ at
which the $k$ would be charged and a delay time $d_k$ for the job.  The service
provider's goal is to learn a policy for pricing quotes that maximises long
term revenue when competing in a market environment with other providers.  A
differentiated pricing model is implemented where a standard service is priced
at 1~\$/CPU-hour and a premium sercice a $P$~\$/CPU-hour, with premium jobs
prioratised over standard jobs.  The state of the market environment is defined
by the current expected delays in the standard and premium service classes and
the product of the number of CPUs requested and the job execution time,
$n_k \tau_k$.  The reward $r(s,a)$ for action $a$ in state $s$ is the total
price paid for the job.  The policy gradient method employed is a modified
version of Williams' REINFORCE where
\begin{equation}
Q(s_t,a_t) = \sum_{t=1}^T r(s_t,a_t) - \overline{r}_t
\end{equation}
and $\overline{r}_t$ is the current average reward.

The authors recognise that their grid market model could be applied to any
multi-seller retail market environment.  The experimental results show that if
all grid service providers simultaneously use the learning algorithm then the
process converges to a Nash equilibrium.  The results also showed that
significant increases in profit were possible by offering both standard and
premium services.

% \section{Agent-Based Simulation}
% Relative to the traditional closed-form equilibrium approaches, agent-based
% simulation of (electricity) markets is a new field of research.  For
% comprehensive reviews and surveys of the many different techniques that have
% been applied in recent years the interested reader is directed to
% \cite{anke:2008,tesfatsi:handbook,visud:thesis}.  This section will focus on
% reviewing literature from the field in which reinforcement learning techniques
% were applied in combination with explicit power system models.  A short review
% is also provided of some more general applications of reinforcement learning
% with connectionist systems and policy-gradient methods.

% Game theoretic models are commonly associated with economics and attempt to
% capture behaviour in strategic situations mathematically.  They have been
% applied to electric energy problems of many forms, including but not limited
% to analysis of market structure, market liquidity, pricing methodologies,
% regulatory structure, plant positioning and network congestion.  More
% recently, agent-based simulation has received a certain degree of attention
% from researchers and has been applied in some of these fields also.
%
% While popular and seemingly promising, agent-based simulation is still centred
% around abstracted models.  The assumptions made is this abstraction must be
% subjected to the same verification and validation as with equation-based
% models.  Verification of assumptions and model validation are often overlooked
% in agent-based simulations of energy markets, yet they are possibly the most
% important steps in the model building process.  Techniques used to develop,
% debug and maintain large computer programs can often be used to verify that a
% model does what it is intended to do.
%
% Validation of an energy market model is more difficult.  It can be accomplished
% using the intuition of experts or through comparison of simulation results
% with either historical market data or theoretical results from more abstract
% representations of the model.  Finding verifyable trends in existing markets
% is a very large challenge.  To then prove that a computational model
% replicates these characteristics with suitable fidelity is yet more
% challenging still.  Only when a model is suitably verified and validated can
% any conclusions be drawn from results obtained through implementation and
% simulation of suitable scenarios.

\section{Simulations Applying Q-learning}
% Krause et al.\ have published agent-based energy market research in which
% Q-learning methods were applied while considering physical system properties.
% In a comparison between Nash equilibrium analysis and agent-based simulation,
% the suitability of bottom-up modelling for the assessment of market evolution
% was assessed\cite{krause:nash}.  This is built upon in subsequent publications
% which evaluate the influence on market power and social welfare of three
% congestion management schemes and which analyse strategic behavior in combined
% gas and electricity markets\cite{krause:cong,krause:gas}.  Power Transmission
% Distribution Factors (PTDF) are used in place of explicit power flow equations
% in determining line flows.  The action domain of generating agents is limited
% to 0\%, 5\% and 10\% markups on true marginal costs.  The implementation of the
% Q-learning method used does not differentiate between environment states when
% selecting actions. This is a modification to the traditional formulation that
% still results in convincing conclusions, as with the popular Roth-Erev method.
%
% There are similar applications of Q-learning in which states \textit{are}
% defined, but none model the AC transmission system.  A common approach is to
% use categorised market price from the previous period as state
% information\cite{bakirtzis:psce,xiong:discrim}.
Agent-based simulation of electricity markets has been researched with
participants behavioral aspects modelled using Q-learning methods.  The most
prominent work in which this method has been is adopted has been conducted at
the Swiss Federal Institutes of Technology in Zurich and Lausanne.  The
foundations for this were laid in \cite{krause:nash04} with a
comparison of agent-based modelling using reinforcement learning with Nash
equilibrium analysis in assessing network constrained power pool market
dynamics.  Parameter sensistivity of comparison results were later analysed in
\cite{krause:nash06}.

In the papers a mandatory spot market is modelled and cleared using a DC
optimal power flow formulation.  A five bus power system model is defined with
three generators and four inelastic and constant loads.  Linear marginal cost
functions
\begin{equation}
C_{g,i}(P_{g,i}) = b_{g,i} + s_{g,i}P_{g,i}
\end{equation}
are assumed for each generator $i$ where $P_{g,i}$ is the active power output,
$s_{g,i}$ is the slope of the cost function and $b_{g,i}$ is the cost when
$P_{g,i} = 0$.  Suppliers are given the option to markup their bids to the
market not by increasing the slope, but by increasing $b_{g,i}$ by 10\%, 20\%
or 30\%.  A price cap of \$60/MW is set, but may not be exceeded by any of the
available markups.

Nash equilibrium of the market is computed by clearing the market for all
possible markup combinations and determining the actions for which no player is
motivated to deviate from as it would result in a decrease in expected reward.
Experiments are conducted in which there is a single Nash equilibrium and two
equilibria.

An $\epsilon$-greedy strategy is applied for action selection and a stateless
action value function is updated at each time step $t$ according to
\begin{equation}
Q(a_t) \leftarrow Q(a_t) + \alpha(r_{t+1} - Q(a_t))
\end{equation}
where $\alpha$ is the learning rate.  Further to \cite{krause:nash04},
simulations with discrete sets of values for the parameters $\alpha$ and
$\epsilon$ were carried out in \cite{krause:nash06}.  While parameter
variations affected the frequency of equilibrium oscillations, Nash equilibrium
was still approached and the oscillatory behaviour observed for almost all
combinations.

The significance of this research is that is verifies that the agent-based
approach settles at the same theoretical optimum as with closed-form equilibrium
approaches and that exploratory policies result in the exploitation of multiple
equlibria if they exist.


Having validated the suitability of an agent-based, bottom-up, approach to
assessing evolution of market characteristics, the authors applied the technique
in a comparison of congestion management schemes \cite{krause:cong}.  The first
scheme considered was \textit{locational marginal pricing}, or nodal pricing,
where congestion is managed by optimising the output of generators with respect to
maximum social welfare.  Loading of branches to their flow limits results in
non-uniform nodal marginal prices.  A nodal marginal price equals the
increase in the total system cost (the value of the objective function) when
generation at that node is increased by 1MW.  These prices are commonly used in
electricity market analysis as they may be determined from the Lagrangian
multipliers on the active power balance constraints in the optimal power flow
formulation.  The second scheme considered, named \textit{market splitting}, is
similar to locational marginal pricing, but the system gets subdivided into
zones within which the nodal prices are uniform.  The final \textit{flow based
market coupling} scheme also uses uniform zonal pricing, but requires a
simplified representation of the network.  Power flows within zones are not
represented and all lines within zones are aggregated into one equivalent
interconnector.

In a simpler alternative to the conventional DC optimal power flow formulation,
the computation of line power flows is done using a matrix power transfer
distribution factors.  The $(i,j)^{th}$ element of this matrix corresponds to
the change in active power flow on line $j$ given an additional injection of
1MW at the slack bus and corresponding withdrawl of 1MW at node $i$.

The congestion management schemes are evaluated under perfect competition, where
suppliers bid at marginal cost, and under oligopolistic competition, in which
markups of 5\% and 10\% may be added to marginal cost.  The benefits obtained
between reward at marginal cost and a maximum markup are used to assess market
power.  The experimental results show different market power allocations under
the three constraint management schemes.  The significance of this work is that
it demonstrates an agent-based model being applied to an important problem in
the electricity supply industry.

The same Q-learning method is used in \cite{krause:gas} to analyse strategic
behaviour in integrated electricity and gas markets.  Again, power flow are
modelled using a power transfer distribution matrix.  Pipeline losses in the
gas network are approximated using using a cubic function of the flow.  Three
combined gas and electricity models are compared.  In the first model,
operators of gas-fired power plant submit separate bid functions for gas and
electricity.  Bids are then cleared as a single optimisation problem.  In model
two, operators submit one offer for their capacity to convert gas to
electricity.  In the final model, bids are submitted only to the electricity
market, after which gas is purchased regardless of price.  Gas supply offers are
modelled as a linear function with no strategic involvement.  The models are
compared in terms of social welfare using a three bus power system model with
three non-gas-fired power plants and one gas-fired plant.  The experimental
results show little difference between electricity prices and social welfare
prices between the models.

However, this research illustrates the interest in and complexity associated
with modelling relationships between markets.  The authors recognise the need
for further and more detailed simulation in order to improve evaluation of
market coupling models.


Researchers at the Argonne Nationa Laboratory have published results from a
preliminary study of interactions between \textit{emission} and electricity
markets \cite{wang:09}.  A cap-and-trade system for emissions is modelled where
generator companies are allocated with $\mbox{CO}_2$ allowances that may
subsequently be traded.  Generator companies are assumed to be price takers in
the emissions market and to have negligible influence on market clearing
prices. Prices for allowances from the European Energy Exchange were used.
Whereas in the electricity market, an oligopoly is assumed and bids are cleared
using a DC optimal power flow formulation.  To improve selection of the
$\epsilon$ parameter for exploratory action selection, a simulated annealing
(SA) Q-learning method based on the Metropolis criterion is used \cite{guo:sa}.
Under this method $\epsilon$ is changed at each simulation step to allow
solutions to escape from local optima.  A two bus system is used to study cases
in which, allowance trading is not used, allowances can be exchanged in the
emission market and with variations in the allowance allocations.

A one year, hourly load profile with a summer peak is used to represent changes
in demand.  The electricity market is cleared each hour and the emissions market
at the end of each simulated week.  The agents learn, when they have a defecit
of allowances, to borrow future allowances in the summer when load and
allowance prices are high.  Conversly, when they have a surplus they learn to
sell at this time.  In the third case, the authors show the sensitivity of
profits to initial allocations and conclude that the experimental results can
not be generalised.  The authors cite further model validation and agent
learning method improvements as necessary future work.

The SA-Q-learning method is also used by researchers from the University of
Thessaloniki in \cite{tellidou:tacit} to study capacity withholding and tacit
collusion among electricity market participants.  A mandatory spot market is
implemented, where bid quantities may be less than net capacity and bid prices
may be marked up upon marginal cost by increasing the slope of a linear cost
function.  Again market clearing is achieved using DC optimal power flow and
locational marginal prices are used to calculate profits and reinforce the
learning process.  Demand is assumed to be inelastic and transmission
system parameters constant between simulation periods.  A simple two node
power system model with two generators is used in three test cases.  In a
reference case each generator bids full capacity at marginal cost.  In the
second case, generators bid quantities in steps of 10MW and price markups in
steps of \euro{2}/MWh.  In the final case the same generation capacity is
split among eight identical generators to increase the level of competition.  the
experimental results show that generators learn to withhold capacity and develop
tacit collusion strategies to capture congestion profits.

The convergence to Nash equilibrium shown in \cite{krause:nash04} is confirmed
in \cite{sistani:06}.  Boltzman (soft-max) exploration is used for action
selection with the temperature parameter adjusted during the simulations.  A
modified version of the IEEE 30 bus test system is used with the number of
generators reduced from nine to six.  No optimal power flow formulation or
details of the reward signal are provided.  Generators are given a three step
action space where the slope of a linear supply function may be less then, equal
to or above marginal cost.  The experimental results show that with temperature
parameter adjustment Nash equilibrium is achieved and oscillation associated
with $\epsilon$-greedy action selection are avoided.


\section{Simulations Applying Roth-Erev}
% The AMES (Agent-based Modeling of Electricity Systems) power market test bed is
% a software package that models core features of the Wholesale Power Market
% Platform (WPMP) -- a market design proposed by the US Federal Energy Regulatory
% Commission (FERC) in April 2003 for common adoption in regions of the
% US\cite{tesfatsi:wpmp}. The design features:
% \begin{itemize}
%   \item a centralised structure managed by an independent market operator,
%   \item parallel day-ahead and real-time markets and
%   \item locational marginal pricing.
% \end{itemize}
% Learning agents may represent load serving entities or generating companies and
% learn using implementations of the Roth-Erev method (See sections
% \ref{sec:rotherev} and \ref{sec:variant}, above) built on the Repast agent
% simulation toolkit\cite{gieseler:thesis}.  The permissive license under which
% the source code for these algorithms has been released allowed direct
% translation of them for use in this study.  Agents learn from the solutions of
% hourly bid/offer based DC-OPF problems formulated as quadratic programs and
% solved using QuadProgJ\cite{tesfatsi:dcopf}.
%
% The ability of generator agents to learn particular supply offers has been
% demonstrated along with the plotting and data handling capabilities of AMES
% using a 5-bus network model\cite{tesfatsi:pes09}.  The same network has been
% used to investigate strategic capacity withholding in FERC wholesale power
% market design\cite{tesfasi:psce}.  Generator agents report linear marginal cost
% functions to the market operator and supply functions are formed through linear
% interpolation between the prices at minimum and maximum production limits.
% Load serving agents submit combinations of fixed demand bids and
% price-sensitive bid functions, the ratio between which is varied between 0.0 and
% 1.0 to test physical and economic capacity withholding potential.
A reinforcement learning method based on empirical results obtained from
observing how humans learn decision making strategies in games against multiple
strategic players has received considerable attention from the agent-based
electricity market simultaion community \cite{roth:games,roth:aer}.

In \cite{nicolaisen:2001} an agent-based model of a wholesale electricity market
with both supply and demand side participation is constructed.  It is used to
study market power and short-run market efficiency under discriminatory pricing
through systematic variation of concentration and capacity conditions.  To model
the power system, each trader is assigned values of available transmission
capability (ATC) with respect to each other trader.  Offers from buyers and
sellers are matched on a merit order basis, with quantities restricted by
the ATCs.  Two issues with the original Roth-Erev method are observed and the
modified version defined in Section \ref{sec:variant} above is proposed.

A maximum markup (markdown) of \$40/MWh is specified for each seller (buyer).
Traders are not permitted to make negative profits and the feasible price range
is divided into 30 offer prices for 1000 auction rounds cases and 100 offer
prices for 10000 aution round cases.  The parameters of the Roth-Erev method are
calibrated using direct search within reasonable ranges.  Nine combinations of
buyer and sellers numbers and total trading capacities are tested using the
calibarated values and \textit{best-fit} parameter values determined by Erev \&
Roth.

The experimental results show that good market efficiency is achieved under all
configurations and the sensitivity to method parameters is small.  Levels of
market power are found to be strongly predictive and little difference is found
between cases in which opportunistic price offers are permitted and when traders
are forced to bid at marginal cost.  The results are compared with those from
\cite{nicolaisen:2000}, in which genetic algorithms were used.  The authors
conclude that the reinforcement learning approach leads to higher market
efficiency due their adaption according to \textit{individual} profits.

Further research from Iowa State University, involving the modified Roth-Erev
method, has centered around the AMES wholesale electricity market test bed.  A
detailed description of which is provided in Section \ref{sec:ames} below.

In \cite{tesfatsi:psce} AMES is used to investigate strategic capacity
withholding in a wholesale electricity market design proposed by U.S. Federal
Energy Regulatory Commission in April 2003.  A five bus power system model with
five generators and three dispatchable loads is defined and capacity withholding
is represented by premitting traders to lower than true operating capacity and
higher than true marginal costs.

Comparing results from a benchmark case (in which true production costs are
reported, but higher than marginal cost functions may be reported) and cases in
which reported production limits may be less than the true values, the authors
find, that with sufficient capacity reserve, no evidence to suggest potential
for inducing higher net earnings through capacity withholding in the WPMP.

Researchers from the University of Genoa have used the modified Roth-Erev
method to study strategic behaviour in the Italian wholsale electricity market
\cite{cincotti:09}.  The exact clearing procedure is replicated and a model of
the Italian transmission system, including an interconnector to Sicily, with
zonal subdivision, is defined.  Within each of the 11 zones, thermal plant is
combined according to technology (coal, oil, combined cycle gas, turbo gas and
repower) and associated with one of 16 generation companies according to the
size of the companies share.  The resulting 53 agents are assumed to bid full
capacity and may markup bid prices in steps of 5\%, with a maximum markup of
300\%.  Bids are cleared using a DC optimal power flow formulation with
generation capacity constraints and zone interconnector flow limits.  Agents are
rewarded according to a uniform national price, computed as a weighted average
of zonal prices with respect to zonal load.  Using actual hourly load data, it
is shown that in experiments in which agents \textit{learn} their optimal
strategy, histrorical trends were replicated in all but certain hours of peak
load.  The authors state a desire to test different learning methods and perform
further empirical validation.

In \cite{micola:08} a multi-tier model of wholesale natural gas, wholesale
electricity and retail electricity markets is studied using another variant of
the Roth-Erev method.  Coordination between strategic business units (SBU)
within the same firm, but participating in different markets, is varied
systematically and profit differences observed.

An initial two-tier model involves firms with two associated agents rewards,
$r^1$ and $r^2$, are initially independant.  A \textit{reward independance}
parameter $\alpha$ is used to control the fraction of profit from the other
market that is used in rewarding the agent.  The total rewards are
\begin{equation}
R^1(t) = (1-\alpha)r^1(t) + \alpha r^2(t)
\end{equation}
and
\begin{equation}
R^2(t) = (1-\alpha)r^2(t) + \alpha r^1(t)
\end{equation}
Each action $a$ is a single price bid between zero and the price from the
preceeding market.  The Roth-Erev method is modified such that similar
actions, $a-1$ and $a+1$, are reinforced also.  For each agent, the action
selection propensities in auction round $t$ are
\begin{equation}
p^i_a(t) = \begin{cases}
(1-\gamma)p^i_a(t-1) + R^i(t)& \text{if $s=k$}\\
(1-\gamma)p^i_a(t-1) + (1-\delta)R^i(t)& \text{if $s=k-1$ or $s=k+1$}\\
(1-\gamma)p^i_a(t-1)& \text{if $s\neq k-1$, $s\neq k$ or $s\neq k+1$}
\end{cases}
\end{equation}
where $\delta$, with $0\leq \delta \leq 1$, is the \textit{local
experimentation} parameter, $\gamma$ is the discount parameter and $i\in \lbrace
1,2 \rbrace$.  Actions whose probability of selection fall below a specified
value are removed from the action space.

The initial simulation consists of two wholesalers and three retailers and
$\alpha$ is varied from $0$ to $0.5$ in $51$ discrete steps.  The experiment
is repeated using a three tier model in which two natural gas shippers supply
three electricity generators who sell to four electricity retailers.  The
results show a rise in market prices as reward interdependance is increased and
greater profits for integrated firms.

% \subsection{Learning Method Comparisons}
% \subsection{Heuristic Approaches}
% \section{Simulations Applying Genetic Algorithms}
% \section{Learning Classifier Systems}

% \section{Closed-Form Equilibrium}
% Hobbs, Neuhoff

% \section{Policy-Gradient Reinforcement Learning}
% TD-Gammon

\section{Open Source Power Engineering Software}
% MATPOWER, OpenDSS
% Feature matrix
\subsection{Agent-based Modelling of Electricity Systems}
\label{sec:ames}
